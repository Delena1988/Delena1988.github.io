<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[瑞士旅行 Day 1]]></title>
    <url>%2Fposts%2F636a%2F</url>
    <content type="text"><![CDATA[2019.8.17 杭州–&gt;上海–&gt;俄罗斯–&gt;苏黎世万科西庐 –&gt; 杭州东站（打车） 杭州东站 –&gt; 上海虹桥（高铁 G7552检票口15A 7:16–8:22） 上海虹桥 –&gt; 龙阳路站（地铁2号线） 龙阳路站 –&gt; 上海浦东机场（磁悬浮） T1航站楼 东方航空MU591]]></content>
      <categories>
        <category>旅行</category>
      </categories>
      <tags>
        <tag>旅行</tag>
        <tag>瑞士</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo短地址]]></title>
    <url>%2Fposts%2F179c%2F</url>
    <content type="text"><![CDATA[每次从博客分享文章给别人都很苦恼，Hexo 默认生成的链接太长了，而且一旦文章名字改变，链接也跟着改变。有没有什么方法让地址尽量短小精悍，同时永久化呢？ 感谢 rozbo/hexo-abbrlink，完美解决此痛点。 安装在博客主目录下执行 1npm install hexo-abbrlink --save 配置在 _config.yml 配置文件写入 1234567# abbrlink configabbrlink: alg: crc16 #support crc16(default) and crc32 rep: hex #support dec(default) and hex# 更改 permalink 值permalink: posts/:abbrlink.html]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo添加live2d看板动画]]></title>
    <url>%2Fposts%2Fe4e6%2F</url>
    <content type="text"><![CDATA[官网https://www.npmjs.com/package/hexo-helper-live2d 安装live2d在博客主目录下执行如下命令： 1npm install --save hexo-helper-live2d 下载各种动画model地址：https://github.com/xiazeyu/live2d-widget-models.git 下载好之后将packages里的所有动画模板拷贝到博客的node_modules目录里 配置博客配置文件_config.yml 1234567891011live2d: enable: true pluginModelPath: assets/ model: use: live2d-widget-model-epsilon2_1 #模板目录，在node_modules里 display: position: left width: 150 height: 300 mobile: show: false #是否在手机进行显示 部署1hexo s]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat启动闪退]]></title>
    <url>%2Fposts%2F27b2%2F</url>
    <content type="text"><![CDATA[Tomcat启动闪退，可以稍微修改下startup.bat批处理文件，添加PAUSE，这样运行结束只有按任意键才会关掉窗口(调试成功，在去掉PAUSE)。]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2Fposts%2Fbc66%2F</url>
    <content type="text"><![CDATA[Linux是目前应用最广泛的服务器操作系统，基于Unix，开源免费，由于系统的稳定性和安全性，市场占有率很高，几乎成为程序代码运行的最佳系统环境。linux不仅可以长时间的运行我们编写的程序代码，还可以安装在各种计算机硬件设备中，如手机、路由器等，Android程序最底层就是运行在linux系统上的。 linux的目录结构 bin (binaries)存放二进制可执行文件 sbin (super user binaries)存放二进制可执行文件，只有root才能访问 etc (etcetera)存放系统配置文件 usr (unix shared resources)用于存放共享的系统资源 home 存放用户文件的根目录 root 超级用户目录 dev (devices)用于存放设备文件 lib (library)存放跟文件系统中的程序运行所需要的共享库及内核模块 mnt (mount)系统管理员安装临时文件系统的安装点 boot 存放用于系统引导时使用的各种文件 tmp (temporary)用于存放各种临时文件 var (variable)用于存放运行时需要改变数据的文件 linux常用命令命令格式：命令 -选项 参数 （选项和参数可以为空） 1如：ls -la /usr 操作文件及目录 命令 参数 示例 说明 cd cd /home 切换目录 pwd pwd 显示当前工作目录 touch touch 1.txt 创建空文件 mkdir mkdir testdir 创建一个新目录 -p mkdir -p dir1/dir2/dir3/ 创建多级目录，父目录不存在情况下先生成父目录 cp cp 1.txt 复制文件或目录 -r cp -r dir1/ 递归处理，将指定目录下的文件与子目录一并拷贝 mv mv dir1 dir2 移动文件或目录、文件或目录改名 rm rm 1.txt 删除文件 -r-f rm -rf dir1 r 同时删除该目录下的所有文件f强制删除文件或目录 rmdir rmdir dir1 删除空目录 cat cat 1.txt 显示文本文件内容 more more 1.txt 分页显示文本文件内容，可前后翻页，空格向后，b向前 less less 1.txt 分页显示文本文件内容，可前后翻译，空格向后，b向前，支持底行模式 head head 1.txt 查看文本开头部分，默认十行 -[num] head -20 1.txt 查看文本开头部分指定行数 tail tail 1.txt 查看文本结尾部分，默认十行 -[num] 查看文本结尾部分指定行数 -f 循环滚动读取文件并动态显示在屏幕上，根据文件属性追踪 -F 循环滚动读取文件并动态显示在屏幕上，文件文件名追踪 wc wc 1.txt 统计文本的行数、字数、字符数 -m wc -m 1.txt 字符数 -w wc -w 1.txt 文本字数 -l wc -l 1.txt 文本行数 find -name find / -name 1.txt 在文件系统中的指定目录下查找指定的文件 grep grep aaa 1.txt 在指定文件中查找包含指定内容的行，例：在1.txt中查找包含aaa的所有行 ln ln 1.txt 1_bak.txt 建立链接文件 -s ln -s 1.txt 1_bak.txt 对源文件建立符号连接，而非硬连接 系统常用命令 命令 参数 示例 说明 top top 显示当前系统中耗费资源最多的进程 date date 显示系统当前时间 ps 较少单独使用，配参数根据需求，ps -ef 或者 ps -aux -e/-A ps -e 显示所有进程，环境变量 -f ps -ef 全格式显示 -a ps -a 显示所有用户的所有进程（包括其他用户） -u ps -au 按用户名和启动时间的顺序来显示进程 -x ps -aux 显示无控制终端的进程 kill -9 kill -9 pid 强制杀死一个进程 df df 显示文件系统磁盘空间的使用情况 -h df -h 以人类可读的方式显示，Kb，Mb，GB等 du 显示指定的目录及其子目录已使用的磁盘空间的总和 -s du -s * 显示指定目录的总和，*当前目录下表示所有 -h du -sh * 以人类可读的方式显示，Kb，Mb，GB等 free free 显示当前内存和交换空间的使用情况 ifconfig ifconfig 网卡网络配置，常用于查看当前IP地址 ifconfig eth0 192.168.12.22 临时修改系统IP（重启后失效） ping ping baidu.com 测试网络的连通性 hostname hostname 查看主机名 shutdown -r shutdown -r 先关机再重启 -h shutdown -h 关机后不重启 halt halt 关机后关闭电源，相当于shutdown -h reboot reboot 重新启动，相当于shutdown -r 压缩解压缩 命令 参数 示例 说明 gzip gzip 1.txt 压缩后面的文件或者文件夹 -d gzip -d 1.txt.gz 解压后面的压缩文件 -[num] gzip -9 1.txt 用指定的数字num调整压缩的速度，-1或–fast表示最快压缩方法（低压缩比），-9或–best表示最慢压缩方法（高压缩比）。系统缺省值为6 tar -c tar -cvf 1.tar 1.txt 建立一个压缩文件的参数指令，例，将1.txt压缩为1.tar，也可指定多个文件或文件夹 -x tar -xvf 1.tar 1.txt 解开一个压缩文件的参数指令 -z tar -acvf 1.tar.gz 1.txttar -zxvf 1.tar.gz 1.txt 是否需要用gzip，使用gzip压缩或解压 -v 压缩的过程中显示文件 -f 使用档名，再f之后要立即接档名 SCPscp命令用于Linux之间复制文件和目录。 scp是 secure copy的缩写, scp是linux系统下基于ssh登陆进行安全的远程文件拷贝命令。 1234567#从本地复制到远程scp local_file remote_username@remote_ip:remote_folder scp -r local_folder remote_username@remote_ip:remote_folder #从远程复制到本地scp root@www.runoob.com:/home/root/others/music /home/space/music/1.mp3 scp -r www.runoob.com:/home/root/others/ /home/space/music/ 如果远程服务器防火墙有为scp命令设置了指定的端口，我们需要使用 -P 参数来设置命令的端口号，命令格式如下： 12#scp 命令使用端口号 4588scp -P 4588 remote@www.runoob.com:/usr/local/sin.sh /home/administrator 文件权限操作 linux文件权限的描述格式解读 r 可读权限，w可写权限，x可执行权限（也可以用二进制表示 111 110 100 –&gt; 764） 第1位：文件类型（d 目录，- 普通文件，l 链接文件） 第2-4位：所属用户权限，用u（user）表示 第5-7位：所属组权限，用g（group）表示 第8-10位：其他用户权限，用o（other）表示 第2-10位：表示所有的权限，用a（all）表示 命令 参数 示例 说明 chmod chmod u+r 1.txt 修改文件或目录的权限u表示当前用户，g表示同组用户，o表示其他用户，a表示所有用户r表示可读，w表示可写，x表示可执行例：修改1.txt文件给当前用户添加可执行权限 -R chmod -R u+r dir1 修改指定目录及其子目录的所有文件的权限 三位数字 chmod 764 1.sh 直接指定文件的权限7：表示可读可写可执行，4+2+16：表示可读可写，4+2… chown chown user1:group1 1.txt 修改文件的所属用户和组例：将1.txt文件的所属用户指定为user1，组为group1 -R chown -R user1:group1 1.txt 修改目录下所有文件及子目录的所属用户和组，用数字来表示泉下（r=4，w=2，x=1，-=0） linux系统常用快捷键及符号命令 命令 参数 实例 说明 ctrl + c 停止进程 ctrl + l 清屏 ctrl + r 搜索历史命令 ctrl + q 退出 tab 自动不全 &gt; echo “haha” &gt; 1.txt 将前一条命令的输出，写入到后面的文本中将文本清空，然后写入 &gt;&gt; echo “lala” &gt;&gt; 1.txt 将前一条命令的输出，写入到后面的文本中不清空文本，追加到文本最后 &#124; cat 1.txt &#124; grep ‘hello’ 管道命令，以前一个命令的输出作为输入，然后进行运算例：打印1.txt中带有hello字符串的行 * 通配符，指所有 vim编辑器vi / vim是Linux上最常用的文本编辑器而且功能非常强大。 修改文本 操作 说明 i 在光标前插入 I 在光标当前行开始插入 a 在光标后插入 A 在光标当前行末尾插入 o 在光标当前行的下一行插入新行 O 在光标当前行的上一行插入新行 :wq 保存并退出 定位命令 操作 说明 :set nu 显示行号 :set nonu 取消行号 gg 跳到首行 G 跳到末行 :n 跳到第n行 替换和取消命令 操作 说明 u undo，取消上一步操作 ctrl + r redo，返回到undo之前 r 替换光标所在处的字符 R 从光标所在处开始替换，按Esc键结束 删除命令 操作 说明 x 删除光标所在处字符 nx 删除光标所在处后的n个字符 dd 删除光标所在行。ndd删除n行 dG 删除光标所在行到末尾行的所有内容 D 删除光标所在处到行尾的内容 :5,7d 删除指定范围的行 常用快捷键 操作 说明 Shift + zz 保存退出，与“:wq”作用相同 v 进入字符可视模式 V 进入行可视模式 ctrl + v 进入块可视模式]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql定时任务]]></title>
    <url>%2Fposts%2F9917%2F</url>
    <content type="text"><![CDATA[MySQL 从 5.0 开始自带了定时事件操作。 后台周期定时任务可以有多种解决方案，我所知道的大概有以下几种： (1). 后台框架自带定时任务。比如 Php 中的 Laravel 框架里有提供定时任务操作接口，其他的框架大家可以单独针对了解。 (2). 服务器操作系统层面的定时。通常我们的服务器主要基于两大平台，一个 Windows Server, 它的定时任务系统有提供的。Linux 下也有，通常流行的是 crontab 工具实现的 ( 想了解这里有个 视频教程 ), 但是 crontab 的定时任务通常定时操作脚本这样的文件，而直接定时操作数据库的就比较麻烦了。但是也有解决办法，就是在服务器端写一个 get 请求 url，在后台里完成要定时完成的数据库操作，这样我们只要实现定时访问该接口就行了，Linux 下的 curl 命令可以很方便发出 get 请求，我们只要写个包含访问该接口的脚本，再结合 crontab 就可以完成后台数据的定时更新操作了。 (3). 但是毕竟写个接口安全性不是太高，而大家用的如果是 MySQL 数据库，那就正好可以利用其自带的定时操作了，下面简单介绍 MySQL 定时操作的使用。 Mysql配置查看定时策略是否开启，查看命令: 1show variables like '%event_sche%'; 显示的 event_scheduler 为 OFF 时用以下命令开启: 1set global event_scheduler=1; 以上的改法在数据库重启后将会恢复为原来状态，要想数据库重启后也可以让 event_scheduler 开启，则需要在配置文件 my.ini 的设置。修改如下，然后重启 MySQL 服务即可: 12[mysqld]event_scheduler=ON // 这一行加入 mysqld 标签下 创建定时任务 event ( 事件 )1234create event second_eventon schedule every 1 secondon completion preserve disabledo call test_proce(); 代码说明： 第一行 create event day_event 是创建名为 second_event 的事件,注意此处没有括号； 第二行是创建周期定时的规则，本处的意思是每秒钟执行一次； 第三行 on completion preserve disable 是表示创建后并不开始生效； 第四行 do call test_proce() 是该 event(事件) 的操作内容，表示调用我们刚刚创建的 test_proce() 存储过程。 查看定时任务 event ( 事件 )查看本机所有的事件： 1SELECT event_name,event_definition,interval_value,interval_field,status FROM information_schema.EVENTS; 开启已经创建好的 event ( 事件 )12alter event second_event on completion preserve enable;//开启定时任务alter event second_event on completion preserve disable;//关闭定时任务 常见周期定时规则① 周期执行 – 关键字 EVERY 单位有：second, minute, hour, day, week(周), quarter(季度), month, year，如： 123on schedule every 1 second //每秒执行1次on schedule every 2 minute //每两分钟执行1次on schedule every 3 day //每3天执行1次 ② 在具体某个时间执行 – 关键字 AT, 如： 123on schedule at current_timestamp()+interval 5 day // 5天后执行on schedule at current_timestamp()+interval 10 minute // 10分钟后执行on schedule at '2016-10-01 21:50:00' // 在2016年10月1日，晚上9点50执行 ③ 在某个时间段执行 – 关键字 STARTS ENDS, 如： 12on schedule every 1 day starts current_timestamp()+interval 5 day ends current_timestamp()+interval 1 month // 5天后开始每天都执行执行到下个月底on schedule every 1 day ends current_timestamp()+interval 5 day //从现在起每天执行，执行5天]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下查看最消耗CPU、内存的进程]]></title>
    <url>%2Fposts%2Fa3d8%2F</url>
    <content type="text"><![CDATA[CPU占用最多的前10个进程1ps auxw|head -1;ps auxw|sort -rn -k3|head -10 内存消耗最多的前10个进程1ps auxw|head -1;ps auxw|sort -rn -k4|head -10 虚拟内存使用最多的前10个进程1ps auxw|head -1;ps auxw|sort -rn -k5|head -10 参数含义 参数 含义 USER 进程所属用户 PID 进程ID %CPU 进程占用CPU百分比 %MEM 进程的内存占用率 MAJFL is the major page fault count VSZ 进程所使用的虚存的大小，单位：kb（killobytes） RSS 实际内存占用大小，单位：kb（killobytes） TTY 与进程关联的终端 STAT 进程状态D 不可中断 Uninterruptible sleep (usually IO)R 正在运行，或在队列中的进程 S 处于休眠状态 T 停止或被追踪 Z 僵尸进程 W 进入内存交换（从内核2.6开始无效） X 死掉的进程 &lt; 高优先级 N 低优先级 L 有些页被锁进内存 s 包含子进程 + 位于后台的进程组； l 多线程，克隆线程 multi-threaded (using CLONE_THREAD, like NPTL pthreads do) START 进程启动时刻 TIME 进程运行时长 COMMAND 启动进程的命令]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql存储过程]]></title>
    <url>%2Fposts%2F3657%2F</url>
    <content type="text"><![CDATA[MySQL 5.0 版本开始支持存储过程。 存储过程（Stored Procedure）是一种在数据库中存储复杂程序，以便外部程序调用的一种数据库对象。 存储过程是为了完成特定功能的SQL语句集，经编译创建并保存在数据库中，用户可通过指定存储过程的名字并给定参数(需要时)来调用执行。 存储过程思想上很简单，就是数据库 SQL 语言层面的代码封装与重用。 优点 存储过程可封装，并隐藏复杂的商业逻辑。 存储过程可以回传值，并可以接受参数。 存储过程无法使用 SELECT 指令来运行，因为它是子程序，与查看表，数据表或用户定义函数不同。 存储过程可以用在数据检验，强制实行商业逻辑等。 缺点 存储过程，往往定制化于特定的数据库上，因为支持的编程语言不同。当切换到其他厂商的数据库系统时，需要重写原有的存储过程。 存储过程的性能调校与撰写，受限于各种数据库系统。 存储过程的创建和调用12345678910111213141516171819202122CREATE [DEFINER = &#123; user | CURRENT_USER &#125;] PROCEDURE sp_name ([proc_parameter[,...]]) [characteristic ...] routine_bodyproc_parameter: [ IN | OUT | INOUT ] param_name typecharacteristic: COMMENT 'string' | LANGUAGE SQL | [NOT] DETERMINISTIC | &#123; CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA &#125; | SQL SECURITY &#123; DEFINER | INVOKER &#125;routine_body: Valid SQL routine statement[begin_label:] BEGIN [statement_list] ……END [end_label] MYSQL 存储过程中的关键语法声明语句结束符，可以自定义 123DELIMITER $$或DELIMITER // 声明存储过程: 1CREATE PROCEDURE demo_in_parameter(IN p_in int) 存储过程开始和结束符号: 1BEGIN .... END 变量赋值: 1SET @p_in=1 变量定义: 1DECLARE l_int int unsigned default 4000000; 存储过程体 存储过程体包含了在过程调用时必须执行的语句，例如：dml、ddl语句，if-then-else和while-do语句、声明变量的declare语句等 过程体格式：以begin开始，以end结束(可嵌套) 1234567BEGIN BEGIN BEGIN statements; END ENDEND 注意：每个嵌套块及其中的每条语句，必须以分号结束，表示过程体结束的begin-end块(又叫做复合语句compound statement)，则不需要分号。 为语句块贴标签: 123[begin_label:] BEGIN [statement_list]END [end_label] 例如： 1234567label1: BEGIN label2: BEGIN label3: BEGIN statements; END label3 ; END label2;END label1 标签有两个作用： 1、增强代码的可读性 2、在某些语句(例如:leave和iterate语句)，需要用到标签 存储过程的参数MySQL存储过程的参数用在存储过程的定义，共有三种参数类型,IN,OUT,INOUT,形式如： 1CREATEPROCEDURE 存储过程名([[IN |OUT |INOUT ] 参数名 数据类形...]) IN 输入参数：表示调用者向过程传入值（传入值可以是字面量或变量） OUT 输出参数：表示过程向调用者传出值(可以返回多个值)（传出值只能是变量） INOUT 输入输出参数：既表示调用者向过程传入值，又表示过程向调用者传出值（值只能是变量） in 输入参数123456789101112131415161718192021222324252627282930mysql&gt; delimiter $$mysql&gt; create procedure in_param(in p_in int) -&gt; begin -&gt; select p_in; -&gt; set p_in=2; -&gt; select P_in; -&gt; end$$mysql&gt; delimiter ;mysql&gt; set @p_in=1;mysql&gt; call in_param(@p_in);+------+| p_in |+------+| 1 |+------++------+| P_in |+------+| 2 |+------+mysql&gt; select @p_in;+-------+| @p_in |+-------+| 1 |+-------+ 以上可以看出，p_in 在存储过程中被修改，但并不影响 @p_id 的值，因为前者为局部变量、后者为全局变量。 out输出参数1234567891011121314151617181920212223242526272829303132mysql&gt; delimiter //mysql&gt; create procedure out_param(out p_out int) -&gt; begin -&gt; select p_out; -&gt; set p_out=2; -&gt; select p_out; -&gt; end -&gt; //mysql&gt; delimiter ;mysql&gt; set @p_out=1;mysql&gt; call out_param(@p_out);+-------+| p_out |+-------+| NULL |+-------+ #因为out是向调用者输出参数，不接收输入的参数，所以存储过程里的p_out为null+-------+| p_out |+-------+| 2 |+-------+mysql&gt; select @p_out;+--------+| @p_out |+--------+| 2 |+--------+ #调用了out_param存储过程，输出参数，改变了p_out变量的值 inout输入参数1234567891011121314151617181920212223242526272829303132mysql&gt; delimiter $$mysql&gt; create procedure inout_param(inout p_inout int) -&gt; begin -&gt; select p_inout; -&gt; set p_inout=2; -&gt; select p_inout; -&gt; end -&gt; $$mysql&gt; delimiter ;mysql&gt; set @p_inout=1;mysql&gt; call inout_param(@p_inout);+---------+| p_inout |+---------+| 1 |+---------++---------+| p_inout |+---------+| 2 |+---------+mysql&gt; select @p_inout;+----------+| @p_inout |+----------+| 2 |+----------+#调用了inout_param存储过程，接受了输入的参数，也输出参数，改变了变量 注意： 如果过程没有参数，也必须在过程名后面写上小括号例： 1CREATE PROCEDURE sp_name ([proc_parameter[,...]]) …… 确保参数的名字不等于列的名字，否则在过程体中，参数名被当做列名来处理 建议： 输入值使用in参数。 返回值使用out参数。 inout参数就尽量的少用。 变量变量定义局部变量声明一定要放在存储过程体的开始： 1DECLARE variable_name [,variable_name...] datatype [DEFAULT value]; 其中，datatype 为 MySQL 的数据类型，如: int, float, date,varchar(length) 例如: 12345DECLARE l_int int unsigned default 4000000; DECLARE l_numeric number(8,2) DEFAULT 9.95; DECLARE l_date date DEFAULT '1999-12-31'; DECLARE l_datetime datetime DEFAULT '1999-12-31 23:59:59'; DECLARE l_varchar varchar(255) DEFAULT 'This will not be padded'; 变量赋值1SET 变量名 = 表达式值 [,variable_name = expression ...] 用户变量在MySQL客户端使用用户变量: 12345678910111213141516171819202122mysql &gt; SELECT 'Hello World' into @x; mysql &gt; SELECT @x; +-------------+ | @x | +-------------+ | Hello World | +-------------+ mysql &gt; SET @y='Goodbye Cruel World'; mysql &gt; SELECT @y; +---------------------+ | @y | +---------------------+ | Goodbye Cruel World | +---------------------+ mysql &gt; SET @z=1+2+3; mysql &gt; SELECT @z; +------+ | @z | +------+ | 6 | +------+ 在存储过程中使用用户变量 12345678mysql &gt; CREATE PROCEDURE GreetWorld( ) SELECT CONCAT(@greeting,' World'); mysql &gt; SET @greeting='Hello'; mysql &gt; CALL GreetWorld( ); +----------------------------+ | CONCAT(@greeting,' World') | +----------------------------+ | Hello World | +----------------------------+ 在存储过程间传递全局范围的用户变量 123456789mysql&gt; CREATE PROCEDURE p1() SET @last_procedure='p1'; mysql&gt; CREATE PROCEDURE p2() SELECT CONCAT('Last procedure was ',@last_procedure); mysql&gt; CALL p1( ); mysql&gt; CALL p2( ); +-----------------------------------------------+ | CONCAT('Last procedure was ',@last_proc | +-----------------------------------------------+ | Last procedure was p1 | +-----------------------------------------------+ 注意: 1、用户变量名一般以@开头 2、滥用用户变量会导致程序难以理解及管理 执行动态sql1234set @sql = sql --（预处理的sql语句，可以是用concat拼接的语句）PREPARE stmt_name FROM @sql; --预处理动态sql语句EXECUTE stmt_name; --执行sql语句DEALLOCATE PREPARE stmt_name; --释放prepare MySQL存储过程的控制语句变量作用域内部的变量在其作用域范围内享有更高的优先权，当执行到 end。变量时，内部变量消失，此时已经在其作用域外，变量不再可见了，应为在存储过程外再也不能找到这个申明的变量，但是你可以通过 out 参数或者将其值指派给会话变量来保存其值。 123456789101112mysql &gt; DELIMITER // mysql &gt; CREATE PROCEDURE proc3() -&gt; begin -&gt; declare x1 varchar(5) default 'outer'; -&gt; begin -&gt; declare x1 varchar(5) default 'inner'; -&gt; select x1; -&gt; end; -&gt; select x1; -&gt; end; -&gt; // mysql &gt; DELIMITER ; 条件语句if-then-else 语句 12345678910111213141516mysql &gt; DELIMITER // mysql &gt; CREATE PROCEDURE proc2(IN parameter int) -&gt; begin -&gt; declare var int; -&gt; set var=parameter+1; -&gt; if var=0 then -&gt; insert into t values(17); -&gt; end if; -&gt; if parameter=0 then -&gt; update t set s1=s1+1; -&gt; else -&gt; update t set s1=s1+2; -&gt; end if; -&gt; end; -&gt; // mysql &gt; DELIMITER ; case语句： 1234567891011121314151617181920212223mysql &gt; DELIMITER // mysql &gt; CREATE PROCEDURE proc3 (in parameter int) -&gt; begin -&gt; declare var int; -&gt; set var=parameter+1; -&gt; case var -&gt; when 0 then -&gt; insert into t values(17); -&gt; when 1 then -&gt; insert into t values(18); -&gt; else -&gt; insert into t values(19); -&gt; end case; -&gt; end; -&gt; // mysql &gt; DELIMITER ; case when var=0 then insert into t values(30); when var&gt;0 then when var&lt;0 then elseend case 循环语句while ···· end while 123456789101112mysql &gt; DELIMITER // mysql &gt; CREATE PROCEDURE proc4() -&gt; begin -&gt; declare var int; -&gt; set var=0; -&gt; while var&lt;6 do -&gt; insert into t values(var); -&gt; set var=var+1; -&gt; end while; -&gt; end; -&gt; // mysql &gt; DELIMITER ; repeat···· end repeat 12345678910111213mysql &gt; DELIMITER // mysql &gt; CREATE PROCEDURE proc5 () -&gt; begin -&gt; declare v int; -&gt; set v=0; -&gt; repeat -&gt; insert into t values(v); -&gt; set v=v+1; -&gt; until v&gt;=5 -&gt; end repeat; -&gt; end; -&gt; // mysql &gt; DELIMITER ; 1234repeat --循环体until 循环条件 end repeat; loop ·····endloop 123456789101112131415mysql &gt; DELIMITER // mysql &gt; CREATE PROCEDURE proc6 () -&gt; begin -&gt; declare v int; -&gt; set v=0; -&gt; LOOP_LABLE:loop -&gt; insert into t values(v); -&gt; set v=v+1; -&gt; if v &gt;=5 then -&gt; leave LOOP_LABLE; -&gt; end if; -&gt; end loop; -&gt; end; -&gt; // mysql &gt; DELIMITER ; LABLES 标号： 标号可以用在 begin repeat while 或者 loop 语句前，语句标号只能在合法的语句前面使用。可以跳出循环，使运行指令达到复合语句的最后一步。 ITERATE迭代ITERATE 通过引用复合语句的标号,来从新开始复合语句: 12345678910111213141516171819mysql &gt; DELIMITER // mysql &gt; CREATE PROCEDURE proc10 () -&gt; begin -&gt; declare v int; -&gt; set v=0; -&gt; LOOP_LABLE:loop -&gt; if v=3 then -&gt; set v=v+1; -&gt; ITERATE LOOP_LABLE; -&gt; end if; -&gt; insert into t values(v); -&gt; set v=v+1; -&gt; if v&gt;=5 then -&gt; leave LOOP_LABLE; -&gt; end if; -&gt; end loop; -&gt; end; -&gt; // mysql &gt; DELIMITER ;]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql常用命令]]></title>
    <url>%2Fposts%2F39d2%2F</url>
    <content type="text"><![CDATA[查看版本1select @@version 查看所有自定义函数1show function status 查看所有自定义存储过程1show procedure status]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java不重启服务动态加载properties文件]]></title>
    <url>%2Fposts%2Fdba%2F</url>
    <content type="text"><![CDATA[Java动态读取properties配置文件，不需要重启服务。核心根据File.lastModified判断文件是否有变动，重新读取配置文件内容。 PropertiesUtil.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package cn.joinhealth.interview.common.util;import cn.joinhealth.celina.common.enums.FileTypeEnum;import cn.joinhealth.celina.common.utils.StringUtils;import lombok.extern.slf4j.Slf4j;import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStream;import java.nio.charset.StandardCharsets;import java.util.*;@Slf4jpublic class PropertiesUtil &#123; private static Properties prop; private static Long lastModified = 0L; private static final String HUG_INTERVIEW_ENV = "hug_interview"; private static final String SERVER_PROPERTIES_PATH = "/cfg/server/server.properties"; private static void init() &#123; prop = new Properties(); try &#123; String path = System.getenv(HUG_INTERVIEW_ENV); if (StringUtils.isBlank(path)) &#123; path = System.getProperty(HUG_INTERVIEW_ENV); &#125; FileInputStream in = new FileInputStream(path + SERVER_PROPERTIES_PATH); prop.load(in); in.close(); &#125; catch (IOException e) &#123; log.error("", e); &#125; &#125; /** * 判断配置文件是否改动 * * @return returnValue ：true:改动过 ，false:没有改动过 */ private static boolean isPropertiesModified() &#123; boolean returnValue = false; String path = System.getenv(HUG_INTERVIEW_ENV); if (StringUtils.isBlank(path)) &#123; path = System.getProperty(HUG_INTERVIEW_ENV); &#125; File file = new File(path + SERVER_PROPERTIES_PATH); if (file.lastModified() &gt; lastModified) &#123; log.info("修改server.properties配置文件"); lastModified = file.lastModified(); returnValue = true; &#125; return returnValue; &#125; /** * 根据key获取配置文件中的值 * * @param key key值 * @return 返回value */ public static String getPropertiesValue(String key) &#123; if (prop == null || isPropertiesModified()) &#123; init(); &#125; String value; try &#123; value = prop.getProperty(key); if (value != null) &#123; value = value.trim(); &#125; &#125; catch (Exception e) &#123; log.error("getperty:", e); return null; &#125; return value; &#125;&#125; java.io.File.lastModified()该方法返回表示此抽象路径名的文件的最后修改时间。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Idea 中Maven 多线程编译]]></title>
    <url>%2Fposts%2F9c1e%2F</url>
    <content type="text"><![CDATA[Maven3.X 里支持了多线程编译, 分析项目的依赖关系图, 并行构建各个模块: 123mvn -T 4 clean install #指定起4个线程编译 mvn -T 1C clean install #每个CPU核心起1个线程mvn -T 1.5C clean install #每个CPU核心起1.5个线程 到底能提升多少编译的速度:这很大程度上取决于项目的具体模块结构, 但官方的说法是: 速度能普遍提高20 - 50 %. 那么如何在IDEA里开启这个功能: 什么是Wall 一个项目默认构建的总时间是06:10 min, 我们称之为必须的耗时. 但是因为你开启了多线程编译, 现在需要03:32 min, 我们称之为实际的耗时. 为了体现出是并行编译消耗了03:32 min, Maven把实际的耗时叫做Wall Clock.非常形象的告诉你, 虽然项目编译需要六分钟, 但实际上编译完成时, 墙上的钟表才过去了三分钟:)]]></content>
      <categories>
        <category>Maven</category>
      </categories>
      <tags>
        <tag>Idea</tag>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[钉钉对接流程]]></title>
    <url>%2Fposts%2Fe91f%2F</url>
    <content type="text"><![CDATA[Please enter the password to read the blog. Decrypt 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]]></content>
      <categories>
        <category>Jh</category>
      </categories>
      <tags>
        <tag>Jh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java调用SSL异常，javax.net.ssl.SSLHandshakeException: No appropriate protocol]]></title>
    <url>%2Fposts%2Ff37a%2F</url>
    <content type="text"><![CDATA[现象jdk从1.7升级到1.8后，程序运行报错，错误信息如下 123456javax.net.ssl.SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate) at sun.security.ssl.Handshaker.activate(Handshaker.java:529) at sun.security.ssl.SSLSocketImpl.kickstartHandshake(SSLSocketImpl.java:1492) at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1361) at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1413) at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1397) 问题原因jdk1.8版本导致SSL调用权限上有问题 解决方案找到jdk安装目录，jre\lib\security\java.security，找到对应的SSLv3，删除掉，重启项目即可。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SSL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Maven实现一个protobuf]]></title>
    <url>%2Fposts%2F5613%2F</url>
    <content type="text"><![CDATA[Protocol BuffersProtocal Buffers(简称protobuf)是谷歌的一项技术，用于结构化的数据序列化、反序列化，常用于RPC 系统（Remote Procedure Call Protocol System）和持续数据存储系统。 其类似于XML生成和解析，但protobuf的效率高于XML，不过protobuf生成的是字节码，可读性比XML差，类似的还有json、Java的Serializable等。 很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。 Idea安装protobuf插件 配置依赖pom.xml 1234567891011121314151617181920212223242526272829303132333435363738&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt; &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt; &lt;version&gt;3.4.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;extensions&gt; &lt;extension&gt; &lt;groupId&gt;kr.motd.maven&lt;/groupId&gt; &lt;artifactId&gt;os-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.4.1.Final&lt;/version&gt; &lt;/extension&gt; &lt;/extensions&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.xolstice.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;protobuf-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.5.0&lt;/version&gt; &lt;configuration&gt; &lt;protocArtifact&gt; com.google.protobuf:protoc:3.1.0:exe:$&#123;os.detected.classifier&#125; &lt;/protocArtifact&gt; &lt;pluginId&gt;grpc-java&lt;/pluginId&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;compile-custom&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 书写proto文件1234567syntax = &quot;proto3&quot;;option java_package = &quot;cn.joinhealth&quot;;option java_outer_classname = &quot;LicenseModel&quot;;message License &#123; string permission = 1;&#125; 转化成Java文件 测试1234567891011121314151617181920212223242526272829303132package cn.joinhealth.interview.web.root;import com.google.protobuf.InvalidProtocolBufferException;/** * Test * * @author jlin * @date 2019-07-18 19:36 * @Description */public class Test &#123; public static void main(String[] args) throws InvalidProtocolBufferException &#123; LicenseModel.License.Builder builder = LicenseModel.License.newBuilder(); builder.setPermission("1,2,3,4,5,6,7,8,9,10"); LicenseModel.License license = builder.build(); System.out.println("before:" + license); System.out.println("===Person Byte:"); for (byte b : license.toByteArray()) &#123; System.out.print(b); &#125; System.out.println(); System.out.println("================\n"); byte[] byteArray = license.toByteArray(); LicenseModel.License license1 = LicenseModel.License.parseFrom(byteArray); System.out.println("after:" + license1.getPermission()); &#125;&#125; 结果1234567before:permission: "1,2,3,4,5,6,7,8,9,10"===Person Byte:10204944504451445244534454445544564457444948================after:1,2,3,4,5,6,7,8,9,10]]></content>
      <categories>
        <category>protobuf</category>
      </categories>
      <tags>
        <tag>protobuf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SonarQube分析Maven]]></title>
    <url>%2Fposts%2F6f11%2F</url>
    <content type="text"><![CDATA[Mavensetting.xml 12345678910111213141516171819&lt;settings&gt; &lt;pluginGroups&gt; &lt;pluginGroup&gt;org.sonarsource.scanner.maven&lt;/pluginGroup&gt; &lt;/pluginGroups&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;sonar&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;properties&gt; &lt;!-- Optional URL to server. Default value is http://localhost:9000 --&gt; &lt;sonar.host.url&gt; http://myserver:9000 &lt;/sonar.host.url&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/settings&gt; 工程pom.xml 12345678910111213141516171819202122232425262728293031323334353637&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;sonar&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;properties&gt; &lt;!-- Optional URL to server. Default value is http://localhost:9000 --&gt; &lt;sonar.host.url&gt; http://myserver:9000 &lt;/sonar.host.url&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!-- 配置编译插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 配置分析扫描插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.sonarsource.scanner.maven&lt;/groupId&gt; &lt;artifactId&gt;sonar-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.5.0.1254&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt; 运行12mvn clean installmvn sonar:sonar]]></content>
      <categories>
        <category>SonarQube</category>
      </categories>
      <tags>
        <tag>SonarQube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac常用软件]]></title>
    <url>%2Fposts%2Fad3a%2F</url>
    <content type="text"><![CDATA[开发工具Idea Postman Kafka Tool Navicat Charles（抓包） Cyberduck（ftp） iTerm2 SecureCRT JD-GUI（反编译） Alfred Dash rdm（redis） 远程TeamViewer Microsoft Remote Desktop Beta 向日葵 浏览器Chrome 通讯QQ 微信 钉钉 娱乐网易云音乐 QQ音乐 Movist 文本编辑器Sublime Text Mark Text 解压缩BetterZip The Unarchiver 其他Focus Matrix Hammerspoon Flux CleanMyMac 3 ShadowsocksX-NG Parallels Desktop（虚拟机）]]></content>
      <categories>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Saas随访环境]]></title>
    <url>%2Fposts%2F866e%2F</url>
    <content type="text"><![CDATA[Please enter the password to read the blog. Decrypt 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]]></content>
      <categories>
        <category>Jh</category>
      </categories>
      <tags>
        <tag>Jh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客加密功能添加]]></title>
    <url>%2Fposts%2Ffa33%2F</url>
    <content type="text"><![CDATA[安装插件 hexo-blog-encrypt 在 hexo 根目录里找到package.json。 在package.json文件的&quot;dependencies&quot;: {.....里添加 &quot;hexo-blog-encrypt&quot;: &quot;1.1.*&quot; 记得添加”hexo-blog-encrypt”: “1.1.*”之后加逗号,。dependencies中的每行代码都要有,隔开 接着在终端执行 npm install 命令 等待该插件自动安装 注意：每个模板后台不同，我的package.json在node_modules文件下。 启动插件 在根目录的_config.yml中启用该插件: 123#Securityencrypt: enable: true 然后在你文章的头部添加上对应的字段，如 password, abstract, message 123456789title: 瑞士旅行准备date: 2019-07-11 20:45:38tags: - Travelcategories: - Travelpassword: 123456abstract: Welcome to my blog, enter password to read.message: Welcome to my blog, enter password to read. password: 是该博客加密使用的密码 abstract: 是该博客的摘要，会显示在博客的列表页 message: 这个是博客查看时，密码输入框上面的描述性文字 其他，如对 TOC 进行加密、修改加密模板都可以在 官方ReadMe.zh.md中找到。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot配置https访问]]></title>
    <url>%2Fposts%2Fa6d8%2F</url>
    <content type="text"><![CDATA[获取ssl证书方式： 通过keytool生成 通过证书授权机构购买 https相关配置1234567server: port: 8443 httpport: 8080 ssl: key-store: classpath:keystore.p12 key-password: 123456 key-store-password: 123456 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.linjian.https;import org.apache.catalina.Context;import org.apache.catalina.connector.Connector;import org.apache.tomcat.util.descriptor.web.SecurityCollection;import org.apache.tomcat.util.descriptor.web.SecurityConstraint;import org.springframework.beans.factory.annotation.Value;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory;import org.springframework.boot.web.servlet.server.ServletWebServerFactory;import org.springframework.context.annotation.Bean;@SpringBootApplicationpublic class HttpsApplication &#123; @Value("$&#123;server.port&#125;") Integer httpsPort; @Value("$&#123;server.httpport&#125;") Integer httpPort; public static void main(String[] args) &#123; SpringApplication.run(HttpsApplication.class, args); &#125; @Bean public ServletWebServerFactory servletContainer() &#123; TomcatServletWebServerFactory tomcat = new TomcatServletWebServerFactory() &#123; @Override protected void postProcessContext(Context context) &#123; SecurityConstraint securityConstraint = new SecurityConstraint(); securityConstraint.setUserConstraint("CONFIDENTIAL"); SecurityCollection collection = new SecurityCollection(); collection.addPattern("/*"); securityConstraint.addCollection(collection); context.addConstraint(securityConstraint); &#125; &#125;; tomcat.addAdditionalTomcatConnectors(initiateHttpConnector()); return tomcat; &#125; private Connector initiateHttpConnector() &#123; Connector connector = new Connector("org.apache.coyote.http11.Http11NioProtocol"); connector.setScheme("http"); connector.setPort(httpPort); connector.setSecure(false); connector.setRedirectPort(httpsPort); return connector; &#125;&#125; pom.xml 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-core&lt;/artifactId&gt; &lt;version&gt;9.0.19&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[瑞士旅行准备]]></title>
    <url>%2Fposts%2Fc2bd%2F</url>
    <content type="text"><![CDATA[物料 雨伞 高倍数防晒霜 转换插头 通讯（电话卡） 227 Swiss Pass 5318 牙膏/牙刷/毛巾 登山鞋/冲锋衣/登山杖 中英文对照 中文 英文 苏黎世 Zurich 卢塞恩 Lucerne 格林德瓦 Grindelwald 布里恩茨 Brienz 龙疆 Lungern 因特拉肯 Interlaken 施皮茨 Spiez 文根 Wengen 米伦 Murren 蒙特勒 Montreux 尼永 Nyon 采尔马特 Zermatt 门利兴 Mannlichen 小沙伊德克 Kleine Scheidegg 劳特布伦嫩 Lauterbrunnen 图恩 Thun 沃韦 Vevey 韦吉斯 Weggis 菲斯特 First 交通]]></content>
      <categories>
        <category>Travel</category>
      </categories>
      <tags>
        <tag>Travel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Idea中sonar插件使用]]></title>
    <url>%2Fposts%2F2610%2F</url>
    <content type="text"><![CDATA[安装在Idea Plugins中搜索sonar 选择SonarLint进行install 配置 配置General Settings 添加SonarQube Servers，输入URL，User,Password这些 配置Project Settings 绑定server，它会提醒你要update binding在general settings中 选中在sonar服务上已经存在的project，事实上本地的一切代码都能在Issues窗口预览到规则下的改动]]></content>
      <categories>
        <category>Idea</category>
      </categories>
      <tags>
        <tag>Idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[把Tomcat的http改为https的步骤方法]]></title>
    <url>%2Fposts%2F6cc2%2F</url>
    <content type="text"><![CDATA[生成证书1keytool -genkey -alias tomcat -keyalg RSA -keystore D:\\a.keystore 按步骤输入相关信息 修改tomcat相关配置conf/server.xml12345&lt;Connector port=&quot;8443&quot; protocol=&quot;org.apache.coyote.http11.Http11NioProtocol&quot; SSLEnabled=&quot;true&quot; maxThreads=&quot;150&quot; scheme=&quot;https&quot; secure=&quot;true&quot; clientAuth=&quot;false&quot; sslProtocol=&quot;TLS&quot; keystoreFile=&quot;D:\a.keystore&quot; keystorePass=&quot;123456&quot; /&gt; conf/web.xml123456789&lt;security-constraint&gt; &lt;web-resource-collection &gt; &lt;web-resource-name&gt;SSL&lt;/web-resource-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/web-resource-collection&gt; &lt;user-data-constraint&gt; &lt;transport-guarantee&gt;CONFIDENTIAL&lt;/transport-guarantee&gt; &lt;/user-data-constraint&gt; &lt;/security-constraint&gt; ###]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Idea中Java文件太长导致无法识别]]></title>
    <url>%2Fposts%2F1e54%2F</url>
    <content type="text"><![CDATA[只需要修改配置文件help–&gt;Edit Custom Properties 增加配置 1idea.max.intellisense.filesize=9999]]></content>
      <categories>
        <category>Idea</category>
      </categories>
      <tags>
        <tag>Idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Idea修改内存]]></title>
    <url>%2Fposts%2F431f%2F</url>
    <content type="text"><![CDATA[Mac Idea修改内存配置文件路径： /Users/linjian/Library/Preferences/IntelliJIdea2019.1/idea.vmoptions 1234567891011121314151617-Xms1024m-Xmx2048m-XX:ReservedCodeCacheSize=1024m-XX:+UseCompressedOops-Dfile.encoding=UTF-8-XX:+UseConcMarkSweepGC-XX:SoftRefLRUPolicyMSPerMB=50-ea-Dsun.io.useCanonCaches=false-Djava.net.preferIPv4Stack=true-XX:+HeapDumpOnOutOfMemoryError-XX:-OmitStackTraceInFastThrow-Xverify:none-XX:ErrorFile=$USER_HOME/java_error_in_idea_%p.log-XX:HeapDumpPath=$USER_HOME/java_error_in_idea.hprof-javaagent:/Applications/IntelliJ IDEA.app/Contents/bin/JetbrainsCrack.jar Idea内存显示]]></content>
      <categories>
        <category>Idea</category>
      </categories>
      <tags>
        <tag>Idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lombok]]></title>
    <url>%2Fposts%2F680b%2F</url>
    <content type="text"></content>
      <categories>
        <category>Lombok</category>
      </categories>
      <tags>
        <tag>Lombok</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[签证准备]]></title>
    <url>%2Fposts%2F7bfd%2F</url>
    <content type="text"><![CDATA[[x] 签证申请表复印件 [x] 护照（原件和复印件） [x] 2寸免冠近照（2张） [x] 医疗保险（原件和复印件） [x] 往返机票（复印件） [x] 住宿证明（复印件） [ ] 详细的行程单（复印件） [ ] 近3个月银行卡流水 [x] 户口本（复印件） [x] 工作单位证明 [x] 公司营业执照盖章（复印件） http://www.vfsglobal.ch/switzerland/china/ KMSW3230503404]]></content>
      <categories>
        <category>Travel</category>
      </categories>
      <tags>
        <tag>Travel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo分类标签中关于大小写的bug]]></title>
    <url>%2Fposts%2F3077%2F</url>
    <content type="text"><![CDATA[描述1当我的分类标签写的是Scrapy时，打开我的博客找到Scrapy标签，点击Scrapy却出现404页面。 当我把标签改为scrapy小写，再发布到网上，点击scrapy就不会出现404问题。 后来发现原来是git标签生成时忽略了大写，生成的实际标签为scrapy。 于是我来到我的Github中，找到Gladysgong.github.io/categories/爬虫/这个目录，发现实际生成的也是scrapy，所以 原因就在这里了。 解决12345678910111213修改文件： cd blog/.deploy_git vi .git/config 将ignorecase=true改为ignorecase=false删除Gladysgong.github.io中的文件并提交： git rm -rm * git commit -m &quot;clean all files&quot; git pushHexo再次生成及部署： cd .. hexo clean hexo g hexo d]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java面试--基础篇]]></title>
    <url>%2Fposts%2Fbdc1%2F</url>
    <content type="text"><![CDATA[抽象和封装的不同点抽象和封装是互补的概念。一方面，抽象关注对象的行为。另一方面，封装关注对象行为的细节。一般是通过隐藏对象内部状态信息做到封装，因此，封装可以看成是用来提供抽象的一种策略。 重载和重写的区别重载： 发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同，发生在编译时。 重写： 发生在父子类中，方法名、参数列表必须相同，返回值范围小于等于父类，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类；如果父类方法访问修饰符为private则子类就不能重写该方法。 字符型常量和字符串常量的区别字符常量是单引号引起的一个字符 字符串常量是双引号引起的若干个字符字符常量相当于一个整形值(ASCII值),可以参加表达式运算 字符串常量代表一个地址值(该字符串在内存中存放位置)字符常量只占一个字节 字符串常量占若干个字节(至少一个字符结束标志)4.成员变量与局部变量的区别有那些？ 从语法形式上，看成员变量是属于类的，而局部变量是在方法中定义的变量或是方法的参数；成员变量可以被public,private,static等修饰符所修饰，而局部变量不能被访问控制修饰符及static所修饰；但是，成员变量和局部变量都能被final所修饰；从变量在内存中的存储方式来看，成员变量是对象的一部分，而对象存在于堆内存，局部变量存在于栈内存从变量在内存中的生存时间上看，成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动消失。成员变量如果没有被赋初值，则会自动以类型的默认值而赋值（一种情况例外被final修饰但没有被static修饰的成员变量必须显示地赋值）；而局部变量则不会自动赋值。5.讲讲对static的理解？Java中是否可以覆盖一个private或者是static的方法？ 如果一个类的变量或者方法前面有static修饰，那么表明这个方法或者变量属于这个类，也就是说可以在不创建对象的情况下直接使用 当父类的方法被private修饰时，表明该方法为父类私有，对其他任何类都是不可见的，因此如果子类定了一个与父类一样的方法，这对于子类来说相当于是一个新的私有方法，且如果要进行向上转型，然后去调用该“覆盖方法”，会产生编译错误 static方法时编译时静态绑定的，属于类，而覆盖是运行时动态绑定的(动态绑定的多态),因此不能覆盖. 是否可以在static环境中访问非static变量？static变量在Java中是属于类的，它在所有的实例中的值是一样的。 当类被Java虚拟机载入的时候，会对static变量进行初始化。 如果代码尝试不用实例来访问非static的变量，编译器会报错，因为这些变量还没有被创建出来，还没有跟任何实例关联上。 Java支持的基本数据类型有哪些？java支持的基本数据类型有以下9种:byte,shot,int,long,float,double,char,boolean,void. 怎么理解JAVA的自动拆箱装箱？所谓自动装箱就是将基本数据类型自动的转换为对应的对象包装类型，而拆箱就是将对象包装类型转换为基本数据类型。 java中的自动拆装箱通常发生在变量赋值的过程中，如：把int转化成Integer，double转化成double就是自动装箱，反之就是自动拆箱 在实际中，应该注意自动拆装箱，因为有时可能因为java自动装箱机制，而导致创建了许多对象，对于内存小的平台会造成压力。 重写和重载是什么?重写：发生在子类与父类之间，表示子类中的方法可以与父类中的某个方法的名称和参数完全相同，通过子类创建的实例对象调用这个方法时，将调用子类中的定义方法，这相当于把父类中定义的那个完全相同的方法给覆盖了，这也是面向对象编程的多态性的一种表现。重载：是指在一个类中，可以有多个相同名称的方法，但是他们的参数列表的个数或类型不同，当调用该方法时，根据传递的参数类型调用对应参数列表的方法。当参数列表相同但返回值不同时，将会出现编译错误，这并不是重载，因为jvm无法根据返回值类型来判断应该调用哪个方法。10.Java支持多继承么？如果不支持，如何实现? 不支持，Java不支持多继承。每个类都只能继承一个类，但是可以实现多个接口。 在java中是单继承的，也就是说一个类只能继承一个父类。 java中实现多继承有两种方式,一是接口，而是内部类. 什么是值传递和引用传递？Java中是值传递还是引用传递，还是都有?值传递：就是在方法调用的时候，实参是将自己的一份拷贝赋给形参，在方法内，对该参数值的修改不影响原来实参。引用传递：是在方法调用的时候，实参将自己的地址传递给形参，此时方法内对该参数值的改变，就是对该实参的实际操作。在java中只有一种传递方式，那就是值传递.可能比较让人迷惑的就是java中的对象传递时，对形参的改变依然会影响到该对象的内容。 接口和抽象类的区别是什么?接口中所有的方法隐含的都是抽象的。而抽象类则可以同时包含抽象和非抽象的方法。类可以实现很多个接口，但是只能继承一个抽象类类如果要实现一个接口，它必须要实现接口声明的所有方法。但是，类可以不实现抽象类声明的所有方法，当然，在这种情况下，类也必须得声明成是抽象的。抽象类可以在不提供接口方法实现的情况下实现接口。Java 接口中声明的变量默认都是 final 的。抽象类可以包含非 final 的变量。Java 接口中的成员函数默认是 public 的。抽象类的成员函数可以是 private，protected 或者是 public 。接口是绝对抽象的，不可以被实例化(java 8已支持在接口中实现默认的方法)。抽象类也不可以被实例化，但是，如果它包含 main 方法的话是可以被调用的。13.构造器（constructor）是否可被重写（override）? 构造方法是不能被子类重写的，但是构造方法可以重载 简单的讲，就是说一个类可以有多个构造方法。 String, StringBuffer StringBuilder的区别String 的长度是不可变的；StringBuffer的长度是可变的，线程安全；如果对一个字符串要经常改变的话，就一定不要用String,否则会创建许多无用的对象出来. HashMap的工作原理是什么?HashMap内部是通过一个数组实现的，只是这个数组比较特殊，数组里存储的元素是一个Entry实体(在JAVA8中为Node)，这个Entry实体主要包含key、value以及一个指向自身的next指针。 HashMap是基于hashing实现的，当进行put操作时，根据传递的key值得到它的hashcode，然后再用这个hashcode与数组的长度进行模运算，得到一个int值，就是Entry要存储在数组的位置（下标）；当通过get方法获取指定key的值时，会根据这个key算出它的hash值（数组下标），根据这个hash值获取数组下标对应的Entry，然后判断Entry里的key，hash值或者通过equals()比较是否与要查找的相同，如果相同，返回value，否则的话，遍历该链表（有可能就只有一个Entry，此时直接返回null），直到找到为止，否则返回null。 HashMap之所以在每个数组元素存储的是一个链表，是为了解决hash冲突问题，当两个对象的hash值相等时，那么一个位置肯定是放不下两个值的，于是hashmap采用链表来解决这种冲突，hash值相等的两个元素会形成一个链表。 HashMap与Hashtable的区别是什么?Hashtable基于Dictionary类，而HashMap是基于AbstractMap。Dictionary是任何可将键映射到相应值的类的抽象父类，而AbstractMap是基于Map接口的实现，它以最大限度地减少实现此接口所需的工作。HashMap和Hashtable都实现了Map接口，Hashtable基于Dictionary类，而HashMap是基于AbstractMap。Dictionary是任何可将键映射到相应值的类的抽象父类，而AbstractMap是基于Map接口的实现，它以最大限度地减少实现此接口所需的工作。HashMap允许键和值是null，而Hashtable不允许键或者值是null。Hashtable是同步(线程安全)的，而HashMap不是同步(非线程安全)。因此，HashMap更适合于单线程环境，而Hashtable适合于多线程环境。HashMap提供了可供应用迭代的键的集合，因此，HashMap是快速失败的。另一方面，Hashtable提供了对键的列举(Enumeration)。 CorrentHashMap的工作原理ConcurrenHashMap说是HashMap的升级版 ConcurrentHashMap是线程安全的，但是与Hashtable相比，实现线程安全的方式不同。 Hashtable是通过对hash表结构进行锁定，是阻塞式的，当一个线程占有这个锁时，其他线程必须阻塞等待其释放锁。 ConcurrentHashMap是采用分离锁的方式，它并没有对整个hash表进行锁定，而是局部锁定，也就是说当一个线程占有这个局部锁时，不影响其他线程对hash表其他地方的访问。 ConcurrentHashMap内部有一个Segment&lt;K,V&gt;数组,该Segment对象可以充当锁。Segment对象内部有一个HashEntry&lt;K,V&gt;数组，于是每个Segment可以守护若干个桶(HashEntry),每个桶又有可能是一个HashEntry连接起来的链表，存储发生碰撞的元素。 每个ConcurrentHashMap在默认并发级下会创建包含16个Segment对象的数组，每个数组有若干个桶，当进行put方法时，通过hash方法对key进行计算，得到hash值，找到对应的segment，然后对该segment进行加锁，然后调用segment的put方法进行存储操作，此时其他线程就不能访问当前的segment，但可以访问其他的segment对象，不会发生阻塞等待。 在Java8中，ConcurrentHashMap不再使用Segment分离锁，而是采用一种乐观锁CAS算法来实现同步问题，但其底层还是“数组+链表-&gt;红黑树”的实现。 Array和ArrayList有什么区别？Array可以容纳基本类型和对象，而ArrayList只能容纳对象。Array是指定大小的，而ArrayList大小是固定的Array可以包含基本类型和对象类型，ArrayList只能包含对象类型。Array大小是固定的，ArrayList的大小是动态变化的。19.ArrayList和LinkedList有什么区别？ ArrayList和LinkedList都实现了List接口ArrayList是基于数组实现，它的底层是数组。它可以以O(1)时间复杂度对元素进行随机访问。LinkedList是基于链表实现，每一个元素都和它的前一个和后一个元素链接在一起，在这种情况下，查找某个元素的时间复杂度是O(n)。ArrayList在查找时速度快LinkedList的插入，添加，删除操作速度更快，因为当元素被添加到集合任意位置的时候，不需要像数组那样重新计算大小或者是更新索引。LinkedList比ArrayList更占内存，因为LinkedList为每一个节点存储了两个引用，一个指向前一个元素，一个指向下一个元素。 哪些集合类提供对元素的随机访问？ArrayList、HashMap、TreeMap和HashTable类提供对元素的随机访问。 HashSet的底层实现是什么?HashSet的实现是依赖于HashMap的，HashSet的值都是存储在HashMap中的。 在HashSet的构造法中会初始化一个HashMap对象，HashSet不允许值重复。 因此，HashSet的值是作为HashMap的key存储在HashMap中的，当存储的值已经存在时返回false。 Comparable和Comparator接口的区别。Comparable接口只包含一个compareTo()方法。这个方法可以个给两个对象排序。具体来说，它返回负数，0，正数来表明输入对象小于，等于，大于已经存在的对象。Comparator接口包含compare()和equals()两个方法。compare()方法用来给两个输入参数排序，返回负数，0，正数表明第一个参数是小于，等于，大于第二个参数。 equals()方法需要一个对象作为参数，它用来决定输入参数是否和comparator相等。只有当输入参数也是一个comparator并且输入参数和当前comparator的排序结果是相同的时候，这个方法才返回true。 HashSet和TreeSet有什么区别？HashSet是由一个hash表来实现的，因此，它的元素是无序的。add()，remove()，contains()方法的时间复杂度是O(1)。TreeSet是由一个树形的结构来实现的，它里面的元素是有序的。因此，add()，remove()，contains()方法的时间复杂度是O(logn)。 Java中==与equals的区别“==” 的作用是判断两个对象的地址是不是相等。即判断两个对象是不是同一个对象。(基本数据类型==比较的是值，引用数据类型==比较的是内存地址)equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况：类没有覆盖equals()方法。则通过equals()比较该类的两个对象时，等价于通过“==”比较这两个对象。类覆盖了equals()方法。一般，我们都覆盖equals()方法来两个对象的内容相等；若它们的内容相等，则返回true(即，认为这两个对象相等)。 你重写过 hashcode 和 equals 么，为什么重写equals时必须重写hashCode方法？hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。如果两个对象相等，则hashcode一定也是相同的如果两个对象相等,对两个对象分别调用equals方法都返回true如果两个对象有相同的hashcode值，它们也不一定是相等的因此，equals方法被覆盖过，则hashCode方法也必须被覆盖 hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） Java的四种引用，强弱软虚，用到的场景强引用：如果一个对象具有强引用，它就不会被垃圾回收器回收。即使当前内存空间不足，JVM也不会回收它，而是抛出 OutOfMemoryError 错误，使程序异常终止。如果想中断强引用和某个对象之间的关联，可以显式地将引用赋值为null，这样一来的话，JVM在合适的时间就会回收该对象软引用：在使用软引用时，如果内存的空间足够，软引用就能继续被使用，而不会被垃圾回收器回收，只有在内存不足时，软引用才会被垃圾回收器回收。弱引用：具有弱引用的对象拥有的生命周期更短暂。因为当 JVM 进行垃圾回收，一旦发现弱引用对象，无论当前内存空间是否充足，都会将弱引用回收。不过由于垃圾回收器是一个优先级较低的线程，所以并不一定能迅速发现弱引用对象虚引用：顾名思义，就是形同虚设，如果一个对象仅持有虚引用，那么它相当于没有引用，在任何时候都可能被垃圾回收器回收。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Innotop]]></title>
    <url>%2Fposts%2F9948%2F</url>
    <content type="text"><![CDATA[mac安装1brew install innotop 环境变量配置编辑配置文件 1vi ~/.bash_profile 添加配置 123export LDFLAGS="-L/usr/local/opt/mysql-client/lib"export CPPFLAGS="-I/usr/local/opt/mysql-client/include"export PATH="/usr/local/opt/mysql-client/bin:$PATH" 使配置文件生效 1source ~/.bash_profile]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql优化配置]]></title>
    <url>%2Fposts%2Fc1b1%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155[client]port = 3306 socket = /var/lib/mysql/mysql.sock[mysql]#这个配置段设置启动MySQL服务的条件；在这种情况下，no-auto-rehash确保这个服务启动得比较快。no-auto-rehash[mysqld]user = mysql port = 3306 socket = /var/lib/mysql/mysql.sock basedir = /usr/local/mysql datadir = /data/mysql/data/ open_files_limit = 10240back_log = 600 #在MYSQL暂时停止响应新请求之前，短时间内的多少个请求可以被存在堆栈中。如果系统在短时间内有很多连接，则需要增大该参数的值，该参数值指定到来的TCP/IP连接的监听队列的大小。默认值80。max_connections = 3000 #MySQL允许最大的进程连接数，如果经常出现Too Many Connections的错误提示，则需要增大此值。默认151max_connect_errors = 6000 #设置每个主机的连接请求异常中断的最大次数，当超过该次数，MYSQL服务器将禁止host的连接请求，直到mysql服务器重启或通过flush hosts命令清空此host的相关信息。默认100external-locking = FALSE #使用–skip-external-locking MySQL选项以避免外部锁定。该选项默认开启max_allowed_packet = 32M #设置在网络传输中一次消息传输量的最大值。系统默认值 为4MB，最大值是1GB，必须设置1024的倍数。#sort_buffer_size = 2M # Sort_Buffer_Size 是一个connection级参数，在每个connection（session）第一次需要使用这个buffer的时候，一次性分配设置的内存。#Sort_Buffer_Size 并不是越大越好，由于是connection级的参数，过大的设置+高并发可能会耗尽系统内存资源。例如：500个连接将会消耗 500*sort_buffer_size(8M)=4G内存#Sort_Buffer_Size 超过2KB的时候，就会使用mmap() 而不是 malloc() 来进行内存分配，导致效率降低。 系统默认2M，使用默认值即可#join_buffer_size = 2M #用于表间关联缓存的大小，和sort_buffer_size一样，该参数对应的分配内存也是每个连接独享。系统默认2M，使用默认值即可thread_cache_size = 300 #默认38# 服务器线程缓存这个值表示可以重新利用保存在缓存中线程的数量,当断开连接时如果缓存中还有空间,那么客户端的线程将被放到缓存中,如果线程重新被请求，那么请求将从缓存中读取,如果缓存中是空的或者是新的请求，那么这个线程将被重新创建,如果有很多新的线程，增加这个值可以改善系统性能.通过比较 Connections 和 Threads_created 状态的变量，可以看到这个变量的作用。设置规则如下：1GB 内存配置为8，2GB配置为16，3GB配置为32，4GB或更高内存，可配置更大。#thread_concurrency = 8 #系统默认为10，使用10先观察# 设置thread_concurrency的值的正确与否, 对mysql的性能影响很大, 在多个cpu(或多核)的情况下，错误设置了thread_concurrency的值, 会导致mysql不能充分利用多cpu(或多核), 出现同一时刻只能一个cpu(或核)在工作的情况。thread_concurrency应设为CPU核数的2倍. 比如有一个双核的CPU, 那么thread_concurrency的应该为4; 2个双核的cpu, thread_concurrency的值应为8query_cache_size = 64M #在MyISAM引擎优化中，这个参数也是一个重要的优化参数。但也爆露出来一些问题。机器的内存越来越大，习惯性把参数分配的值越来越大。这个参数加大后也引发了一系列问题。我们首先分析一下 query_cache_size的工作原理：一个SELECT查询在DB中工作后，DB会把该语句缓存下来，当同样的一个SQL再次来到DB里调用时，DB在该表没发生变化的情况下把结果从缓存中返回给Client。这里有一个关建点，就是DB在利用Query_cache工作时，要求该语句涉及的表在这段时间内没有发生变更。那如果该表在发生变更时，Query_cache里的数据又怎么处理呢？首先要把Query_cache和该表相关的语句全部置为失效，然后在写入更新。那么如果Query_cache非常大，该表的查询结构又比较多，查询语句失效也慢，一个更新或是Insert就会很慢，这样看到的就是Update或是Insert怎么这么慢了。所以在数据库写入量或是更新量也比较大的系统，该参数不适合分配过大。而且在高并发，写入量大的系统，建议把该功能禁掉。query_cache_limit = 4M #指定单个查询能够使用的缓冲区大小，缺省为1Mquery_cache_min_res_unit = 2k #默认是4KB，设置值大对大数据查询有好处，但如果你的查询都是小数据查询，就容易造成内存碎片和浪费#查询缓存碎片率 = Qcache_free_blocks / Qcache_total_blocks * 100%#如果查询缓存碎片率超过20%，可以用FLUSH QUERY CACHE整理缓存碎片，或者试试减小query_cache_min_res_unit，如果你的查询都是小数据量的话。#查询缓存利用率 = (query_cache_size – Qcache_free_memory) / query_cache_size * 100%#查询缓存利用率在25%以下的话说明query_cache_size设置的过大，可适当减小;查询缓存利用率在80%以上而且Qcache_lowmem_prunes &gt; 50的话说明query_cache_size可能有点小，要不就是碎片太多。#查询缓存命中率 = (Qcache_hits – Qcache_inserts) / Qcache_hits * 100%#default-storage-engine = MyISAM#default_table_type = InnoDB #开启失败#thread_stack = 192K #设置MYSQL每个线程的堆栈大小，默认值足够大，可满足普通操作。可设置范围为128K至4GB，默认为256KB，使用默认观察transaction_isolation = READ-COMMITTED # 设定默认的事务隔离级别.可用的级别如下:READ UNCOMMITTED-读未提交 READ COMMITTE-读已提交 REPEATABLE READ -可重复读 SERIALIZABLE -串行tmp_table_size = 256M # tmp_table_size 的默认大小是 32M。如果一张临时表超出该大小，MySQL产生一个 The table tbl_name is full 形式的错误，如果你做很多高级 GROUP BY 查询，增加 tmp_table_size 值。如果超过该值，则会将临时表写入磁盘。max_heap_table_size = 256Mexpire_logs_days = 7 key_buffer_size = 2048M #批定用于索引的缓冲区大小，增加它可以得到更好的索引处理性能，对于内存在4GB左右的服务器来说，该参数可设置为256MB或384MB。read_buffer_size = 1M #默认128K# MySql读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySql会为它分配一段内存缓冲区。read_buffer_size变量控制这一缓冲区的大小。如果对表的顺序扫描请求非常频繁，并且你认为频繁扫描进行得太慢，可以通过增加该变量值以及内存缓冲区大小提高其性能。和sort_buffer_size一样，该参数对应的分配内存也是每个连接独享。read_rnd_buffer_size = 16M # MySql的随机读（查询操作）缓冲区大小。当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。进行排序查询时，MySql会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。但MySql会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。bulk_insert_buffer_size = 64M #批量插入数据缓存大小，可以有效提高插入效率，默认为8Mmyisam_sort_buffer_size = 128M # MyISAM表发生变化时重新排序所需的缓冲 默认8Mmyisam_max_sort_file_size = 10G # MySQL重建索引时所允许的最大临时文件的大小 (当 REPAIR, ALTER TABLE 或者 LOAD DATA INFILE).# 如果文件大小比此值更大,索引会通过键值缓冲创建(更慢)#myisam_max_extra_sort_file_size = 10G 5.6无此值设置#myisam_repair_threads = 1 默认为1# 如果一个表拥有超过一个索引, MyISAM 可以通过并行排序使用超过一个线程去修复他们.# 这对于拥有多个CPU以及大量内存情况的用户,是一个很好的选择.myisam_recover #自动检查和修复没有适当关闭的 MyISAM 表skip-name-resolve lower_case_table_names = 1 server-id = 1innodb_additional_mem_pool_size = 16M #这个参数用来设置 InnoDB 存储的数据目录信息和其它内部数据结构的内存池大小，类似于Oracle的library cache。这不是一个强制参数，可以被突破。innodb_buffer_pool_size = 2048M # 这对Innodb表来说非常重要。Innodb相比MyISAM表对缓冲更为敏感。MyISAM可以在默认的 key_buffer_size 设置下运行的可以，然而Innodb在默认的 innodb_buffer_pool_size 设置下却跟蜗牛似的。由于Innodb把数据和索引都缓存起来，无需留给操作系统太多的内存，因此如果只需要用Innodb的话则可以设置它高达 70-80% 的可用内存。一些应用于 key_buffer 的规则有 — 如果你的数据量不大，并且不会暴增，那么无需把 innodb_buffer_pool_size 设置的太大了#innodb_data_file_path = ibdata1:1024M:autoextend 设置过大导致报错，默认12M观察#表空间文件 重要数据#innodb_file_io_threads = 4 不明确，使用默认值#文件IO的线程数，一般为 4，但是在 Windows 下，可以设置得较大。innodb_thread_concurrency = 8 #服务器有几个CPU就设置为几，建议用默认设置，一般为8.innodb_flush_log_at_trx_commit = 2 # 如果将此参数设置为1，将在每次提交事务后将日志写入磁盘。为提供性能，可以设置为0或2，但要承担在发生故障时丢失数据的风险。设置为0表示事务日志写入日志文件，而日志文件每秒刷新到磁盘一次。设置为2表示事务日志将在提交时写入日志，但日志文件每次刷新到磁盘一次。#innodb_log_buffer_size = 16M 使用默认8M#此参数确定些日志文件所用的内存大小，以M为单位。缓冲区更大能提高性能，但意外的故障将会丢失数据.MySQL开发人员建议设置为1－8M之间#innodb_log_file_size = 128M 使用默认48M#此参数确定数据日志文件的大小，以M为单位，更大的设置可以提高性能，但也会增加恢复故障数据库所需的时间#innodb_log_files_in_group = 3 使用默认2#为提高性能，MySQL可以以循环方式将日志文件写到多个文件。推荐设置为3M#innodb_max_dirty_pages_pct = 90 使用默认75观察#推荐阅读 http://www.taobaodba.com/html/221_innodb_max_dirty_pages_pct_checkpoint.html# Buffer_Pool中Dirty_Page所占的数量，直接影响InnoDB的关闭时间。参数innodb_max_dirty_pages_pct 可以直接控制了Dirty_Page在Buffer_Pool中所占的比率，而且幸运的是innodb_max_dirty_pages_pct是可以动态改变的。所以，在关闭InnoDB之前先将innodb_max_dirty_pages_pct调小，强制数据块Flush一段时间，则能够大大缩短 MySQL关闭的时间。innodb_lock_wait_timeout = 120 #默认为50秒 # InnoDB 有其内置的死锁检测机制，能导致未完成的事务回滚。但是，如果结合InnoDB使用MyISAM的lock tables 语句或第三方事务引擎,则InnoDB无法识别死锁。为消除这种可能性，可以将innodb_lock_wait_timeout设置为一个整数值，指示 MySQL在允许其他事务修改那些最终受事务回滚的数据之前要等待多长时间(秒数)innodb_file_per_table = 0 #默认为No#独享表空间（关闭）[mysqldump]quick # max_allowed_packet = 32M[mysqld_safe]log-error=/data/mysql/mysql_oldboy.err pid-file=/data/mysql/mysqld.pidsql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intellij IDEA 去掉@AutoWired注入bean报错]]></title>
    <url>%2Fposts%2Fb77a%2F</url>
    <content type="text"><![CDATA[有时开发项目时，会遇到注入的bean报红，但是并不影响运行。 解决方案：调整IDE对于@Autowired的检查级别]]></content>
      <categories>
        <category>Idea</category>
      </categories>
      <tags>
        <tag>Idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RESTful API设计规范]]></title>
    <url>%2Fposts%2F8e30%2F</url>
    <content type="text"></content>
      <categories>
        <category>Other</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Other</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OAuth 2.0 协议]]></title>
    <url>%2Fposts%2Ffec1%2F</url>
    <content type="text"><![CDATA[什么是 OAuth 2.0OAuth 2.0 是一个行业的标准授权协议。OAuth 2.0 专注于简化客户端开发人员，同时为 Web 应用程序，桌面应用程序，手机和客厅设备提供特定的授权流程。 它的最终目的是为第三方应用颁发一个有时效性的令牌 token。使得第三方应用能够通过该令牌获取相关的资源。常见的场景就是：第三方登录。当你想要登录某个论坛，但没有账号，而这个论坛接入了如 QQ、Facebook 等登录功能，在你使用 QQ 登录的过程中就使用的 OAuth 2.0 协议。 如果你想了解更多，其官方网址为：https://oauth.net/2/。下面我们来了解下 OAuth 协议的基本原理 角色首先需要介绍的是 OAuth 2.0 协议中的一些角色，整个授权协议的流程都将围绕着这些角色： resource owner，资源所有者，能够允许访问受保护资源的实体。如果是个人，被称为 end-user。 resource server，资源服务器，托管受保护资源的服务器。 client，客户端，使用资源所有者的授权代表资源所有者发起对受保护资源的请求的应用程序。如：web网站，移动应用等。 authorization server，授权服务器，能够向客户端颁发令牌。 user-agent，用户代理，帮助资源所有者与客户端沟通的工具，一般为 web 浏览器，移动 APP 等。 可能有些朋友依然不太理解，这里举例说明：假如我想要在 coding.net 这个网站上用 github.com 的账号登录。那么 coding 相对于 github 就是一个客户端。而我们用什么操作的呢？浏览器，这就是一个用户代理。当从 github 的授权服务器获得 token 后，coding 是需要请求 github 账号信息的，从哪请求？从 github 的资源服务器。 协议流程 上图详细的描述了这四个角色之间的步骤流程： (A) Client 请求 Resource Owner 的授权。授权请求可以直接向 Resource Owner 请求，也可以通过 Authorization Server 间接的进行。 (B) Client 获得授权许可。 © Client 向 Authorization Server 请求访问令牌。 (D) Authorization Server 验证授权许可，如果有效则颁发访问令牌。 (E) Client 通过访问令牌从 Resource Server 请求受保护资源。 (F) Resource Server 验证访问令牌，有效则响应请求。 12345678910111213141516171819+--------+ +---------------+| |--(A)- Authorization Request -&gt;| Resource || | | Owner || |&lt;-(B)-- Authorization Grant ---| || | +---------------+| || | +---------------+| |--(C)-- Authorization Grant --&gt;| Authorization || Client | | Server || |&lt;-(D)----- Access Token -------| || | +---------------+| || | +---------------+| |--(E)----- Access Token ------&gt;| Resource || | | Server || |&lt;-(F)--- Protected Resource ---| |+--------+ +---------------+ Figure 1: Abstract Protocol Flow 授权一个客户端想要获得授权，就需要先到服务商那注册你的应用。一般需要你提供下面这些信息： 应用名称 应用网站 重定向 URI 或回调 URL（redirect_uri） 重定向 URI 是服务商在用户授权（或拒绝）应用程序之后重定向用户的地址，因此也是用于处理授权代码或访问令牌的应用程序的一部分。在你注册成功之后，你会从服务商那获取到你的应用相关的信息： 客户端标识 client_id 客户端密钥 client_secret client_id 用来表识客户端（公开），client_secret 用来验证客户端身份（保密）。 授权类型OAuth 2.0 列举了四种授权类型，分别用于不同的场景： Authorization Code（授权码 code）：服务器与客户端配合使用。 Implicit（隐式 token）：用于移动应用程序或 Web 应用程序（在用户设备上运行的应用程序）。 Resource Owner Password Credentials（资源所有者密码凭证 password）：资源所有者和客户端之间具有高度信任时（例如，客户端是设备的操作系统的一部分，或者是一个高度特权应用程序），以及当其他授权许可类型（例如授权码）不可用时被使用。 Client Credentials（客户端证书 client_credentials）：当客户端代表自己表演（客户端也是资源所有者）或者基于与授权服务器事先商定的授权请求对受保护资源的访问权限时，客户端凭据被用作为授权许可。 下面来具体说说这四种授权。注意重定向一定要用 302。 授权码模式 该方式需要资源服务器的参与，应用场景大概是： 资源拥有者（用户）需要登录客户端（APP），他选择了第三方登录。 客户端（APP）重定向到第三方授权服务器。此时客户端携带了客户端标识（client_id），那么第三方就知道这是哪个客户端，资源拥有者确定（拒绝）授权后需要重定向到哪里。 用户确认授权，客户端（APP）被重定向到注册时给定的 URI，并携带了第三方给定的 code。 在重定向的过程中，客户端拿到 code 与 client_id、client_secret 去授权服务器请求令牌，如果成功，直接请求资源服务器获取资源，整个过程，用户代理是不会拿到令牌 token 的。 客户端（APP）拿到令牌 token 后就可以向第三方的资源服务器请求资源了。 123456789101112131415161718192021222324252627282930 +----------+ | Resource | | Owner | | | +----------+ ^ | (B) +----|-----+ Client Identifier +---------------+ | -+----(A)-- &amp; Redirection URI ----&gt;| | | User- | | Authorization | | Agent -+----(B)-- User authenticates ---&gt;| Server | | | | | | -+----(C)-- Authorization Code ---&lt;| | +-|----|---+ +---------------+ | | ^ v (A) (C) | | | | | | ^ v | | +---------+ | | | |&gt;---(D)-- Authorization Code ---------&apos; | | Client | &amp; Redirection URI | | | | | |&lt;---(E)----- Access Token -------------------&apos; +---------+ (w/ Optional Refresh Token)Note: The lines illustrating steps (A), (B), and (C) are broken intotwo parts as they pass through the user-agent. Figure 3: Authorization Code Flow 具体说明，这里以 coding 和 github 为例。当我想在 coding 上通过 github 账号登录时： 1、GET 请求 点击登录，重定向到 github 的授权端点： 12345https://github.com/login/oauth/authorize? response_type=code&amp; client_id=a5ce5a6c7e8c39567ca0&amp; redirect_uri=https://coding.net/api/oauth/github/callback&amp; scope=user:email 字段 描述 response_type 必须，固定为 code，表示这是一个授权码请求。 client_id 必须，在 github 注册获得的客户端 ID。 redirect_uri 可选，通过客户端注册的重定向 URI（一般要求且与注册时一致）。 scope 可选，请求资源范围，多个空格隔开。 state 可选（推荐），如果存在，原样返回给客户端。 返回值： 1https://coding.net/api/oauth/github/callback?code=fb6a88dc09e843b33f 字段 描述 code 必须。授权码 state 如果出现在请求中，必须包含。 授权错误 第一种，客户端没有被识别或错误的重定向 URI，授权服务器没有必要重定向资源拥有者到重定向URI，而是通知资源拥有者发生了错误。 第二种，客户端被正确地授权了，但是其他某些事情失败了。这种情况下下面地错误响应会被发送到客户端，包括在重定向 URI 中。 1234https://coding.net/api/oauth/github/callback? error=redirect_uri_mismatch&amp; error_description=The+redirect_uri+MUST+match+the+registered+callback+URL+for+this+application.&amp; error_uri=https%3A%2F%2Fdeveloper.github.com%2Fapps%2Fmanaging-oauth-apps%2Ftroubleshooting-authorization-request-errors%2F%23redirect-uri-mismatch 字段 描述 error 必须，必须是预先定义的错误码。 error_description 可选，错误描述 error_uri 可选，指向可解读错误的 URI state 必须，如果出现在授权请求中 2、POST 请求 获取令牌 token，当获取到授权码 code 后，客户端需要用它获取访问令牌： 123456https://github.com/login/oauth/access_token? client_id=a5ce5a6c7e8c39567ca0&amp; client_secret=xxxx&amp; grant_type=authorization_code&amp; code=fb6a88dc09e843b33f&amp; redirect_uri=https://coding.net/api/oauth/github/callback 字段 描述 client_id 必须，客户端标识。 client_secret 必须，客户端密钥。 grant_type 必须，固定为 authorization_code／refresh_token。 code 必须，上一步获取到的授权码。 redirect_uri 必须，完成授权后的回调地址，与注册时一致。 返回值： 123456&#123; &quot;access_token&quot;:&quot;a14afef0f66fcffce3e0fcd2e34f6ff4&quot;, &quot;token_type&quot;:&quot;bearer&quot;, &quot;expires_in&quot;:3920, &quot;refresh_token&quot;:&quot;5d633d136b6d56a41829b73a424803ec&quot;&#125; 字段 描述 access_token 这个就是最终获取到的令牌。 token_type 令牌类型，常见有 bearer/mac/token（可自定义）。 expires_in 失效时间。 refresh_token 刷新令牌，用来刷新 access_token。 3、获取资源服务器资源，拿着 access_token 就可以获取账号的相关信息了： 1curl -H &quot;Authorization: token a14afef0f66fcffce3e0fcd2e34f6ff4&quot; https://api.github.com/user 4、POST 请求 刷新令牌 我们的 access_token 是有时效性的，当在获取 github 用户信息时，如果返回 token 过期： 123456https://github.com/login/oauth/access_token? client_id=a5ce5a6c7e8c39567ca0&amp; client_secret=xxxx&amp; redirect_uri=https://coding.net/api/oauth/github/callback&amp; grant_type=refresh_token&amp; refresh_token=5d633d136b6d56a41829b73a424803ec 字段 描述 client_id 必须 client_secret 必须 redirect_uri 必须 grant_type 必须，固定为 refresh_token refresh_token 必须，上面获取到的 refresh_token 返回值： 123456&#123; &quot;access_token&quot;:&quot;a14afef0f66fcffce3e0fcd2e34f6ee4&quot;, &quot;token_type&quot;:&quot;bearer&quot;, &quot;expires_in&quot;:3920, &quot;refresh_token&quot;:&quot;4a633d136b6d56a41829b73a424803vd&quot;&#125; refresh_token 只有在 access_token 过期时才能使用，并且只能使用一次。当换取到的 access_token 再次过期时，使用新的 refresh_token 来换取 access_token 12345678910111213141516171819202122+--------+ +---------------+| |--(A)------- Authorization Grant ---------&gt;| || | | || |&lt;-(B)----------- Access Token -------------| || | &amp; Refresh Token | || | | || | +----------+ | || |--(C)---- Access Token ----&gt;| | | || | | | | || |&lt;-(D)- Protected Resource --| Resource | | Authorization || Client | | Server | | Server || |--(E)---- Access Token ----&gt;| | | || | | | | || |&lt;-(F)- Invalid Token Error -| | | || | +----------+ | || | | || |--(G)----------- Refresh Token -----------&gt;| || | | || |&lt;-(H)----------- Access Token -------------| |+--------+ &amp; Optional Refresh Token +---------------+ Figure 2: Refreshing an Expired Access Token 隐式模式 该方式一般用于移动客户端或网页客户端。隐式授权类似于授权码授权，但 token 被返回给用户代理再转发到客户端（APP），因此它可能会暴露给用户和用户设备上的其它客户端（APP）。此外，此流程不会对客户端（APP）的身份进行身份验证，并且依赖重定向 URI（已在服务商中注册）来实现此目的。 基本原理：要求用户授权应用程序，然后授权服务器将访问令牌传回给用户代理，用户代理将其传递给客户端。 12345678910111213141516171819202122232425262728293031323334353637 +----------+ | Resource | | Owner | | | +----------+ ^ | (B) +----|-----+ Client Identifier +---------------+ | -+----(A)-- &amp; Redirection URI ---&gt;| | | User- | | Authorization | | Agent -|----(B)-- User authenticates --&gt;| Server | | | | | | |&lt;---(C)--- Redirection URI ----&lt;| | | | with Access Token +---------------+ | | in Fragment | | +---------------+ | |----(D)--- Redirection URI ----&gt;| Web-Hosted | | | without Fragment | Client | | | | Resource | | (F) |&lt;---(E)------- Script ---------&lt;| | | | +---------------+ +-|--------+ | | (A) (G) Access Token | | ^ v +---------+ | | | Client | | | +---------+Note: The lines illustrating steps (A) and (B) are broken into twoparts as they pass through the user-agent. Figure 4: Implicit Grant Flow 1、同样以 coding 和 github 为例： 12345https://github.com/login/oauth/authorize? response_type=token&amp; client_id=a5ce5a6c7e8c39567ca0&amp; redirect_uri=https://coding.net/api/oauth/github/callback&amp; scope=user:email 字段 描述 response_type 必须，固定为 token。 client_id 必须。当客户端被注册时，有授权服务器分配的客户端标识。 redirect_uri 可选。由客户端注册的重定向URI。 scope 可选。请求可能的作用域。 state 可选(推荐)。任何需要被传递到客户端请求的URI客户端的状态。 返回值： 1234https://coding.net/api/oauth/github/callback# access_token=a14afef0f66fcffce3e0fcd2e34f6ff4&amp; token_type=token&amp; expires_in=3600 字段 描述 access_token 必须。授权服务器分配的访问令牌。 token_type 必须。令牌类型。 expires_in 推荐，访问令牌过期的秒数。 scope 可选，访问令牌的作用域。 state 必须，如果出现在授权请求期间，和请求中的 state 参数一样。 授权错误和上面一样 2、用户代理提取令牌 token 并提交给 coding 3、coding 拿到 token 就可以获取用户信息了 1curl -H &quot;Authorization: token a14afef0f66fcffce3e0fcd2e34f6ff4&quot; https://api.github.com/user 资源所有者密码模式 用户将其服务凭证（用户名和密码）直接提供给客户端，该客户端使用凭据从服务获取访问令牌。如果其它方式不可行，则只应在授权服务器上启用该授权类型。此外，只有在客户端受到用户信任时才能使用它（例如，它由服务商自有，或用户的桌面操作系统）。 12345678910111213141516171819+----------+| Resource || Owner || |+----------+ v | Resource Owner (A) Password Credentials | v+---------+ +---------------+| |&gt;--(B)---- Resource Owner -------&gt;| || | Password Credentials | Authorization || Client | | Server || |&lt;--(C)---- Access Token ---------&lt;| || | (w/ Optional Refresh Token) | |+---------+ +---------------+ Figure 5: Resource Owner Password Credentials Flow POST 请求 密码凭证流程 1https://oauth.example.com/token?grant_type=password&amp;username=USERNAME&amp;password=PASSWORD&amp;client_id=CLIENT_ID 字段 描述 grant_type 必须，固定为 password。 username 必须，UTF-8 编码的资源拥有者用户名。 password 必须，UTF-8 编码的资源拥有者密码。 scope 可选，授权范围。 返回值： 123456&#123; &quot;access_token&quot; : &quot;...&quot;, &quot;token_type&quot; : &quot;...&quot;, &quot;expires_in&quot; : &quot;...&quot;, &quot;refresh_token&quot; : &quot;...&quot;,&#125; 如果授权服务器验证成功，那么将直接返回令牌 token，改客户端已被授权。 客户端模式 这种模式只需要提供 client_id 和 client_secret 即可获取授权。一般用于后端 API 的相关操作。 123456789+---------+ +---------------+| | | || |&gt;--(A)- Client Authentication ---&gt;| Authorization || Client | | Server || |&lt;--(B)---- Access Token ---------&lt;| || | | |+---------+ +---------------+ Figure 6: Client Credentials Flow POST 请求 客户端凭证流程： 1https://oauth.example.com/token?grant_type=client_credentials&amp;client_id=CLIENT_ID&amp;client_secret=CLIENT_SECRET 字段 描述 grant_type 必须。必须设置到客户端证书中。 scope 可选。授权的作用域。 返回值 12345&#123; &quot;access_token&quot; : &quot;...&quot;, &quot;token_type&quot; : &quot;...&quot;, &quot;expires_in&quot; : &quot;...&quot;,&#125; 如果授权服务器验证成功，那么将直接返回令牌 token，改客户端已被授权。 参考网站[1] https://developers.douban.com/wiki/?title=oauth2 [2] https://www.digitalocean.com/community/tutorials/an-introduction-to-oauth-2 [3] https://tools.ietf.org/html/rfc6749 [4] https://oauth.net/2/ 本文链接：https://deepzz.com/post/what-is-oauth2-protocol.html]]></content>
      <categories>
        <category>OAuth2.0</category>
      </categories>
      <tags>
        <tag>OAuth2.0</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Interceptor VS Filter 拦截器和过滤器的区别]]></title>
    <url>%2Fposts%2F2ef0%2F</url>
    <content type="text"><![CDATA[Spring的Interceptor(拦截器)与Servlet的Filter有相似之处，都能实现权限检查、日志记录等。不同的是： Filter Interceptor Summary Filter 接口定义在 javax.servlet 包中 接口 HandlerInterceptor 定义在org.springframework.web.servlet 包中 Filter 定义在 web.xml 中 Filter在只在 Servlet 前后起作用。Filters 通常将 请求和响应（request/response） 当做黑盒子，Filter 通常不考虑servlet 的实现。 拦截器能够深入到方法前后、异常抛出前后等，因此拦截器的使用具有更大的弹性。允许用户介入（hook into）请求的生命周期，在请求过程中获取信息，Interceptor 通常和请求更加耦合。 在Spring构架的程序中，要优先使用拦截器。几乎所有 Filter 能够做的事情， interceptor 都能够轻松的实现 Filter 是 Servlet 规范规定的。 而拦截器既可以用于Web程序，也可以用于Application、Swing程序中。 使用范围不同 Filter 是在 Servlet 规范中定义的，是 Servlet 容器支持的。 而拦截器是在 Spring容器内的，是Spring框架支持的。 规范不同 Filter 不能够使用 Spring 容器资源 拦截器是一个Spring的组件，归Spring管理，配置在Spring文件中，因此能使用Spring里的任何资源、对象，例如 Service对象、数据源、事务管理等，通过IoC注入到拦截器即可 Spring 中使用 interceptor 更容易 Filter 是被 Server(like Tomcat) 调用 Interceptor 是被 Spring 调用 因此 Filter 总是优先于 Interceptor 执行 Interceptor 使用interceptor 的执行顺序大致为： 请求到达 DispatcherServlet DispatcherServlet 发送至 Interceptor ，执行 preHandle 请求达到 Controller 请求结束后，postHandle 执行 Spring 中主要通过 HandlerInterceptor 接口来实现请求的拦截，实现 HandlerInterceptor 接口需要实现下面三个方法： preHandle() – 在handler执行之前，返回 boolean 值，true 表示继续执行，false 为停止执行并返回。 postHandle() – 在handler执行之后, 可以在返回之前对返回的结果进行修改 afterCompletion() – 在请求完全结束后调用，可以用来统计请求耗时等等 统计请求耗时 12345678910111213141516171819202122232425262728293031323334353637383940414243import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.apache.log4j.Logger;import org.springframework.web.servlet.ModelAndView;import org.springframework.web.servlet.handler.HandlerInterceptorAdapter;public class ExecuteTimeInterceptor extends HandlerInterceptorAdapter&#123; private static final Logger logger = Logger.getLogger(ExecuteTimeInterceptor.class); //before the actual handler will be executed public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; long startTime = System.currentTimeMillis(); request.setAttribute("startTime", startTime); return true; &#125; //after the handler is executed public void postHandle( HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; long startTime = (Long)request.getAttribute("startTime"); long endTime = System.currentTimeMillis(); long executeTime = endTime - startTime; //modified the exisitng modelAndView modelAndView.addObject("executeTime",executeTime); //log it if(logger.isDebugEnabled())&#123; logger.debug("[" + handler + "] executeTime : " + executeTime + "ms"); &#125; &#125;&#125; 使用mvc:interceptors标签来声明需要加入到SpringMVC拦截器链中的拦截器 12345678910111213&lt;mvc:interceptors&gt; &lt;!-- 使用bean定义一个Interceptor，直接定义在mvc:interceptors根下面的Interceptor将拦截所有的请求 --&gt; &lt;bean class=&quot;com.company.app.web.interceptor.AllInterceptor&quot;/&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/**&quot;/&gt; &lt;mvc:exclude-mapping path=&quot;/parent/**&quot;/&gt; &lt;bean class=&quot;com.company.authorization.interceptor.SecurityInterceptor&quot; /&gt; &lt;/mvc:interceptor&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/parent/**&quot;/&gt; &lt;bean class=&quot;com.company.authorization.interceptor.SecuritySystemInterceptor&quot; /&gt; &lt;/mvc:interceptor&gt; &lt;/mvc:interceptors&gt; 可以利用mvc:interceptors标签声明一系列的拦截器，然后它们就可以形成一个拦截器链，拦截器的执行顺序是按声明的先后顺序执行的，先声明的拦截器中的preHandle方法会先执行，然而它的postHandle方法和afterCompletion方法却会后执行。 在mvc:interceptors标签下声明interceptor主要有两种方式： 直接定义一个Interceptor实现类的bean对象。使用这种方式声明的Interceptor拦截器将会对所有的请求进行拦截。 使用mvc:interceptor标签进行声明。使用这种方式进行声明的Interceptor可以通过mvc:mapping子标签来定义需要进行拦截的请求路径。 经过上述两步之后，定义的拦截器就会发生作用对特定的请求进行拦截了。 Filter 使用Servlet 的 Filter 接口需要实现如下方法： void init(FilterConfig paramFilterConfig) – 当容器初始化 Filter 时调用，该方法在 Filter 的生命周期只会被调用一次，一般在该方法中初始化一些资源，FilterConfig 是容器提供给 Filter 的初始化参数，在该方法中可以抛出 ServletException 。init 方法必须执行成功，否则 Filter 可能不起作用，出现以下两种情况时，web 容器中 Filter 可能无效： 1）抛出 ServletException 2）超过 web 容器定义的执行时间。 doFilter(ServletRequest paramServletRequest, ServletResponse paramServletResponse, FilterChain paramFilterChain) – Web 容器每一次请求都会调用该方法。该方法将容器的请求和响应作为参数传递进来， FilterChain 用来调用下一个 Filter。 void destroy() – 当容器销毁 Filter 实例时调用该方法，可以在方法中销毁资源，该方法在 Filter 的生命周期只会被调用一次。 Filter 和 Interceptor 的一些用途 Authentication Filters Logging and Auditing Filters Image conversion Filters Data compression Filters Encryption Filters Tokenizing Filters Filters that trigger resource access events XSL/T filters Mime-type chain Filter Request Filters 可以: 执行安全检查 perform security checks 格式化请求头和主体 reformat request headers or bodies 审查或者记录日志 audit or log requests 根据请求内容授权或者限制用户访问 Authentication-Blocking requests based on user identity. 根据请求频率限制用户访问 Response Filters 可以: 压缩响应内容,比如让下载的内容更小 Compress the response stream 追加或者修改响应 append or alter the response stream 创建或者整体修改响应 create a different response altogether 根据地方不同修改响应内容 Localization-Targeting the request and response to a particular locale.]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot Restful API拦截方式]]></title>
    <url>%2Fposts%2Fc914%2F</url>
    <content type="text"><![CDATA[SpringBoot RestFul API拦截大概有三种方式：Filter、Interceptor、Aspect。 Filter123456789101112131415161718192021222324252627import org.springframework.stereotype.Component;import javax.servlet.*;import java.io.IOException;import java.util.Date;@Componentpublic class TimerFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println("Time filter init"); &#125; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; System.out.println("Time filter start"); long startTime = new Date().getTime(); filterChain.doFilter(servletRequest, servletResponse); System.out.println("time filter:"+(new Date().getTime()-startTime)); System.out.println("time filter finish"); &#125; @Override public void destroy() &#123; System.out.println("Time filter destroy"); &#125;&#125; Interceptor123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import org.springframework.stereotype.Component;import org.springframework.web.method.HandlerMethod;import org.springframework.web.servlet.HandlerInterceptor;import org.springframework.web.servlet.ModelAndView;import javax.persistence.Convert;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.util.Date;/** * this is spring interceptor * * @author hsj * @create 2017-11-11 18:16 **/@Componentpublic class TimeInterceptor implements HandlerInterceptor &#123; /** * 控制器方法处理之前 * * @param httpServletRequest * @param httpServletResponse * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object handler) throws Exception &#123; System.out.println("preHandle"); System.out.println(((HandlerMethod) handler).getBean().getClass().getName()); System.out.println(((HandlerMethod) handler).getMethod().getName()); httpServletRequest.setAttribute("startTime", new Date().getTime()); return false; &#125; /** * 控制器方法处理之后 * 控制器方法调用不抛异常调用 * * @param httpServletRequest * @param httpServletResponse * @param o * @param modelAndView * @throws Exception */ @Override public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object handler, ModelAndView modelAndView) throws Exception &#123; System.out.println("postHandle"); Long startTime = (Long) httpServletRequest.getAttribute("startTime"); System.out.println("time interceptor 耗时" + (new Date().getTime() - startTime)); &#125; /** * 控制器方法抛不抛异常都会被调用 * * @param httpServletRequest * @param httpServletResponse * @param o * @param e * @throws Exception */ @Override public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception &#123; System.out.println("afterCompletion"); Long startTime = (Long) httpServletRequest.getAttribute("startTime"); System.out.println("time interceptor 耗时" + (new Date().getTime() - startTime)); System.out.println("ex is" + e); &#125;&#125; Aspect12345678910111213141516171819202122232425262728293031323334import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.springframework.stereotype.Component;import java.util.Date;/** * this is a acpect * 切入点 * 在那些方法上起作用 * 在什么时候起作用 * * @author hsj * @create 2017-11-11 20:52 **/@Aspect@Componentpublic class TimeAspect &#123; @Around("execution(* com.nbkj.controller.UserController.*(..))") public Object handleControllerMethod(ProceedingJoinPoint proceedingJoinPoint) throws Throwable &#123; System.out.println("time aspect start"); Object[] args = proceedingJoinPoint.getArgs(); for (Object arg : args) &#123; System.out.println(arg.getClass().getName()); System.out.println("arg is " + arg); &#125; long startTime = new Date().getTime(); Object obj = proceedingJoinPoint.proceed(); System.out.println("time aspect 耗时" + (new Date().getTime() - startTime)); System.out.println("time aspect end"); return obj; &#125;&#125; 总结 过滤器（Filter） ：可以拿到原始的http请求，但是拿不到你请求的控制器和请求控制器中的方法的信息。 拦截器（Interceptor）：可以拿到你请求的控制器和方法，却拿不到请求方法的参数。 切片 （Aspect） : 可以拿到方法的参数，但是却拿不到http请求和响应的对象]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot使用HandlerInterceptor拦截器]]></title>
    <url>%2Fposts%2Ff31b%2F</url>
    <content type="text"><![CDATA[编写一个拦截器，实现HandlerInterceptor接口 123456789101112131415161718192021222324252627282930313233343536373839/** * TimerInterceptor * * @author jlin * @date 2019-05-30 09:19 * @Description */@Component@Slf4jpublic class TimerInterceptor implements HandlerInterceptor &#123; /** * This implementation always returns &#123;@code true&#125;. * * @param request * @param response * @param handler */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; log.debug("收到请求--&gt;&#123;&#125;", request.getRequestURI()); request.setAttribute("startTime", System.currentTimeMillis()); return true; &#125; /** * 控制器方法抛不抛异常都会被调用 * * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; Long startTime = (Long) request.getAttribute("startTime"); log.debug("请求耗时--&gt;url:&#123;&#125;--&gt;time:&#123;&#125;,", request.getRequestURI(), (System.currentTimeMillis() - startTime)); &#125;&#125; 编写一个类，继承WebMvcConfigurerAdapter抽象类，将其放入Spring容器中，然后重写addInterceptors()方法，并调用给的参数InterceptorRegistry.addInterceptor()把自己编写的那个拦截器作为参数加进去。123456789101112131415161718/** * WebConfig * * @author jlin * @date 2019-05-29 16:12 * @Description */@Configuration@EnableWebMvcpublic class WebConfig implements WebMvcConfigurer &#123; @Autowired private TimerInterceptor timerInterceptor; @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(timerInterceptor); &#125;&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot 核心配置文件 bootstrap & application 详解]]></title>
    <url>%2Fposts%2F907%2F</url>
    <content type="text"><![CDATA[用过 Spring Boot 的都知道在 Spring Boot 中有以下两种配置文件 bootstrap (.yml 或者 .properties) application (.yml 或者 .properties) 为什么会有这两种配置文件呢？大家都清楚它们的区别和具体使用场景吗？ bootstrap/ application 的区别特意去翻了下 Spring Boot 的官方文档，没有找到关于这两种文件的具体定义，然后再翻了下 Spring Cloud 的官方文档找到了它们的区别。 http://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_the_bootstrap_application_context 认真阅读了下文档，原文大概意思是这样。 Spring Cloud 构建于 Spring Boot 之上，在 Spring Boot 中有两种上下文，一种是 bootstrap, 另外一种是 application, bootstrap 是应用程序的父上下文，也就是说 bootstrap 加载优先于 applicaton。bootstrap 主要用于从额外的资源来加载配置信息，还可以在本地外部配置文件中解密属性。这两个上下文共用一个环境，它是任何Spring应用程序的外部属性的来源。bootstrap 里面的属性会优先加载，它们默认也不能被本地相同配置覆盖。 因此，对比 application 配置文件，bootstrap 配置文件具有以下几个特性。 boostrap 由父 ApplicationContext 加载，比 applicaton 优先加载 boostrap 里面的属性不能被覆盖 bootstrap/ application 的应用场景application 配置文件这个容易理解，主要用于 Spring Boot 项目的自动化配置。 bootstrap 配置文件有以下几个应用场景。 使用 Spring Cloud Config 配置中心时，这时需要在 bootstrap 配置文件中添加连接到配置中心的配置属性来加载外部配置中心的配置信息； 一些固定的不能被覆盖的属性 一些加密/解密的场景； 以下这个截图是一个国外网友问了一个 Spring Cloud 工程师得到的回答。 做过 Spring Cloud 微服务的朋友应该对 bootstrap 的应用十分清楚，我们也有 Spring Cloud 的实战教程，在 Spring 专题中都能看到。]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven：Non-resolvable parent POM: Failure to find错误]]></title>
    <url>%2Fposts%2F341c%2F</url>
    <content type="text"><![CDATA[使用Maven编译项目时遇到如下错误： 12[ERROR] The project dfjr.generic:dfjr-generic:1.0-SNAPSHOT (F:\workspace\DFJR-PERSONNEL\dfjr-generic\pom.xml) has 1 error[ERROR] Non-resolvable parent POM: Could not find artifact com.dfjr.generic:dfjr-generic-parent:pom:1.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 12, column 13 -&gt; [Help 2] 由提示可知是parent.relativePath出错。解决办法如下： 123456&lt;parent&gt; &lt;groupId&gt;cn.joinhealth&lt;/groupId&gt; &lt;artifactId&gt;followup-job&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;relativePath&gt;../pom.xml&lt;/relativePath&gt;&lt;/parent&gt;]]></content>
      <categories>
        <category>Maven</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内网穿透ngrok]]></title>
    <url>%2Fposts%2F5e0c%2F</url>
    <content type="text"><![CDATA[下载 ngrokhttps://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-darwin-amd64.zip 解压1$ unzip /path/to/ngrok.zip 映射1$ ./ngrok http 80 帮助1$ ./ngrok help]]></content>
      <categories>
        <category>ngrok</category>
      </categories>
      <tags>
        <tag>ngrok</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机]]></title>
    <url>%2Fposts%2Ff9b5%2F</url>
    <content type="text"><![CDATA[Java堆溢出 1234567891011121314151617/*** VM Args 堆的最大、最小值设置为一样可以避免堆自动扩展* -Xms20m 堆的最小值* -Xmx20m 堆的最大值* -XX:+HeapDumpOnOutOfMemoryError 虚拟机在出现内存溢出异常时Dump出当前的堆转储快照*/public class HeapOOM&#123; static class OOMObject&#123; &#125; public static void main(String[] args)&#123; List&lt;OOMObject&gt; list = new ArrayList&lt;&gt;(); while(true)&#123; list.add(new OOMObject); &#125; &#125; &#125; 异常堆栈信息： 1java.lang.OutOfMemoryError Java heap space 首先要确认是内存泄漏（Memory Leak）还是内存溢出（Memory Overflow） 如果是内存泄漏可以通过工具查看泄漏对象到GC Roots的引用链，定位到泄漏代码的位置。 如果是内存溢出检查虚拟机的堆参数（-Xmx与-Xms），与机器物理内存对比看是否可以调大，从代码上检查是否存在某些对象生命周期过长、持有状态时间过长的情况，尝试减少程序运行期间的内存消耗。 虚拟机栈和本地方法栈溢出如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出StackOverflowError异常。 如果虚拟机在扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim替换]]></title>
    <url>%2Fposts%2F37fe%2F</url>
    <content type="text"><![CDATA[全局替换语法1[addr]s/sourseString/targetString/[option]]]></content>
      <categories>
        <category>Vim</category>
      </categories>
      <tags>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat查看版本]]></title>
    <url>%2Fposts%2Fd9b6%2F</url>
    <content type="text"><![CDATA[在tomcat bin目录下有version.sh可执行脚本]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud Service-zuul]]></title>
    <url>%2Fposts%2F5585%2F</url>
    <content type="text"><![CDATA[在SpringCloud中了提供了基于Netflix Zuul实现的API网关组件Spring Cloud Zuul。 SpringCloud Zuul可以通过与SpringCloud Eureka进行整合，将自身注册为Eureka服务治理下的应用，同时从Eureka中获得了所有其他微服务的实例信息。这样的设计非常巧妙地将服务治理体系中维护的实例信息利用起来，使得将维护服务实例的工作交给了服务治理框架自动完成，不再需要人工介入。 SpringCloud Zuul提供了一套过滤器机制，它可以 很好地支持这样的任务。开发者可以通过使用Zuul来创建各种校验过滤器，然后指定哪些规则的请求需要执行校验逻辑，只有通过校验的才会被路由到具体的微服务接口，不然就返回错误提示。通过这样的改造，各个业务层的微服务应用就不再需要非业务性质的校验逻辑了，这使得我们的微服务应用可以更专注千业务逻辑的开发，同时微服务的自动化测试也变得更容易实现。 pom.xml123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-zuul&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; application.yml123456789101112131415161718192021222324252627eureka: client: service-url: #设置与Eureka Server交互的地址，查询服务和注册服务都需要依赖这个地址。默认是http://localhost:8761/eureka ；多个地址可使用 , 分隔。 defaultZone: http://localhost:7001/eureka/server: port: 7005spring: application: name: service-zuulzuul: routes: #/api-a/ 开头匹配到service-producer api-a: path: /api-a/** serviceId: service-producer #/api-b/ 开头匹配到service-producer api-b: path: /api-b/** serviceId: service-producer #匹配/github/直接重定向到https://github.com/ github: path: /github/** url: https://github.com/ ServiceZuulApplication.java1234567891011121314151617181920package com.linjian.servicezuul;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.netflix.eureka.EnableEurekaClient;import org.springframework.cloud.netflix.zuul.EnableZuulProxy;import org.springframework.cloud.netflix.zuul.EnableZuulServer;@EnableEurekaClient@EnableDiscoveryClient@EnableZuulProxy@SpringBootApplicationpublic class ServiceZuulApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ServiceZuulApplication.class, args); &#125;&#125; 测试： 启动eureka-server、service-producer、service-zuul 访问 http://localhost:7005/github/Delena 访问 http://localhost:7005/api-a/hello?name=linjian]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud Service-producer]]></title>
    <url>%2Fposts%2F3f6d%2F</url>
    <content type="text"><![CDATA[pom.xml 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; application.yml 12345678910111213server: port: 7003eureka: instance: hostname: localhost client: serviceUrl: defaultZone: http://localhost:7001/eureka/ #注册中心的地址spring: application: name: service-producer #服务的名字 启动类添加注解@EnableDiscoveryClient RestController.java 123456789101112131415161718192021package com.linjian.serviceproducer.controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;/** * ProducerController * * @author jlin * @date 2019/3/19 22:20 * @Description */@RestControllerpublic class ProducerController &#123; @RequestMapping("/hello") public String hello(@RequestParam String name) &#123; return "hello " + name + "，this is new world"; &#125;&#125;]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud Service-consumer]]></title>
    <url>%2Fposts%2F36f5%2F</url>
    <content type="text"><![CDATA[pom.xml 12345678910111213141516171819 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; application.yml12345678910111213server: port: 7004eureka: instance: hostname: localhost client: serviceUrl: defaultZone: http://localhost:7001/eureka/ #注册中心的地址spring: application: name: service-consumer #服务的名字 RibbonRibbonService.java12345678910111213141516171819package com.linjian.serviceconsumer.service;/** * RibbonService * * @author jlin * @date 2019/3/19 22:23 * @Description */public interface RibbonService &#123; /** * say hello * * @param name * @return */ String hello(String name);&#125; RibbonServiceImpl.java123456789101112131415161718192021222324252627282930package com.linjian.serviceconsumer.service.impl;import com.linjian.serviceconsumer.service.RibbonService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.web.client.RestTemplate;/** * RibbonServiceImpl * * @author jlin * @date 2019/3/19 22:23 * @Description */@Servicepublic class RibbonServiceImpl implements RibbonService &#123; @Autowired RestTemplate restTemplate; /** * say hello * * @param name * @return */ @Override public String hello(String name) &#123; return restTemplate.getForObject("http://service-producer/hello?name=" + name, String.class); &#125;&#125; FeignFeignExampleService.java12345678910111213141516171819package com.linjian.serviceconsumer.service;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestParam;/** * FeignExampleService * * @author jlin * @date 2019/3/20 10:40 * @Description */@FeignClient(value = "service-producer")public interface FeignExampleService &#123; @GetMapping("hello") public String hello(@RequestParam(value = "name") String name);&#125; 启动类12345678910111213141516171819202122232425262728package com.linjian.serviceconsumer;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.client.loadbalancer.LoadBalanced;import org.springframework.context.annotation.Bean;import org.springframework.web.client.RestTemplate;@SpringBootApplication@EnableDiscoveryClient@EnableFeignClients@EnableEurekaClientpublic class ServiceConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ServiceConsumerApplication.class, args); &#125; /** * LoadBalanced 注解表明restTemplate使用LoadBalancerClient执行请求 */ @Bean @LoadBalanced RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; ConsumerController.java1234567891011121314151617181920212223242526272829303132333435363738package com.linjian.serviceconsumer.controller;import com.linjian.serviceconsumer.service.FeignExampleService;import com.linjian.serviceconsumer.service.impl.RibbonServiceImpl;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RestController;import javax.annotation.Resource;/** * ConsumerController * * @author jlin * @date 2019/3/19 22:22 * @Description */@RestControllerpublic class ConsumerController &#123; @Resource private FeignExampleService feignExampleService; @Autowired RibbonServiceImpl ribbonServiceImpl; @GetMapping("/hello/&#123;name&#125;") public String hello(@PathVariable("name") String name) &#123; return ribbonServiceImpl.hello(name); &#125; @GetMapping("/helloFeign/&#123;name&#125;") public String helloFeign(@PathVariable("name") String name) &#123; return feignExampleService.hello(name); &#125;&#125; 启动eureka-server、service-producer、service-consumer调用http://localhost:7004/hello/linjian http://localhost:7004/helloFeign/linjian]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud ParentPom]]></title>
    <url>%2Fposts%2F33cf%2F</url>
    <content type="text"><![CDATA[parent pom.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;groupId&gt;com.linjian&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-learn&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;name&gt;spring-cloud-learn&lt;/name&gt; &lt;description&gt;Project for Spring Cloud Learn&lt;/description&gt; &lt;modules&gt; &lt;module&gt;eureka-server&lt;/module&gt; &lt;/modules&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;!-- 文件拷贝时的编码 --&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;!-- 编译时的编码 --&gt; &lt;maven.compiler.encoding&gt;UTF-8&lt;/maven.compiler.encoding&gt; &lt;spring-cloud.version&gt;Greenwich.SR1&lt;/spring-cloud.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt;&lt;/project&gt;]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud Hystrix]]></title>
    <url>%2Fposts%2F4511%2F</url>
    <content type="text"><![CDATA[断路器模式源于Martin Fowler的Circuit Breaker一文。“断路器”本身是一种开关装置，用于在电路上保护线路过载，当线路中有电器发生短路时，“断路器”能够及时的切断故障电路，防止发生过载、发热、甚至起火等严重后果。 在分布式架构中，断路器模式的作用也是类似的，当某个服务单元发生故障（类似用电器发生短路）之后，通过断路器的故障监控（类似熔断保险丝），直接切断原来的主逻辑调用。但是，在Hystrix中的断路器除了切断主逻辑的功能之外，还有更复杂的逻辑。 正常情况下，当整个服务环境中，某一个服务提供方由于网络原因、数据库原因或者性能原因等，造成响应很慢的话，调用方就有可能短时间内累计大量的请求线程，最终造成调用方down，甚至整个系统崩溃。而加入hystrix之后，如果hystrix发现某个服务的某台机器调用非常缓慢或者多次调用失败，就会短时间内把这条路断掉，所有的请求都不会再发到这台机器上。 如果某个服务所有的机器都挂了，hystrix会迅速失败，马上返回，保证被调用方不会有大量的线程堆积。 使用eureka时，当一个服务提供方挂掉以后，服务订阅者最长可能30s以后才知道，那这30s就会出现大量的调用失败。如果在系统里面集成了hystrix，就会马上把挂掉的这台服务提供方断路掉，让请求不再转发到这台机器上，大量减少调用失败。hystrix执行断路操作以后，并不表示这条路就永远断了，而是会一定时间间隔内缓慢尝试去请求这条路，如果能请求成功，断路就会恢复。 有一点需要注意的是hystrix在做断路时，默认所有的调用请求都会放在一个的线程池中进行，线程池的作用很明显，有隔离性。比如gateway，集成了5个子业务系统，可能其中一个系统的调用量非常大，而另外四个系统的调用很小，如果没有线程池的话，显然第一个系统的大量调用会影响到后面四个系统的调用性能。hystrix的线程池和java标准线程池一样，可以配置一些参数：coreSize、maximumSize、maxQueueSize、queueSizeRejectionThreshold、allowMaximumSizeToDivergeFromCoreSize、keepAliveTimeMinutes等，如果某一个子系统的调用量突然激增，超过了线程池的容量，也会迅速失败，直接返回，起到降级和保护系统本身的作用。当然hystrix也支持非线程池的方式，在本地请求线程中做调用，即semaphore模式，官方不建议，除非系统qps真的很大。 Hystrix案例Feign默认集成了Hystrix。我们可以在上一个moudle service-consumer中增加熔断特性。 application.yml 1234#开启Hystrixfeign: hystrix: enabled: true FeignServiceHystrix.java 12345678910111213141516171819package com.linjian.serviceconsumer.service;import org.springframework.stereotype.Component;/** * FeignServiceHystrix * * @author jlin * @date 2019/3/20 11:43 * @Description */@Componentpublic class FeignServiceHystrix implements FeignExampleService &#123; @Override public String hello(String name) &#123; return "sorry " + name + "，service has fail!"; &#125;&#125; FeignExampleService.java修改FeignClient增加fallback熔断处理 12345678910111213141516171819package com.linjian.serviceconsumer.service;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestParam;/** * FeignExampleService * * @author jlin * @date 2019/3/20 10:40 * @Description */@FeignClient(value = "service-producer", fallback = FeignServiceHystrix.class)public interface FeignExampleService &#123; @GetMapping("hello") public String hello(@RequestParam(value = "name") String name);&#125;]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud Eureka-server]]></title>
    <url>%2Fposts%2F902c%2F</url>
    <content type="text"><![CDATA[pom.xml 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; application.yml1234567891011121314151617server: port: 7001spring: application: name: eureka-servereureka: instance: hostname: localhost client: #表示是否将自己注册到Eureka Server，默认为true。 register-with-eureka: false #表示是否从Eureka Server获取注册信息，默认为true。 fetch-registry: false service-url: #设置与Eureka Server交互的地址，查询服务和注册服务都需要依赖这个地址。默认是http://localhost:8761/eureka ；多个地址可使用 , 分隔。 defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ 启动类加上注释@EnableEurekaServer]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud Eureka-client]]></title>
    <url>%2Fposts%2F1282%2F</url>
    <content type="text"><![CDATA[pom.xml 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; application.yml12345678910111213server: port: 7002eureka: instance: hostname: localhost client: serviceUrl: defaultZone: http://localhost:7001/eureka/ #注册中心的地址spring: application: name: eureka-client #服务的名字 启动类加上注解@EnableEurekaClient]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud Config-server]]></title>
    <url>%2Fposts%2F64fe%2F</url>
    <content type="text"><![CDATA[pom.xml 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt;&lt;/dependency&gt; application.yml1234567891011121314151617181920server: port: 7006spring: application: name: config-server cloud: config: server: git: # 配置git仓库的地址 uri: https://github.com/Delena1988/spring-cloud-config-repo # git仓库地址下的相对地址，可以配置多个，用,分割。 search-paths: /** # git仓库的账号（私有库必填） username: # git仓库的密码（私有库必填） password: # 配置git仓库的分支 label: master ConfigServerApplication.java12345678910111213141516package com.linjian.configserver;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.config.server.EnableConfigServer;//开启配置服务器@EnableConfigServer@SpringBootApplicationpublic class ConfigServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigServerApplication.class, args); &#125;&#125; Github创建repository spring-cloud-config-repo创建配置文件 spring-cloud-config-dev.yml spring-cloud-config-test.yml spring-cloud-config-pro.yml 测试： 启动Config-server 访问 http://localhost:7006/master/spring-cloud-config-dev.yml 访问 http://localhost:7006/master/spring-cloud-config-test.yml 访问 http://localhost:7006/master/spring-cloud-config-pro.yml]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud Config-client]]></title>
    <url>%2Fposts%2Fe650%2F</url>
    <content type="text"><![CDATA[pom.xml 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; application.yml123456server: port: 7007spring: application: name: config-client bootstrap.yml1234567891011121314spring: cloud: config: # 对应&#123;application&#125;部分 name: spring-cloud-config # 对应&#123;profile&#125;部分 profile: pro # 配置中心的具体地址 uri: http://localhost:7006/ # 对应git的分支。如果配置中心使用的是本地存储，则该参数无用 label: master discovery: # 指定配置中心的service-id，便于扩展为高可用配置集群。 service-id: config-server ConfigClientController.java1234567891011121314151617181920212223package com.linjian.configclient.controller;import org.springframework.beans.factory.annotation.Value;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * ConfigClientController * * @author jlin * @date 2019/3/20 14:42 * @Description */@RestControllerpublic class ConfigClientController &#123; @Value("$&#123;environment&#125;") String environment; @RequestMapping(value = "/environment") public String environment() &#123; return environment; &#125;&#125; 测试： 启动config-server、config-client 访问 http://localhost:7007/environment]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud Actuator]]></title>
    <url>%2Fposts%2F87d9%2F</url>
    <content type="text"><![CDATA[动态刷新配置 pom.xml1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; application.yml1234567#关闭安全认证management: #refresh接入点显式暴露出来 endpoints: web: exposure: include: refresh,health,info 给需要加载变量的bean上面加载@RefreshScope注解 客户端执行/refresh的时候就会更新此bean下面的变量值 1curl -X POST http://localhost:7007/actuator/refresh]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot项目创建]]></title>
    <url>%2Fposts%2F9415%2F</url>
    <content type="text"><![CDATA[new -project 选择spring initializr 创建自己的包名，类名 选择需要加载的依赖 新建工程最后一步：修改工程名称（自行修改，也可默认不改）。点击Finish 完成新建工作。 pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.joinhealth&lt;/groupId&gt; &lt;artifactId&gt;springboot-demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;springboot-demo&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.liquibase&lt;/groupId&gt; &lt;artifactId&gt;liquibase-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- 分页插件 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.5&lt;/version&gt; &lt;/dependency&gt; &lt;!-- alibaba的druid数据库连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;!--liquibase--&gt; &lt;dependency&gt; &lt;groupId&gt;org.liquibase&lt;/groupId&gt; &lt;artifactId&gt;liquibase-core&lt;/artifactId&gt; &lt;version&gt;3.5.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- mybatis generator 自动生成代码插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;configuration&gt; &lt;configurationFile&gt;$&#123;basedir&#125;/src/main/resources/generator/generatorConfig.xml&lt;/configurationFile&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; application.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445server: port: 8080spring: datasource: druid: name: test #206 url: jdbc:mysql://192.168.3.206:3306/springboot?useUnicode=true&amp;amp;characterEncoding=UTF-8 username: joinhealth password: 123456 #localhost #url: jdbc:mysql://localhost:3306/springboot?useUnicode=true&amp;amp;characterEncoding=UTF-8 #username: root #password: 123456 # 使用druid数据源 type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.jdbc.Driver filters: stat maxActive: 20 initialSize: 1 maxWait: 60000 minIdle: 1 timeBetweenEvictionRunsMillis: 60000 minEvictableIdleTimeMillis: 300000 validationQuery: select &apos;x&apos; testWhileIdle: true testOnBorrow: false testOnReturn: false poolPreparedStatements: true maxOpenPreparedStatements: 20 #liquibase liquibase: change-log: classpath:liquibase/master.xml check-change-log-location: false enabled: true## 该配置节点为独立的节点，有很多同学容易将这个配置放在spring的节点下，导致配置无法被识别mybatis:mapper-locations: classpath:mapping/*.xml #注意：一定要对应mapper映射xml文件的所在路径type-aliases-package: cn.joinhealth.model # 注意：对应实体类的路径#pagehelper分页插件pagehelper: helperDialect: mysql reasonable: true supportMethodsArguments: true params: count=countSql mybatis-generator generatorConfig.xml123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC &quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd&quot;&gt;&lt;generatorConfiguration&gt; &lt;!-- 数据库驱动:选择你的本地硬盘上面的数据库驱动包--&gt; &lt;classPathEntry location=&quot;/Users/linjian/Documents/maven/repository/mysql/mysql-connector-java/5.1.46/mysql-connector-java-5.1.46.jar&quot;/&gt; &lt;context id=&quot;DB2Tables&quot; targetRuntime=&quot;MyBatis3&quot;&gt; &lt;commentGenerator&gt; &lt;property name=&quot;suppressDate&quot; value=&quot;true&quot;/&gt; &lt;!-- 是否去除自动生成的注释 true：是 ： false:否 --&gt; &lt;property name=&quot;suppressAllComments&quot; value=&quot;true&quot;/&gt; &lt;/commentGenerator&gt; &lt;!--数据库链接URL，用户名、密码 --&gt; &lt;jdbcConnection driverClass=&quot;com.mysql.jdbc.Driver&quot; connectionURL=&quot;jdbc:mysql://192.168.3.206/springboot&quot; userId=&quot;joinhealth&quot; password=&quot;123456&quot;&gt; &lt;/jdbcConnection&gt; &lt;javaTypeResolver&gt; &lt;property name=&quot;forceBigDecimals&quot; value=&quot;false&quot;/&gt; &lt;/javaTypeResolver&gt; &lt;!-- 生成模型的包名和位置--&gt; &lt;javaModelGenerator targetPackage=&quot;cn.joinhealth.model&quot; targetProject=&quot;src/main/java&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;trimStrings&quot; value=&quot;true&quot;/&gt; &lt;/javaModelGenerator&gt; &lt;!-- 生成映射文件的包名和位置--&gt; &lt;sqlMapGenerator targetPackage=&quot;mapping&quot; targetProject=&quot;src/main/resources&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot;/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 生成DAO的包名和位置--&gt; &lt;javaClientGenerator type=&quot;XMLMAPPER&quot; targetPackage=&quot;cn.joinhealth.mapper&quot; targetProject=&quot;src/main/java&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot;/&gt; &lt;/javaClientGenerator&gt; &lt;!-- 要生成的表 tableName是数据库中的表名或视图名 domainObjectName是实体类名--&gt; &lt;table tableName=&quot;t_user&quot; domainObjectName=&quot;User&quot; enableCountByExample=&quot;false&quot; enableUpdateByExample=&quot;false&quot; enableDeleteByExample=&quot;false&quot; enableSelectByExample=&quot;false&quot; selectByExampleQueryId=&quot;false&quot;&gt;&lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot线程池]]></title>
    <url>%2Fposts%2F206f%2F</url>
    <content type="text"><![CDATA[TaskPoolConfig.java 123456789101112131415@Configuration@EnableAsyncpublic class TaskPoolConfig &#123; @Bean("taskExecutor") public Executor taskExecutor() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(10); executor.setMaxPoolSize(20); executor.setQueueCapacity(200); executor.setKeepAliveSeconds(60); executor.setThreadNamePrefix("taskExecutor-"); executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); return executor; &#125;&#125; 使用 12@Autowired private ThreadPoolTaskExecutor taskExecutor;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot集成Swagger]]></title>
    <url>%2Fposts%2Fa9e2%2F</url>
    <content type="text"><![CDATA[pom.xml 12345678910111213&lt;!-- swagger生成接口API --&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 接口API生成html文档 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt;&lt;/dependency&gt; 访问地址http://localhost:8080/swagger-ui.html#/]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot集成redis]]></title>
    <url>%2Fposts%2Fbeb3%2F</url>
    <content type="text"><![CDATA[pom.xml1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; RedisConfig.java123456789101112131415161718192021222324252627282930313233343536/** * RedisConfig * * @author jlin * @date 2018/11/7 18:44 */@Configuration@AutoConfigureAfter(RedisAutoConfiguration.class)public class RedisConfig &#123; /** * 配置自定义redisTemplate * * @return */ @Bean RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(redisConnectionFactory); //使用Jackson2JsonRedisSerializer来序列化和反序列化redis的value值 Jackson2JsonRedisSerializer serializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper mapper = new ObjectMapper(); mapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); mapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); serializer.setObjectMapper(mapper); template.setValueSerializer(serializer); //使用StringRedisSerializer来序列化和反序列化redis的key值 template.setKeySerializer(new StringRedisSerializer()); template.setHashKeySerializer(new StringRedisSerializer()); template.setHashValueSerializer(serializer); template.afterPropertiesSet(); return template; &#125;&#125; RedisTest.java 测试12345678910111213141516171819202122232425262728293031323334/** * RedisTest * * @author jlin * @date 2018/11/9 09:41 */@RunWith(SpringRunner.class)@SpringBootTestpublic class RedisTest &#123; @Autowired private RedisTemplate redisTemplate; @Test public void setRedis() &#123; redisTemplate.opsForValue().set("name", "林剑"); redisTemplate.opsForValue().set("sex", "男"); User user1 = new User(); user1.setUserId(1); user1.setUserName("张三"); user1.setPassword("123"); User user2 = new User(); user2.setUserId(2); user2.setUserName("李四"); user2.setPassword("456"); redisTemplate.opsForHash().put("user", String.valueOf(user1.getUserId()), user1); redisTemplate.opsForHash().put("user", String.valueOf(user2.getUserId()), user2); &#125; @Test public void getRedis() &#123; System.out.println(JSON.toJSONString(redisTemplate.opsForHash().get("user", "1"))); System.out.println(JSON.toJSONString(redisTemplate.opsForHash().get("user", "2"))); &#125;&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot集成mybatis-generator]]></title>
    <url>%2Fposts%2Fc53d%2F</url>
    <content type="text"><![CDATA[pom.xml 12345678910111213141516171819 &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- mybatis generator 自动生成代码插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;configuration&gt; &lt;!-- 指定配置文件路径 --&gt; &lt;configurationFile&gt;$&#123;basedir&#125;/src/main/resources/generator/generatorConfig.xml&lt;/configurationFile&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; mybatis-generator generatorConfig.xml1234567891011121314151617181920212223242526272829303132333435&lt;!DOCTYPE generatorConfiguration PUBLIC &quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd&quot;&gt;&lt;generatorConfiguration&gt; &lt;!-- 数据库驱动:选择你的本地硬盘上面的数据库驱动包--&gt; &lt;classPathEntry location=&quot;/Users/linjian/Documents/maven/repository/mysql/mysql-connector-java/5.1.46/mysql-connector-java-5.1.46.jar&quot;/&gt; &lt;context id=&quot;DB2Tables&quot; targetRuntime=&quot;MyBatis3&quot;&gt; &lt;commentGenerator&gt; &lt;property name=&quot;suppressDate&quot; value=&quot;true&quot;/&gt; &lt;!-- 是否去除自动生成的注释 true：是 ： false:否 --&gt; &lt;property name=&quot;suppressAllComments&quot; value=&quot;true&quot;/&gt; &lt;/commentGenerator&gt; &lt;!--数据库链接URL，用户名、密码 --&gt; &lt;jdbcConnection driverClass=&quot;com.mysql.jdbc.Driver&quot; connectionURL=&quot;jdbc:mysql://192.168.3.206/springboot&quot; userId=&quot;joinhealth&quot; password=&quot;123456&quot;&gt; &lt;/jdbcConnection&gt; &lt;javaTypeResolver&gt; &lt;property name=&quot;forceBigDecimals&quot; value=&quot;false&quot;/&gt; &lt;/javaTypeResolver&gt; &lt;!-- 生成模型的包名和位置--&gt; &lt;javaModelGenerator targetPackage=&quot;cn.joinhealth.model&quot; targetProject=&quot;src/main/java&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;trimStrings&quot; value=&quot;true&quot;/&gt; &lt;/javaModelGenerator&gt; &lt;!-- 生成映射文件的包名和位置--&gt; &lt;sqlMapGenerator targetPackage=&quot;mapping&quot; targetProject=&quot;src/main/resources&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot;/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 生成DAO的包名和位置--&gt; &lt;javaClientGenerator type=&quot;XMLMAPPER&quot; targetPackage=&quot;cn.joinhealth.mapper&quot; targetProject=&quot;src/main/java&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot;/&gt; &lt;/javaClientGenerator&gt; &lt;!-- 要生成的表 tableName是数据库中的表名或视图名 domainObjectName是实体类名--&gt; &lt;table tableName=&quot;t_user&quot; domainObjectName=&quot;User&quot; enableCountByExample=&quot;false&quot; enableUpdateByExample=&quot;false&quot; enableDeleteByExample=&quot;false&quot; enableSelectByExample=&quot;false&quot; selectByExampleQueryId=&quot;false&quot;&gt;&lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot集成liquibase]]></title>
    <url>%2Fposts%2F7773%2F</url>
    <content type="text"><![CDATA[pom.xml 123456&lt;!--liquibase--&gt;&lt;dependency&gt; &lt;groupId&gt;org.liquibase&lt;/groupId&gt; &lt;artifactId&gt;liquibase-core&lt;/artifactId&gt; &lt;version&gt;3.5.3&lt;/version&gt;&lt;/dependency&gt; application.yml12345#liquibaseliquibase: change-log: classpath:liquibase/master.xml check-change-log-location: false enabled: true]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot集成Kafka]]></title>
    <url>%2Fposts%2F260e%2F</url>
    <content type="text"><![CDATA[pom.xml12345&lt;!-- kafka --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;&lt;/dependency&gt; application.yml12345spring: kafka: bootstrap-servers: 192.168.3.206:9002 consumer: group-id: mygroup 提供者123456789101112@RunWith(SpringRunner.class)@SpringBootTestpublic class KafkaTest &#123; @Resource private KafkaTemplate kafkaTemplate; @Test public void providerTest() &#123; kafkaTemplate.send("myTopic", "Test Message!"); &#125;&#125; 消费者1234567891011@Component@Slf4jpublic class KafkaConsumer &#123; @KafkaListener(topics = "myTopic") public void listen(ConsumerRecord&lt;?, String&gt; record) &#123; String value = record.value(); log.info(value); log.info(JSON.toJSONString(record)); &#125;&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot集成elasticsearch]]></title>
    <url>%2Fposts%2F5e6e%2F</url>
    <content type="text"><![CDATA[pom.xml 123456789101112131415161718&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--实体工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.yml123456789101112spring: data: elasticsearch: cluster-name: es-cluster #配置es节点信息，逗号分隔，如果没有指定，则启动ClientNode（9200端口是http查询使用的。9300集群使用。这里使用9300.） cluster-nodes: 127.0.0.1:9300 properties: path: #elasticsearch日志存储目录 logs: ./elasticsearch/log #elasticsearch数据存储目录 data: ./elasticsearch/data Country.java123456789@Datapublic class Country implements Serializable &#123; @Id private Integer id; @Field(searchAnalyzer = "ik_max_word",analyzer = "ik_smart") private String name;&#125; CountrySearchRepository.java12public interface CountrySearchRepository extends ElasticsearchRepository&lt;Country, Long&gt; &#123;&#125; 单元测试 EsDemoApplicationTests.java1234567891011121314151617181920212223242526272829303132@SpringBootTestpublic class EsDemoApplicationTests &#123; @Autowired private CountrySearchRepository countrySearchRepository; @Test public void testSaveCountryIndex() &#123; Country country = new Country(); country.setId(1); country.setName("China"); Country country2 = new Country(); country2.setId(2); country2.setName("America"); List&lt;Country&gt; countryList = new ArrayList&lt;&gt;(2); countryList.add(country); countryList.add(country2); countrySearchRepository.saveAll(countryList); &#125; @Test public void testSearch() &#123; //搜索关键字 String queryString = "china"; QueryStringQueryBuilder builder=new QueryStringQueryBuilder(queryString); Iterable&lt;Country&gt; searchResult = countrySearchRepository.search(builder); Iterator&lt;Country&gt; iterator = searchResult.iterator(); while(iterator.hasNext())&#123; System.out.println(iterator.next()); &#125; &#125;&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot集成dubbo]]></title>
    <url>%2Fposts%2F4518%2F</url>
    <content type="text"><![CDATA[pom.xml 1234567891011121314151617&lt;!--dubbo-springBoot依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;!--zookeeper依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt;&lt;/dependency&gt; application.yml12345678910111213dubbo: #应用配置，用于配置当前应用信息，不管该应用是提供者还是消费者。 application: name: Consumer #注册中心配置，用于配置连接注册中心相关信息。 registry: address: zookeeper://192.168.3.206:2181 #协议配置，用于配置提供服务的协议信息，协议由提供方指定，消费方被动接受。 protocol: name: dubbo port: 20880 #服务暴露与发现消费所在的package scan: cn.joinhealth.hug.model.api.health 使用123@Referenceprivate UserService userService; 开启@EnableDubboConfiguration12345@SpringBootApplication@EnableDubboConfigurationpublic class DubboConsumerLauncher &#123; //...&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot集成Cache]]></title>
    <url>%2Fposts%2Fd5fe%2F</url>
    <content type="text"><![CDATA[pom.xml12345678910&lt;!-- cache --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- ehcache --&gt;&lt;dependency&gt; &lt;groupId&gt;net.sf.ehcache&lt;/groupId&gt; &lt;artifactId&gt;ehcache&lt;/artifactId&gt;&lt;/dependency&gt; 修改Application类，加入启用缓存的注解@EnableCaching1234567891011@SpringBootApplication@MapperScan("cn.joinhealth.mapper")@ComponentScan(basePackages = &#123;"cn.joinhealth.*"&#125;)@EnableDubboConfiguration@EnableCachingpublic class SpringbootDemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootDemoApplication.class, args); &#125;&#125; application.yml12345spring: cache: type: ehcache ehcache: config: classpath:/ehcache.xml ehcache.xml123456789101112131415161718192021222324252627282930&lt;ehcache&gt; &lt;!-- 磁盘存储:将缓存中暂时不使用的对象,转移到硬盘,类似于Windows系统的虚拟内存 path:指定在硬盘上存储对象的路径 path可以配置的目录有： user.home（用户的家目录） user.dir（用户当前的工作目录） java.io.tmpdir（默认的临时目录） ehcache.disk.store.dir（ehcache的配置目录） 绝对路径（如：d:\\ehcache） 查看路径方法：String tmpDir = System.getProperty(&quot;java.io.tmpdir&quot;); --&gt; &lt;diskStore path=&quot;java.io.tmpdir&quot; /&gt; &lt;!-- defaultCache:默认的缓存配置信息,如果不加特殊说明,则所有对象按照此配置项处理 maxElementsInMemory:设置了缓存的上限,最多存储多少个记录对象 eternal:代表对象是否永不过期 (指定true则下面两项配置需为0无限期) timeToIdleSeconds:最大的发呆时间 /秒 timeToLiveSeconds:最大的存活时间 /秒 overflowToDisk:是否允许对象被写入到磁盘 说明：下列配置自缓存建立起600秒(10分钟)有效 。 在有效的600秒(10分钟)内，如果连续120秒(2分钟)未访问缓存，则缓存失效。 就算有访问，也只会存活600秒。 --&gt; &lt;defaultCache maxElementsInMemory=&quot;10000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;600&quot; timeToLiveSeconds=&quot;600&quot; overflowToDisk=&quot;true&quot; /&gt; &lt;cache name=&quot;user&quot; maxElementsInMemory=&quot;10000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;120&quot; timeToLiveSeconds=&quot;600&quot; overflowToDisk=&quot;true&quot; /&gt;&lt;/ehcache&gt; UserServiceImpl.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package cn.joinhealth.service.impl;import cn.joinhealth.mapper.UserMapper;import cn.joinhealth.model.User;import cn.joinhealth.service.UserService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.cache.annotation.CacheConfig;import org.springframework.cache.annotation.CacheEvict;import org.springframework.cache.annotation.CachePut;import org.springframework.cache.annotation.Cacheable;import org.springframework.stereotype.Service;import java.util.List;/** * UserServiceImpl * * @author linjian * @date 2018/7/27 上午11:25 */@Service("userService")@CacheConfig(cacheNames = &#123;"user"&#125;)public class UserServiceImpl implements UserService &#123; @Autowired private UserMapper userMapper; /** * 新增用户 * * @param user */ @Override @CachePut(key = "#user.userId") public User save(User user) &#123; userMapper.insert(user); return user; &#125; /** * 分页获取用户 * * @return */ @Override public List&lt;User&gt; list() &#123; return userMapper.listUser(); &#125; /** * 根据id获取用户 * * @param id * @return */ @Override @Cacheable(key = "#id") public User get(Integer id) &#123; return userMapper.selectByPrimaryKey(id); &#125; /** * 根据id删除用户 * * @param id */ @Override @CacheEvict(key = "#id") public void delete(Integer id) &#123; userMapper.deleteByPrimaryKey(id); &#125; /** * 编辑用户 * * @param user */ @Override @CachePut(key = "#user.userId") public User update(User user) &#123; userMapper.updateByPrimaryKey(user); return user; &#125;&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot多环境配置]]></title>
    <url>%2Fposts%2Fae65%2F</url>
    <content type="text"><![CDATA[多环境配置 application.yml 123spring: profiles: active: dev application-dev.yml application-prod.yml 打包运行： java -jar xxx.jar --spring.profiles.active=prod 属性配置@Value @Value(“${key}”) @Component @ConfigurationProperties Controller@Controller 处理http请求 @RestController @Controller + @ResponseBody @RequestMapping 配置URL映射 @PathVariable 获取url中的数据 @RequestParam 获取请求参数的值 @GetMapping @PostMapping 表单验证@Valid AOP统一处理请求日志]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot logback配置]]></title>
    <url>%2Fposts%2F3fcd%2F</url>
    <content type="text"><![CDATA[logback-test.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration scan=&quot;true&quot; scanPeriod=&quot;30 seconds&quot; debug=&quot;true&quot; packagingData=&quot;true&quot;&gt; &lt;statusListener class=&quot;ch.qos.logback.core.status.OnConsoleStatusListener&quot;/&gt; &lt;!--&lt;include resource=&quot;org/springframework/boot/logging/logback/base.xml&quot;/&gt;--&gt; &lt;contextName&gt;SpringBoot&lt;/contextName&gt; &lt;!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径--&gt; &lt;property name=&quot;LOG_NAME&quot; value=&quot;/home&quot;/&gt; &lt;property name=&quot;LOG_PATH&quot; value=&quot;logs&quot;/&gt; &lt;!--设置系统日志目录--&gt; &lt;property name=&quot;APP_DIR&quot; value=&quot;SpringBoot&quot;/&gt; &lt;!-- 说明： 1、日志级别及文件 日志记录采用分级记录，级别与日志文件名相对应，不同级别的日志信息记录到不同的日志文件中 例如：error级别记录到log_error_xxx.log或log_error.log（该文件为当前记录的日志文件），而log_error_xxx.log为归档日志， 日志文件按日期记录，同一天内，若日志文件大小等于或大于2M，则按0、1、2...顺序分别命名 例如log-level-2013-12-21.0.log 其它级别的日志也是如此。 2、文件路径 若开发、测试用，在Eclipse中运行项目，则到Eclipse的安装路径查找logs文件夹，以相对路径../logs。 若部署到Tomcat下，则在Tomcat下的logs文件中 3、Appender FILE_ERROR对应error级别，文件名以log-error-xxx.log形式命名 FILE_WARN对应warn级别，文件名以log-warn-xxx.log形式命名 FILE_INFO对应info级别，文件名以log-info-xxx.log形式命名 FILEDEBUG对应debug级别，文件名以log-debug-xxx.log形式命名 CONSOLE将日志信息输出到控制上，为方便开发测试使用 --&gt; &lt;!-- %m输出的信息,%p日志级别,%t线程名,%d日期,%c类的全名,,,, --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;!--&lt;pattern&gt;%d %p (%file:%line\)- %m%n&lt;/pattern&gt;--&gt; &lt;!--格式化输出：%d:表示日期 %thread:表示线程名 %-5level:级别从左显示5个字符宽度 %msg:日志消息 %n:是换行符--&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss&#125; [%thread] %-5level %logger - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 日志记录器，日期滚动记录 --&gt; &lt;appender name=&quot;FILE_ERROR&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;!-- 正在记录的日志文件的路径及文件名 --&gt; &lt;file&gt;$&#123;LOG_PATH&#125;/$&#123;APP_DIR&#125;/log_error.log&lt;/file&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy&quot;&gt; &lt;!-- 归档的日志文件的路径，例如今天是2013-12-21日志，当前写的日志文件路径为file节点指定，可以将此文件与file指定文件路径设置为不同路径，从而将当前日志文件或归档日志文件置不同的目录。 而2013-12-21的日志文件在由fileNamePattern指定。%d&#123;yyyy-MM-dd&#125;指定日期格式，%i指定索引 --&gt; &lt;fileNamePattern&gt;$&#123;LOG_PATH&#125;/$&#123;APP_DIR&#125;/error/log-error-%d&#123;yyyy-MM-dd&#125;.%i.log&lt;/fileNamePattern&gt; &lt;!-- 除按日志记录之外，还配置了日志文件不能超过2M，若超过2M，日志文件会以索引0开始， 命名日志文件，例如log-error-2013-12-21.0.log --&gt; &lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 追加方式记录日志 --&gt; &lt;append&gt;true&lt;/append&gt; &lt;!-- 日志文件的格式 --&gt; &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;pattern&gt;===%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %-5level %logger Line:%-3L - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;!-- 此日志文件只记录info级别的 --&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt; &lt;level&gt;error&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 日志记录器，日期滚动记录 --&gt; &lt;appender name=&quot;FILE_WARN&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;!-- 正在记录的日志文件的路径及文件名 --&gt; &lt;file&gt;$&#123;LOG_PATH&#125;/$&#123;APP_DIR&#125;/log_warn.log&lt;/file&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy&quot;&gt; &lt;!-- 归档的日志文件的路径，例如今天是2013-12-21日志，当前写的日志文件路径为file节点指定，可以将此文件与file指定文件路径设置为不同路径，从而将当前日志文件或归档日志文件置不同的目录。 而2013-12-21的日志文件在由fileNamePattern指定。%d&#123;yyyy-MM-dd&#125;指定日期格式，%i指定索引 --&gt; &lt;fileNamePattern&gt;$&#123;LOG_PATH&#125;/$&#123;APP_DIR&#125;/warn/log-warn-%d&#123;yyyy-MM-dd&#125;.%i.log&lt;/fileNamePattern&gt; &lt;!-- 除按日志记录之外，还配置了日志文件不能超过2M，若超过2M，日志文件会以索引0开始， 命名日志文件，例如log-error-2013-12-21.0.log --&gt; &lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 追加方式记录日志 --&gt; &lt;append&gt;true&lt;/append&gt; &lt;!-- 日志文件的格式 --&gt; &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;pattern&gt;===%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %-5level %logger Line:%-3L - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;!-- 此日志文件只记录info级别的 --&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt; &lt;level&gt;warn&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 日志记录器，日期滚动记录 --&gt; &lt;appender name=&quot;FILE_INFO&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;!-- 正在记录的日志文件的路径及文件名 --&gt; &lt;file&gt;$&#123;LOG_PATH&#125;/$&#123;APP_DIR&#125;/log_info.log&lt;/file&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy&quot;&gt; &lt;!-- 归档的日志文件的路径，例如今天是2013-12-21日志，当前写的日志文件路径为file节点指定，可以将此文件与file指定文件路径设置为不同路径，从而将当前日志文件或归档日志文件置不同的目录。 而2013-12-21的日志文件在由fileNamePattern指定。%d&#123;yyyy-MM-dd&#125;指定日期格式，%i指定索引 --&gt; &lt;fileNamePattern&gt;$&#123;LOG_PATH&#125;/$&#123;APP_DIR&#125;/info/log-info-%d&#123;yyyy-MM-dd&#125;.%i.log&lt;/fileNamePattern&gt; &lt;!-- 除按日志记录之外，还配置了日志文件不能超过2M，若超过2M，日志文件会以索引0开始， 命名日志文件，例如log-error-2013-12-21.0.log --&gt; &lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 追加方式记录日志 --&gt; &lt;append&gt;true&lt;/append&gt; &lt;!-- 日志文件的格式 --&gt; &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;pattern&gt;===%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %-5level %logger Line:%-3L - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;!-- 此日志文件只记录info级别的 --&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt; &lt;level&gt;info&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!--日志异步到数据库 --&gt; &lt;appender name=&quot;DBAPPENDER&quot; class=&quot;ch.qos.logback.classic.db.DBAppender&quot;&gt; &lt;connectionSource class=&quot;ch.qos.logback.core.db.DataSourceConnectionSource&quot;&gt; &lt;dataSource class=&quot;com.zaxxer.hikari.HikariDataSource&quot;&gt; &lt;driverClassName&gt;com.mysql.jdbc.Driver&lt;/driverClassName&gt; &lt;jdbcUrl&gt;jdbc:mysql://192.168.3.206:3306/logback?useUnicode=true&amp;amp;characterEncoding=utf8&amp;amp;useSSL=false&lt;/jdbcUrl&gt; &lt;username&gt;joinhealth&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;poolName&gt;HikariPool-logback&lt;/poolName&gt; &lt;/dataSource&gt; &lt;/connectionSource&gt; &lt;!-- 此日志文件只记录info级别的 --&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt; &lt;level&gt;warn&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;!-- 此日志文件只记录info级别的 --&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt; &lt;level&gt;error&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;appender name=&quot;CONSOLE&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %-5level %logger Line:%-3L - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;logger name=&quot;cn.joinhealth&quot; level=&quot;DEBUG&quot;/&gt; &lt;logger name=&quot;org.springframework.data.mybatis&quot; level=&quot;DEBUG&quot;/&gt; &lt;logger name=&quot;org.springframework.aop.aspectj&quot; level=&quot;ERROR&quot;/&gt; &lt;logger name=&quot;javax.activation&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;javax.mail&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;javax.xml.bind&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;ch.qos.logback&quot; level=&quot;INFO&quot;/&gt; &lt;logger name=&quot;com.codahale.metrics&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;com.ryantenney&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;com.sun&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;com.zaxxer&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;io.undertow&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;net.sf.ehcache&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;org.apache&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;org.apache.catalina.startup.DigesterFactory&quot; level=&quot;OFF&quot;/&gt; &lt;logger name=&quot;org.bson&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;org.hibernate.validator&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;org.hibernate&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;org.hibernate.ejb.HibernatePersistence&quot; level=&quot;OFF&quot;/&gt; &lt;logger name=&quot;org.springframework.web&quot; level=&quot;INFO&quot;/&gt; &lt;logger name=&quot;org.springframework.security&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;org.springframework.cache&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;org.thymeleaf&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;org.xnio&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;springfox&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;sun.rmi&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;liquibase&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;sun.rmi.transport&quot; level=&quot;WARN&quot;/&gt; &lt;logger name=&quot;jdbc.connection&quot; level=&quot;ERROR&quot;/&gt; &lt;logger name=&quot;jdbc.resultset&quot; level=&quot;ERROR&quot;/&gt; &lt;logger name=&quot;jdbc.resultsettable&quot; level=&quot;INFO&quot;/&gt; &lt;logger name=&quot;jdbc.audit&quot; level=&quot;ERROR&quot;/&gt; &lt;logger name=&quot;jdbc.sqltiming&quot; level=&quot;ERROR&quot;/&gt; &lt;logger name=&quot;jdbc.sqlonly&quot; level=&quot;INFO&quot;/&gt; &lt;!--&lt;contextListener class=&quot;ch.qos.logback.classic.jul.LevelChangePropagator&quot;&gt;--&gt; &lt;!--&lt;resetJUL&gt;true&lt;/resetJUL&gt;--&gt; &lt;!--&lt;/contextListener&gt;--&gt; &lt;!--&lt;springProfile name=&quot;production&quot;&gt;--&gt; &lt;!--&lt;root level=&quot;DEBUG&quot;&gt;--&gt; &lt;!--&amp;lt;!&amp;ndash;&lt;appender-ref ref=&quot;FILE_ERROR&quot;/&gt;&amp;ndash;&amp;gt;--&gt; &lt;!--&amp;lt;!&amp;ndash;&lt;appender-ref ref=&quot;FILE_WARN&quot;/&gt;&amp;ndash;&amp;gt;--&gt; &lt;!--&amp;lt;!&amp;ndash;&lt;appender-ref ref=&quot;FILE_INFO&quot;/&gt;&amp;ndash;&amp;gt;--&gt; &lt;!--&amp;lt;!&amp;ndash;&lt;appender-ref ref=&quot;DBAPPENDER&quot;/&gt;&amp;ndash;&amp;gt;--&gt; &lt;!--&lt;appender-ref ref=&quot;STDOUT&quot;/&gt;--&gt; &lt;!--&lt;/root&gt;--&gt; &lt;!--&lt;/springProfile&gt;--&gt; &lt;!--&lt;springProfile name=&quot;dev&quot;&gt;--&gt; &lt;!--&lt;root level=&quot;DEBUG&quot;&gt;--&gt; &lt;!--&amp;lt;!&amp;ndash;&lt;appender-ref ref=&quot;FILE_ERROR&quot;/&gt;&amp;ndash;&amp;gt;--&gt; &lt;!--&amp;lt;!&amp;ndash;&lt;appender-ref ref=&quot;FILE_WARN&quot;/&gt;&amp;ndash;&amp;gt;--&gt; &lt;!--&amp;lt;!&amp;ndash;&lt;appender-ref ref=&quot;FILE_INFO&quot;/&gt;&amp;ndash;&amp;gt;--&gt; &lt;!--&amp;lt;!&amp;ndash;&lt;appender-ref ref=&quot;DBAPPENDER&quot;/&gt;&amp;ndash;&amp;gt;--&gt; &lt;!--&lt;appender-ref ref=&quot;CONSOLE&quot;/&gt;--&gt; &lt;!--&lt;/root&gt;--&gt; &lt;!--&lt;/springProfile&gt;--&gt; &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;FILE_ERROR&quot;/&gt; &lt;appender-ref ref=&quot;FILE_WARN&quot;/&gt; &lt;appender-ref ref=&quot;FILE_INFO&quot;/&gt; &lt;appender-ref ref=&quot;STDOUT&quot;/&gt; &lt;/root&gt;&lt;/configuration&gt;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMvc配置jsp、html两个视图解析器]]></title>
    <url>%2Fposts%2F27a7%2F</url>
    <content type="text"><![CDATA[继承InternalResourceView，写解析类 123456789101112131415161718192021package cn.joinhealth.interview.web.root.resolver;import org.springframework.web.servlet.view.InternalResourceView;import java.io.File;import java.util.Locale;/** * HtmlResourceView * * @author jlin * @date 2019/3/11 09:50 * @Description */public class HtmlResourceView extends InternalResourceView &#123; @Override public boolean checkResource(Locale locale) &#123; File file = new File(this.getServletContext().getRealPath("/") + getUrl()); return file.exists();// 判断该页面是否存在 &#125;&#125; 配置xml12345678910111213141516&lt;!-- 定义JSP文件的位置 --&gt;&lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;viewClass&quot; value=&quot;org.springframework.web.servlet.view.JstlView&quot; /&gt; &lt;property name=&quot;order&quot; value=&quot;1&quot; /&gt; &lt;property name=&quot;prefix&quot; value=&quot;/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt;&lt;/bean&gt;&lt;!-- 定义HTML文件的位置 --&gt;&lt;bean id=&quot;htmlviewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;viewClass&quot; value=&quot;cn.joinhealth.interview.web.root.resolver.HtmlResourceView&quot;/&gt; &lt;property name=&quot;order&quot; value=&quot;0&quot; /&gt; &lt;!-- 前缀 --&gt; &lt;property name=&quot;prefix&quot; value=&quot;/&quot; /&gt; &lt;property name=&quot;suffix&quot; value=&quot;.html&quot; /&gt;&lt;/bean&gt;]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC的WebArgumentResolver注入当前登录用户]]></title>
    <url>%2Fposts%2Feaed%2F</url>
    <content type="text"><![CDATA[UserArgumentResolver.java 12345678910111213141516171819202122232425262728293031package cn.joinhealth.interview.web.root.resolver;import cn.joinhealth.interview.common.core.constant.SystemConstant;import cn.joinhealth.interview.module.manage.entity.User;import org.springframework.core.MethodParameter;import org.springframework.web.bind.support.WebArgumentResolver;import org.springframework.web.context.request.NativeWebRequest;import javax.servlet.http.HttpServletRequest;/** * UserArgumentResolver * * @author jlin * @date 2019/4/3 14:05 * @Description */public class UserArgumentResolver implements WebArgumentResolver &#123; @Override public Object resolveArgument(MethodParameter methodParameter, NativeWebRequest nativeWebRequest) throws Exception &#123; if (methodParameter.getParameterType() != null &amp;&amp; methodParameter.getParameterType().equals(User.class)) &#123; // 判断controller方法参数有没有写当前用户，如果有，这里返回即可，通常我们从session里面取出来 HttpServletRequest request = nativeWebRequest.getNativeRequest(HttpServletRequest.class); Object currentUser = request.getSession().getAttribute(SystemConstant.CLOUD_FOLLOWUP_KEY); return currentUser; &#125; return UNRESOLVED; &#125;&#125; spring-mvc.xml 使用 1234567@RequestMapping(value = "list", method = RequestMethod.GET)@ResponseBodypublic SimpleResult list(User user) throws Exception &#123; SimpleResult result = new SimpleResult(); result.addModel("permissionList", permissionService.list(user.getHospCode())); return result;&#125;]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC+FastJson 自定义日期转换器]]></title>
    <url>%2Fposts%2Fbc49%2F</url>
    <content type="text"><![CDATA[对于有的时候要输出日期格式为yyyy-MM-dd，而有的时候要输出yyyy-MM-dd hh:mm:ss时怎么办？ 第一种方案：纯注解式， 对日期类型的字段进行注解 123456789101112131415161718192021@JSONField(format = "yyyy-MM-dd")private Date updateDate;@JSONField(format = "yyyy-MM-dd hh:mm:ss")private Date createDate;public Date getUpdateDate() &#123; return updateDate;&#125;public void setUpdateDate(Date updateDate) &#123; this.updateDate = updateDate;&#125;public void setCreateDate(Date createDate) &#123; this.createDate = createDate;&#125;public Date getCreateDate() &#123; return createDate;&#125; 第二种方案：使用fastjson的WriteDateUseDateFormat配置（使得返回的日期类型默认为yyyy-MM-dd hh:mm:ss）, 特殊类型使用字段@JSONField来进行控制 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;!-- 默认的注解映射的支持，org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping --&gt;&lt;mvc:annotation-driven content-negotiation-manager="contentNegotiationManager"&gt; &lt;mvc:message-converters register-defaults="true"&gt; &lt;!-- 将Jackson2HttpMessageConverter的默认格式化输出为true --&gt; &lt;!-- 配置Fastjson支持 --&gt; &lt;bean class="com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter"&gt; &lt;property name="supportedMediaTypes"&gt; &lt;list&gt; &lt;value&gt;text/html;charset=UTF-8&lt;/value&gt; &lt;value&gt;application/json&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="features"&gt; &lt;list&gt; &lt;!-- 输出key时是否使用双引号 --&gt; &lt;value&gt;QuoteFieldNames&lt;/value&gt; &lt;!-- 是否输出值为null的字段 --&gt; &lt;!-- &lt;value&gt;WriteMapNullValue&lt;/value&gt; --&gt; &lt;!-- 数值字段如果为null,输出为0,而非null --&gt; &lt;value&gt;WriteNullNumberAsZero&lt;/value&gt; &lt;!-- List字段如果为null,输出为[],而非null --&gt; &lt;value&gt;WriteNullListAsEmpty&lt;/value&gt; &lt;!-- 字符类型字段如果为null,输出为"",而非null --&gt; &lt;value&gt;WriteNullStringAsEmpty&lt;/value&gt; &lt;!-- Boolean字段如果为null,输出为false,而非null --&gt; &lt;value&gt;WriteNullBooleanAsFalse&lt;/value&gt; &lt;!-- null String不输出 --&gt; &lt;value&gt;WriteNullStringAsEmpty&lt;/value&gt; &lt;!-- null String也要输出 --&gt; &lt;!-- &lt;value&gt;WriteMapNullValue&lt;/value&gt; --&gt; &lt;!-- Date的日期转换器 --&gt; &lt;value&gt;WriteDateUseDateFormat&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/mvc:message-converters&gt;&lt;/mvc:annotation-driven&gt;&lt;!-- REST中根据URL后缀自动判定Content-Type及相应的View --&gt;&lt;bean id="contentNegotiationManager" class="org.springframework.web.accept.ContentNegotiationManagerFactoryBean"&gt; &lt;property name="mediaTypes" &gt; &lt;map&gt; &lt;entry key="json" value="application/json"/&gt; &lt;/map&gt; &lt;/property&gt; &lt;!-- 这里是否忽略掉accept header，默认就是false --&gt; &lt;property name="ignoreAcceptHeader" value="true"/&gt; &lt;property name="favorPathExtension" value="true"/&gt;&lt;/bean&gt; 12345678910@JSONField(format = "yyyy-MM-dd")private Date updateDate;public Date getUpdateDate() &#123; return updateDate;&#125;public void setUpdateDate(Date updateDate) &#123; this.updateDate = updateDate;&#125; 第三种方案：使用FastJson的消息转换器， 特殊类型使用字段@JSONField来进行控制 12345678910111213141516171819202122232425262728293031323334353637import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.serializer.SerializeConfig;import com.alibaba.fastjson.serializer.SimpleDateFormatSerializer;import com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter;import org.springframework.http.HttpOutputMessage;import org.springframework.http.converter.HttpMessageNotWritableException;import java.io.IOException;import java.io.OutputStream;/** * 如果没有注入默认的日期格式，也没有配置&lt;value&gt;WriteDateUseDateFormat&lt;/value&gt;, 也没有属性注解@JSONField(format="yyyy-MM-dd hh:mm:ss") 则会转换输出时间戳 * 如果只配置&lt;value&gt;WriteDateUseDateFormat&lt;/value&gt;，则会转换输出yyyy-MM-dd hh:mm:ss * 配置&lt;value&gt;WriteDateUseDateFormat&lt;/value&gt;, 属性注解@JSONField(format="yyyy-MM-dd hh:mm:ss") 则会转换输出为属性注解的格式 * 如果注入了默认的日期格式，属性注解@JSONField(format="yyyy-MM-dd hh:mm:ss") 则会转换输出为属性注解的格式 * 如果注入了默认的日期格式，则会转换输出为默认的日期格式 * 如果三者都配置则会转换成属性注解的格式 * Created by PETER on 2016/2/5. */public class CustomerFastJsonHttpMessageConverter extends FastJsonHttpMessageConverter &#123; public static SerializeConfig mapping = new SerializeConfig(); private String defaultDateFormat; @Override protected void writeInternal(Object obj, HttpOutputMessage outputMessage) throws IOException, HttpMessageNotWritableException &#123; OutputStream out = outputMessage.getBody(); String text = JSON.toJSONString(obj, mapping, super.getFeatures()); byte[] bytes = text.getBytes(getCharset()); out.write(bytes); &#125; public void setDefaultDateFormat(String defaultDateFormat) &#123; mapping.put(java.util.Date.class, new SimpleDateFormatSerializer(defaultDateFormat)); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xmlns:mvc="http://www.springframework.org/schema/mvc" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.0.xsd"&gt; &lt;description&gt;Spring MVC Configuration&lt;/description&gt; &lt;!-- 加载配置属性文件 --&gt; &lt;context:property-placeholder ignore-unresolvable="true" location="classpath:/xmutca.properties" /&gt; &lt;!-- 扫描dubbo注解需要在controller之前，否则会造成无法注入的问题 --&gt; &lt;dubbo:annotation package="com.xmutca"&gt;&lt;/dubbo:annotation&gt; &lt;!-- 使用Annotation自动注册Bean,只扫描@Controller --&gt; &lt;context:component-scan base-package="com.xmutca" use-default-filters="false"&gt; &lt;!-- base-package 如果多个，用“,”分隔 --&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt; &lt;/context:component-scan&gt; &lt;!-- 默认的注解映射的支持，org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping --&gt; &lt;mvc:annotation-driven content-negotiation-manager="contentNegotiationManager"&gt; &lt;mvc:message-converters register-defaults="true"&gt; &lt;!-- 将Jackson2HttpMessageConverter的默认格式化输出为true --&gt; &lt;!-- 配置Fastjson支持 --&gt; &lt;bean class="com.ydyx.core.web.converter.CustomerFastJsonHttpMessageConverter"&gt; &lt;property name="supportedMediaTypes"&gt; &lt;list&gt; &lt;value&gt;text/html;charset=UTF-8&lt;/value&gt; &lt;value&gt;application/json&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="features"&gt; &lt;list&gt; &lt;!-- 输出key时是否使用双引号 --&gt; &lt;value&gt;QuoteFieldNames&lt;/value&gt; &lt;!-- 是否输出值为null的字段 --&gt; &lt;!-- &lt;value&gt;WriteMapNullValue&lt;/value&gt; --&gt; &lt;!-- 数值字段如果为null,输出为0,而非null --&gt; &lt;value&gt;WriteNullNumberAsZero&lt;/value&gt; &lt;!-- List字段如果为null,输出为[],而非null --&gt; &lt;value&gt;WriteNullListAsEmpty&lt;/value&gt; &lt;!-- 字符类型字段如果为null,输出为"",而非null --&gt; &lt;value&gt;WriteNullStringAsEmpty&lt;/value&gt; &lt;!-- Boolean字段如果为null,输出为false,而非null --&gt; &lt;value&gt;WriteNullBooleanAsFalse&lt;/value&gt; &lt;!-- null String不输出 --&gt; &lt;value&gt;WriteNullStringAsEmpty&lt;/value&gt; &lt;!-- null String也要输出 --&gt; &lt;!-- &lt;value&gt;WriteMapNullValue&lt;/value&gt; --&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="defaultDateFormat" value="yyyy-MM-dd"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/mvc:message-converters&gt; &lt;/mvc:annotation-driven&gt; &lt;!-- REST中根据URL后缀自动判定Content-Type及相应的View --&gt; &lt;bean id="contentNegotiationManager" class="org.springframework.web.accept.ContentNegotiationManagerFactoryBean"&gt; &lt;property name="mediaTypes" &gt; &lt;map&gt; &lt;entry key="json" value="application/json"/&gt; &lt;/map&gt; &lt;/property&gt; &lt;!-- 这里是否忽略掉accept header，默认就是false --&gt; &lt;property name="ignoreAcceptHeader" value="true"/&gt; &lt;property name="favorPathExtension" value="true"/&gt; &lt;/bean&gt; &lt;!-- 视图文件解析配置 --&gt; &lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="prefix" value="$&#123;web.view.prefix&#125;"/&gt; &lt;property name="suffix" value="$&#123;web.view.suffix&#125;"/&gt; &lt;/bean&gt; &lt;!-- 对静态资源文件的访问， 将无法mapping到Controller的path交给default servlet handler处理 --&gt; &lt;mvc:default-servlet-handler/&gt; &lt;!-- 定义无Controller的path&lt;-&gt;view直接映射 --&gt; &lt;mvc:view-controller path="/" view-name="redirect:$&#123;web.view.index&#125;"/&gt; &lt;!-- 基于注解式子的异常处理 --&gt; &lt;bean id="exceptionHandlerExceptionResolver" class="org.springframework.web.servlet.mvc.method.annotation.ExceptionHandlerExceptionResolver"&gt;&lt;/bean&gt; &lt;!-- Shiro end --&gt; &lt;!-- 上传文件拦截，设置最大上传文件大小 10M=10*1024*1024(B)=10485760 bytes --&gt; &lt;bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver"&gt; &lt;property name="maxUploadSize" value="$&#123;web.maxUploadSize&#125;" /&gt; &lt;/bean&gt;&lt;/beans&gt; 第四种方案：使用SpringMVC的自定义属性编辑器 123456789101112131415161718192021222324252627282930@InitBinderprotected void initBinder(WebDataBinder binder) &#123; // String类型转换，将所有传递进来的String进行前后空格处理， null字符串处理 binder.registerCustomEditor(String.class, new PropertyEditorSupport() &#123; @Override public void setAsText(String text) &#123; setValue(text == null ? null : text.trim()); &#125; @Override public String getAsText() &#123; Object value = getValue(); return value != null ? value.toString() : ""; &#125; &#125;); // Date 类型转换 binder.registerCustomEditor(Date.class, new PropertyEditorSupport() &#123; @Override public void setAsText(String text) &#123; setValue(DateUtils.parseDate(text)); &#125; @Override public String getAsText() &#123; Date date = (Date) getValue(); return DateUtils.formatDate(date, "yyyy-MM-dd"); &#125; &#125;);&#125; DateUtils源代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;import org.apache.commons.lang.time.DateFormatUtils;/** * 日期工具类, 继承org.apache.commons.lang.time.DateUtils类 * */public class DateUtils extends org.apache.commons.lang3.time.DateUtils &#123; private static String[] parsePatterns = &#123; "yyyy-MM-dd", "yyyy-MM-dd HH:mm:ss", "yyyy-MM-dd HH:mm", "yyyy/MM/dd", "yyyy/MM/dd HH:mm:ss", "yyyy/MM/dd HH:mm" ,"yyyyMMdd"&#125;; /** * 得到当前日期字符串 格式（yyyy-MM-dd） */ public static String getDate() &#123; return getDate("yyyy-MM-dd"); &#125; /** * 得到当前日期字符串 格式（yyyy-MM-dd） pattern可以为："yyyy-MM-dd" "HH:mm:ss" "E" */ public static String getDate(String pattern) &#123; return DateFormatUtils.format(new Date(), pattern); &#125; /** * 得到日期字符串 默认格式（yyyy-MM-dd） pattern可以为："yyyy-MM-dd" "HH:mm:ss" "E" */ public static String formatDate(Date date, Object... pattern) &#123; String formatDate = null; if (pattern != null &amp;&amp; pattern.length &gt; 0) &#123; formatDate = DateFormatUtils.format(date, pattern[0].toString()); &#125; else &#123; formatDate = DateFormatUtils.format(date, "yyyy-MM-dd"); &#125; return formatDate; &#125; /** * 得到日期时间字符串，转换格式（yyyy-MM-dd HH:mm:ss） */ public static String formatDateTime(Date date) &#123; return formatDate(date, "yyyy-MM-dd HH:mm:ss"); &#125; /** * 得到当前时间字符串 格式（HH:mm:ss） */ public static String getTime() &#123; return formatDate(new Date(), "HH:mm:ss"); &#125; /** * 得到当前日期和时间字符串 格式（yyyy-MM-dd HH:mm:ss） */ public static String getDateTime() &#123; return formatDate(new Date(), "yyyy-MM-dd HH:mm:ss"); &#125; /** * 得到当前年份字符串 格式（yyyy） */ public static String getYear() &#123; return formatDate(new Date(), "yyyy"); &#125; /** * 得到当前月份字符串 格式（MM） */ public static String getMonth() &#123; return formatDate(new Date(), "MM"); &#125; /** * 得到当天字符串 格式（dd） */ public static String getDay() &#123; return formatDate(new Date(), "dd"); &#125; /** * 得到当前星期字符串 格式（E）星期几 */ public static String getWeek() &#123; return formatDate(new Date(), "E"); &#125; /** * 日期型字符串转化为日期 格式 &#123; "yyyy-MM-dd", "yyyy-MM-dd HH:mm:ss", "yyyy-MM-dd HH:mm", * "yyyy/MM/dd", "yyyy/MM/dd HH:mm:ss", "yyyy/MM/dd HH:mm", "yyyyMMdd" &#125; */ public static Date parseDate(Object str) &#123; if (str == null) &#123; return null; &#125; try &#123; return parseDate(str.toString(), parsePatterns); &#125; catch (ParseException e) &#123; return null; &#125; &#125; /** * 获取过去的天数 * * @param date * @return */ public static long pastDays(Date date) &#123; long t = new Date().getTime() - date.getTime(); return t / (24 * 60 * 60 * 1000); &#125; /** * 获取过去的小时 * @param date * @return */ public static long pastHour(Date date) &#123; long t = new Date().getTime()-date.getTime(); return t/(60*60*1000); &#125; /** * 获取过去的分钟 * @param date * @return */ public static long pastMinutes(Date date) &#123; long t = new Date().getTime()-date.getTime(); return t/(60*1000); &#125; /** * 转换为时间（天,时:分:秒.毫秒） * @param timeMillis * @return */ public static String formatDateTime(long timeMillis)&#123; long day = timeMillis/(24*60*60*1000); long hour = (timeMillis/(60*60*1000)-day*24); long min = ((timeMillis/(60*1000))-day*24*60-hour*60); long s = (timeMillis/1000-day*24*60*60-hour*60*60-min*60); long sss = (timeMillis-day*24*60*60*1000-hour*60*60*1000-min*60*1000-s*1000); return (day&gt;0?day+",":"")+hour+":"+min+":"+s+"."+sss; &#125; /** * 获取某一天的开始时间（0点） * @param date * @return */ public static Date getDateStart(Date date) &#123; if (date == null) &#123; return null; &#125; SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); try &#123; date = sdf.parse(formatDate(date, "yyyy-MM-dd") + " 00:00:00"); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; return date; &#125; /** * 获取某一天的结束时间(23:59) * * @param date * @return */ public static Date getDateEnd(Date date) &#123; if (date == null) &#123; return null; &#125; SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); try &#123; date = sdf.parse(formatDate(date, "yyyy-MM-dd") + " 23:59:59"); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; return date; &#125; /** * 比较两个日期时间的大小,反回1表示preDateStr &gt; nextDateStr，0就相等，-1为小于 * @author: weihuang.peng * @param preDateStr * @param nextDateStr * @return result */ public static int compareDate(Object preDateStr, Object nextDateStr) &#123; int result = 0; Date preDate = parseDate(preDateStr); Date nextDate = parseDate(nextDateStr); try &#123; result = preDate.compareTo(nextDate); &#125; catch (Exception e) &#123; result = 0; e.printStackTrace(); &#125; return result; &#125; /** * 获取某一天的前几天或者后几天，根据数字符号决定天数 * @author: weihuang.peng * @param date * @param days * @return */ public static String getPastDayStr(Object dateObj, int days) &#123; Date date = parseDate(dateObj); long time = date.getTime() + days * (long)(24 * 60 * 60 * 1000); return formatDate(new Date(time)); &#125; /** * preDateStr - nextDateStr 返回秒数 * @author: huiyang.yu * @param preDateStr * @param nextDateStr * @return */ public static long getSubactDate(Object preDateStr, Object nextDateStr) &#123; Date preDate = parseDate(preDateStr); Date nextDate = parseDate(nextDateStr); long result = (preDate.getTime() - nextDate.getTime()) / 1000L; return result; &#125; /** * 返回过去的天数： preDateStr - nextDateStr * @author: weihuang.peng * @param preDateStr * @param nextDateStr * @return */ public static long getDifferDate(Object preDateStr, Object nextDateStr) &#123; return getSubactDate(preDateStr, nextDateStr) / (60 * 60 * 24L); &#125; /** * 传入日期时间与当前系统日期时间的比较, * 若日期相同，则根据时分秒来返回 , * 否则返回具体日期 * @author: huiyang.yu * @param updateDate 传入日期 * @param updateTime 传入时间 * @return 日期或者 xx小时前||xx分钟前||xx秒前 */ public static String getNewUpdateDateString(String updateDate, String updateTime) &#123; String result = updateDate; long time = 0; if (updateDate.equals(DateUtils.getDate())) &#123; time = DateUtils.getSubactDate(DateUtils.getDateTime(), updateDate + " " + updateTime); if (time &gt;= 3600) &#123; result = time / 3600 + "小时前"; &#125; else if (time &gt;= 60) &#123; result = time / 60 + "分钟前"; &#125; else if (time &gt;= 1) &#123; result = time + "秒前"; &#125; else &#123; result = "刚刚"; &#125; &#125; else if (result.length() &gt;= 10) &#123; result = result.substring(5); &#125; return result; &#125; &#125;]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 的编程式事务管理概述]]></title>
    <url>%2Fposts%2F8f5%2F</url>
    <content type="text"><![CDATA[开始之前关于本教程本教程将深入讲解 Spring 简单而强大的事务管理功能，包括编程式事务和声明式事务。通过对本教程的学习，您将能够理解 Spring 事务管理的本质，并灵活运用之。 先决条件本教程假定您已经掌握了 Java 基础知识，并对 Spring 有一定了解。您还需要具备基本的事务管理的知识，比如：事务的定义，隔离级别的概念，等等。本文将直接使用这些概念而不做详细解释。另外，您最好掌握数据库的基础知识，虽然这不是必须。 系统需求要试验这份教程中的工具和示例，硬件配置需求为：至少带有 512MB 内存（推荐 1GB）的系统。需要安装以下软件： Sun JDK 5.0 或更新版本或 IBM Developer Kit for the Java 5 platform 版本。 Spring framework 2.5。本教程附带的示例代码已经在 Spring 2.5.6 上测试过。 MySQL 5.0 或更新版本。 Spring 事务属性分析事务管理对于企业应用而言至关重要。它保证了用户的每一次操作都是可靠的，即便出现了异常的访问情况，也不至于破坏后台数据的完整性。就像银行的自助取款机，通常都能正常为客户服务，但是也难免遇到操作过程中机器突然出故障的情况，此时，事务就必须确保出故障前对账户的操作不生效，就像用户刚才完全没有使用过取款机一样，以保证用户和银行的利益都不受损失。 在 Spring 中，事务是通过 TransactionDefinition 接口来定义的。该接口包含与事务属性有关的方法。具体如清单1所示： 清单1. TransactionDefinition 接口中定义的主要方法123456public interface TransactionDefinition&#123; int getIsolationLevel(); int getPropagationBehavior(); int getTimeout(); boolean isReadOnly();&#125; 也许你会奇怪，为什么接口只提供了获取属性的方法，而没有提供相关设置属性的方法。其实道理很简单，事务属性的设置完全是程序员控制的，因此程序员可以自定义任何设置属性的方法，而且保存属性的字段也没有任何要求。唯一的要求的是，Spring 进行事务操作的时候，通过调用以上接口提供的方法必须能够返回事务相关的属性取值。 事务隔离级别隔离级别是指若干个并发的事务之间的隔离程度。TransactionDefinition 接口中定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT：这是默认值，表示使用底层数据库的默认隔离级别。对大部分数据库而言，通常这值就是TransactionDefinition.ISOLATION_READ_COMMITTED。 TransactionDefinition.ISOLATION_READ_UNCOMMITTED：该隔离级别表示一个事务可以读取另一个事务修改但还没有提交的数据。该级别不能防止脏读和不可重复读，因此很少使用该隔离级别。 TransactionDefinition.ISOLATION_READ_COMMITTED：该隔离级别表示一个事务只能读取另一个事务已经提交的数据。该级别可以防止脏读，这也是大多数情况下的推荐值。 TransactionDefinition.ISOLATION_REPEATABLE_READ：该隔离级别表示一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。即使在多次查询之间有新增的数据满足该查询，这些新增的记录也会被忽略。该级别可以防止脏读和不可重复读。 TransactionDefinition.ISOLATION_SERIALIZABLE：所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 事务传播行为所谓事务的传播行为是指，如果在开始当前事务之前，一个事务上下文已经存在，此时有若干选项可以指定一个事务性方法的执行行为。在TransactionDefinition定义中包括了如下几个表示传播行为的常量： TransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。 TransactionDefinition.PROPAGATION_MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。 TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 这里需要指出的是，前面的六种事务传播行为是 Spring 从 EJB 中引入的，他们共享相同的概念。而 PROPAGATION_NESTED是 Spring 所特有的。以 PROPAGATION_NESTED 启动的事务内嵌于外部事务中（如果存在外部事务的话），此时，内嵌事务并不是一个独立的事务，它依赖于外部事务的存在，只有通过外部的事务提交，才能引起内部事务的提交，嵌套的子事务不能单独提交。如果熟悉 JDBC 中的保存点（SavePoint）的概念，那嵌套事务就很容易理解了，其实嵌套的子事务就是保存点的一个应用，一个事务中可以包括多个保存点，每一个嵌套子事务。另外，外部事务的回滚也会导致嵌套子事务的回滚。 事务超时所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。 事务的只读属性事务的只读属性是指，对事务性资源进行只读操作或者是读写操作。所谓事务性资源就是指那些被事务管理的资源，比如数据源、 JMS 资源，以及自定义的事务性资源等等。如果确定只对事务性资源进行只读操作，那么我们可以将事务标志为只读的，以提高事务处理的性能。在 TransactionDefinition 中以 boolean 类型来表示该事务是否只读。 事务的回滚规则通常情况下，如果在事务中抛出了未检查异常（继承自 RuntimeException 的异常），则默认将回滚事务。如果没有抛出任何异常，或者抛出了已检查异常，则仍然提交事务。这通常也是大多数开发者希望的处理方式，也是 EJB 中的默认处理方式。但是，我们可以根据需要人为控制事务在抛出某些未检查异常时任然提交事务，或者在抛出某些已检查异常时回滚事务。 Spring 事务管理 API 分析Spring 框架中，涉及到事务管理的 API 大约有100个左右，其中最重要的有三个：TransactionDefinition、PlatformTransactionManager、TransactionStatus。所谓事务管理，其实就是“按照给定的事务规则来执行提交或者回滚操作”。“给定的事务规则”就是用 TransactionDefinition 表示的，“按照……来执行提交或者回滚操作”便是用 PlatformTransactionManager 来表示，而 TransactionStatus 用于表示一个运行着的事务的状态。打一个不恰当的比喻，TransactionDefinition 与 TransactionStatus 的关系就像程序和进程的关系。 TransactionDef…该接口在前面已经介绍过，它用于定义一个事务。它包含了事务的静态属性，比如：事务传播行为、超时时间等等。Spring 为我们提供了一个默认的实现类：DefaultTransactionDefinition，该类适用于大多数情况。如果该类不能满足需求，可以通过实现 TransactionDefinition 接口来实现自己的事务定义。 PlatformTrans…PlatformTransactionManager 用于执行具体的事务操作。接口定义如清单2所示： 清单2. PlatformTransactionManager 接口中定义的主要方法123456Public interface PlatformTransactionManager&#123; TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; void commit(TransactionStatus status)throws TransactionException; void rollback(TransactionStatus status)throws TransactionException;&#125; 根据底层所使用的不同的持久化 API 或框架，PlatformTransactionManager 的主要实现类大致如下： DataSourceTransactionManager：适用于使用JDBC和iBatis进行数据持久化操作的情况。 HibernateTransactionManager：适用于使用Hibernate进行数据持久化操作的情况。 JpaTransactionManager：适用于使用JPA进行数据持久化操作的情况。 另外还有JtaTransactionManager 、JdoTransactionManager、JmsTransactionManager等等。 如果我们使用JTA进行事务管理，我们可以通过 JNDI 和 Spring 的 JtaTransactionManager 来获取一个容器管理的 DataSource。JtaTransactionManager 不需要知道 DataSource 和其他特定的资源，因为它将使用容器提供的全局事务管理。而对于其他事务管理器，比如DataSourceTransactionManager，在定义时需要提供底层的数据源作为其属性，也就是 DataSource。与 HibernateTransactionManager 对应的是 SessionFactory，与 JpaTransactionManager 对应的是 EntityManagerFactory 等等。 TransactionStatusPlatformTransactionManager.getTransaction(…) 方法返回一个 TransactionStatus 对象。返回的TransactionStatus 对象可能代表一个新的或已经存在的事务（如果在当前调用堆栈有一个符合条件的事务）。TransactionStatus 接口提供了一个简单的控制事务执行和查询事务状态的方法。该接口定义如清单3所示： 清单3. TransactionStatus 接口中定义的主要方法12345public interface TransactionStatus&#123; boolean isNewTransaction(); void setRollbackOnly(); boolean isRollbackOnly();&#125; 编程式事务管理Spring 的编程式事务管理概述在 Spring 出现以前，编程式事务管理对基于 POJO 的应用来说是唯一选择。用过 Hibernate 的人都知道，我们需要在代码中显式调用beginTransaction()、commit()、rollback()等事务管理相关的方法，这就是编程式事务管理。通过 Spring 提供的事务管理 API，我们可以在代码中灵活控制事务的执行。在底层，Spring 仍然将事务操作委托给底层的持久化框架来执行。 基于底层 API 的编程式事务管理根据PlatformTransactionManager、TransactionDefinition 和 TransactionStatus 三个核心接口，我们完全可以通过编程的方式来进行事务管理。示例代码如清单4所示： 清单4. 基于底层 API 的事务管理示例代码12345678910111213141516171819public class BankServiceImpl implements BankService &#123;private BankDao bankDao;private TransactionDefinition txDefinition;private PlatformTransactionManager txManager;......public boolean transfer(Long fromId， Long toId， double amount) &#123; TransactionStatus txStatus = txManager.getTransaction(txDefinition); boolean result = false; try &#123; result = bankDao.transfer(fromId， toId， amount); txManager.commit(txStatus); &#125; catch (Exception e) &#123; result = false; txManager.rollback(txStatus); System.out.println("Transfer Error!"); &#125; return result; &#125;&#125; 相应的配置文件如清单5所示： 清单5. 基于底层API的事务管理示例配置文件如上所示，我们在类中增加了两个属性：一个是 TransactionDefinition 类型的属性，它用于定义一个事务；另一个是 PlatformTransactionManager 类型的属性，用于执行事务管理操作。 如果方法需要实施事务管理，我们首先需要在方法开始执行前启动一个事务，调用PlatformTransactionManager.getTransaction(…) 方法便可启动一个事务。创建并启动了事务之后，便可以开始编写业务逻辑代码，然后在适当的地方执行事务的提交或者回滚。 基于 TransactionTemplate 的编程式事务管理通过前面的示例可以发现，这种事务管理方式很容易理解，但令人头疼的是，事务管理的代码散落在业务逻辑代码中，破坏了原有代码的条理性，并且每一个业务方法都包含了类似的启动事务、提交/回滚事务的样板代码。幸好，Spring 也意识到了这些，并提供了简化的方法，这就是 Spring 在数据访问层非常常见的模板回调模式。如清单6所示： 清单6. 基于 TransactionTemplate 的事务管理示例代码1234567891011121314151617181920public class BankServiceImpl implements BankService &#123;private BankDao bankDao;private TransactionTemplate transactionTemplate;......public boolean transfer(final Long fromId， final Long toId， final double amount) &#123; return (Boolean) transactionTemplate.execute(new TransactionCallback()&#123; public Object doInTransaction(TransactionStatus status) &#123; Object result; try &#123; result = bankDao.transfer(fromId， toId， amount); &#125; catch (Exception e) &#123; status.setRollbackOnly(); result = false; System.out.println("Transfer Error!"); &#125; return result; &#125; &#125;); &#125;&#125; 相应的XML配置如下： 清单 7. 基于 TransactionTemplate 的事务管理示例配置文件TransactionTemplate 的 execute() 方法有一个 TransactionCallback 类型的参数，该接口中定义了一个 doInTransaction() 方法，通常我们以匿名内部类的方式实现 TransactionCallback 接口，并在其 doInTransaction() 方法中书写业务逻辑代码。这里可以使用默认的事务提交和回滚规则，这样在业务代码中就不需要显式调用任何事务管理的 API。doInTransaction() 方法有一个TransactionStatus 类型的参数，我们可以在方法的任何位置调用该参数的 setRollbackOnly() 方法将事务标识为回滚的，以执行事务回滚。 根据默认规则，如果在执行回调方法的过程中抛出了未检查异常，或者显式调用了TransacationStatus.setRollbackOnly() 方法，则回滚事务；如果事务执行完成或者抛出了 checked 类型的异常，则提交事务。 TransactionCallback 接口有一个子接口 TransactionCallbackWithoutResult，该接口中定义了一个 doInTransactionWithoutResult() 方法，TransactionCallbackWithoutResult 接口主要用于事务过程中不需要返回值的情况。当然，对于不需要返回值的情况，我们仍然可以使用 TransactionCallback 接口，并在方法中返回任意值即可。 声明式事务管理Spring 的声明式事务管理概述Spring 的声明式事务管理在底层是建立在 AOP 的基础之上的。其本质是对方法前后进行拦截，然后在目标方法开始之前创建或者加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。 声明式事务最大的优点就是不需要通过编程的方式管理事务，这样就不需要在业务逻辑代码中掺杂事务管理的代码，只需在配置文件中做相关的事务规则声明（或通过等价的基于标注的方式），便可以将事务规则应用到业务逻辑中。因为事务管理本身就是一个典型的横切逻辑，正是 AOP 的用武之地。Spring 开发团队也意识到了这一点，为声明式事务提供了简单而强大的支持。 声明式事务管理曾经是 EJB 引以为傲的一个亮点，如今 Spring 让 POJO 在事务管理方面也拥有了和 EJB 一样的待遇，让开发人员在 EJB 容器之外也用上了强大的声明式事务管理功能，这主要得益于 Spring 依赖注入容器和 Spring AOP 的支持。依赖注入容器为声明式事务管理提供了基础设施，使得 Bean 对于 Spring 框架而言是可管理的；而 Spring AOP 则是声明式事务管理的直接实现者，这一点通过清单8可以看出来。 通常情况下，笔者强烈建议在开发中使用声明式事务，不仅因为其简单，更主要是因为这样使得纯业务代码不被污染，极大方便后期的代码维护。 和编程式事务相比，声明式事务唯一不足地方是，后者的最细粒度只能作用到方法级别，无法做到像编程式事务那样可以作用到代码块级别。但是即便有这样的需求，也存在很多变通的方法，比如，可以将需要进行事务管理的代码块独立为方法等等。 下面就来看看 Spring 为我们提供的声明式事务管理功能。 基于 TransactionInter… 的声明式事务管理最初，Spring 提供了 TransactionInterceptor 类来实施声明式事务管理功能。先看清单8的配置文件： 清单 8. 基于 TransactionInterceptor 的事务管理示例配置文件12345678910111213141516171819202122......PROPAGATION_REQUIRED...... 首先，我们配置了一个 TransactionInterceptor 来定义相关的事务规则，他有两个主要的属性：一个是 transactionManager，用来指定一个事务管理器，并将具体事务相关的操作委托给它；另一个是 Properties 类型的 transactionAttributes 属性，它主要用来定义事务规则，该属性的每一个键值对中，键指定的是方法名，方法名可以使用通配符，而值就表示相应方法的所应用的事务属性。 指定事务属性的取值有较复杂的规则，这在 Spring 中算得上是一件让人头疼的事。具体的书写规则如下： 传播行为 [，隔离级别] [，只读属性] [，超时属性] [不影响提交的异常] [，导致回滚的异常] 传播行为是唯一必须设置的属性，其他都可以忽略，Spring为我们提供了合理的默认值。 传播行为的取值必须以“PROPAGATION_”开头，具体包括：PROPAGATION_MANDATORY、PROPAGATION_NESTED、PROPAGATION_NEVER、PROPAGATION_NOT_SUPPORTED、PROPAGATION_REQUIRED、PROPAGATION_REQUIRES_NEW、PROPAGATION_SUPPORTS，共七种取值。 隔离级别的取值必须以“ISOLATION_”开头，具体包括：ISOLATION_DEFAULT、ISOLATION_READ_COMMITTED、ISOLATION_READ_UNCOMMITTED、ISOLATION_REPEATABLE_READ、ISOLATION_SERIALIZABLE，共五种取值。 如果事务是只读的，那么我们可以指定只读属性，使用“readOnly”指定。否则我们不需要设置该属性。 超时属性的取值必须以“TIMEOUT_”开头，后面跟一个int类型的值，表示超时时间，单位是秒。 不影响提交的异常是指，即使事务中抛出了这些类型的异常，事务任然正常提交。必须在每一个异常的名字前面加上“+”。异常的名字可以是类名的一部分。比如“+RuntimeException”、“+tion”等等。 导致回滚的异常是指，当事务中抛出这些类型的异常时，事务将回滚。必须在每一个异常的名字前面加上“-”。异常的名字可以是类名的全部或者部分，比如“-RuntimeException”、“-tion”等等。 以下是两个示例： 1PROPAGATION_REQUIRED，ISOLATION_READ_COMMITTED，TIMEOUT_20，+AbcException，+DefException，-HijException 以上表达式表示，针对所有方法名以 Service 结尾的方法，使用 PROPAGATION_REQUIRED 事务传播行为，事务的隔离级别是 ISOLATION_READ_COMMITTED，超时时间为20秒，当事务抛出 AbcException 或者 DefException 类型的异常，则仍然提交，当抛出 HijException 类型的异常时必须回滚事务。这里没有指定”readOnly”，表示事务不是只读的。 1PROPAGATION_REQUIRED，readOnly 以上表达式表示，针对所有方法名为 test 的方法，使用 PROPAGATION_REQUIRED 事务传播行为，并且该事务是只读的。除此之外，其他的属性均使用默认值。比如，隔离级别和超时时间使用底层事务性资源的默认值，并且当发生未检查异常，则回滚事务，发生已检查异常则仍提交事务。 配置好了 TransactionInterceptor，我们还需要配置一个 ProxyFactoryBean 来组装 target 和advice。这也是典型的 Spring AOP 的做法。通过 ProxyFactoryBean 生成的代理类就是织入了事务管理逻辑后的目标类。至此，声明式事务管理就算是实现了。我们没有对业务代码进行任何操作，所有设置均在配置文件中完成，这就是声明式事务的最大优点。 基于 TransactionProxy… 的声明式事务管理前面的声明式事务虽然好，但是却存在一个非常恼人的问题：配置文件太多。我们必须针对每一个目标对象配置一个 ProxyFactoryBean；另外，虽然可以通过父子 Bean 的方式来复用 TransactionInterceptor 的配置，但是实际的复用几率也不高；这样，加上目标对象本身，每一个业务类可能需要对应三个 配置，随着业务类的增多，配置文件将会变得越来越庞大，管理配置文件又成了问题。 为了缓解这个问题，Spring 为我们提供了 TransactionProxyFactoryBean，用于将TransactionInterceptor 和 ProxyFactoryBean 的配置合二为一。如清单9所示： 清单9. 基于 TransactionProxyFactoryBean 的事务管理示例配置文件123456789101112131415......PROPAGATION_REQUIRED...... 如此一来，配置文件与先前相比简化了很多。我们把这种配置方式称为 Spring 经典的声明式事务管理。相信在早期使用 Spring 的开发人员对这种配置声明式事务的方式一定非常熟悉。 但是，显式为每一个业务类配置一个 TransactionProxyFactoryBean 的做法将使得代码显得过于刻板，为此我们可以使用自动创建代理的方式来将其简化，使用自动创建代理是纯 AOP 知识，请读者参考相关文档，不在此赘述。 基于 命名空间的声明式事务管理前面两种声明式事务配置方式奠定了 Spring 声明式事务管理的基石。在此基础上，Spring 2.x 引入了 命名空间，结合使用 命名空间，带给开发人员配置声明式事务的全新体验，配置变得更加简单和灵活。另外，得益于 命名空间的切点表达式支持，声明式事务也变得更加强大。 如清单10所示： 清单10. 基于 的事务管理示例配置文件12345678910111213141516............ 如果默认的事务属性就能满足要求，那么代码简化为如清单 11 所示： 清单 11. 简化后的基于 的事务管理示例配置文件1234567891011............ 由于使用了切点表达式，我们就不需要针对每一个业务类创建一个代理对象了。另外，如果配置的事务管理器 Bean 的名字取值为“transactionManager”，则我们可以省略 tx:advice 的 transaction-manager 属性，因为该属性的默认值即为“transactionManager”。 基于 @Transactional 的声明式事务管理除了基于命名空间的事务配置方式，Spring 2.x 还引入了基于 Annotation 的方式，具体主要涉及@Transactional 标注。@Transactional 可以作用于接口、接口方法、类以及类方法上。当作用于类上时，该类的所有 public 方法将都具有该类型的事务属性，同时，我们也可以在方法级别使用该标注来覆盖类级别的定义。如清单12所示： 清单12. 基于 @Transactional 的事务管理示例配置文件1234@Transactional(propagation = Propagation.REQUIRED)public boolean transfer(Long fromId， Long toId， double amount) &#123;return bankDao.transfer(fromId， toId， amount);&#125; Spring 使用 BeanPostProcessor 来处理 Bean 中的标注，因此我们需要在配置文件中作如下声明来激活该后处理 Bean，如清单13所示： 清单13. 启用后处理Bean的配置与前面相似，transaction-manager 属性的默认值是 transactionManager，如果事务管理器 Bean 的名字即为该值，则可以省略该属性。 虽然 @Transactional 注解可以作用于接口、接口方法、类以及类方法上，但是 Spring 小组建议不要在接口或者接口方法上使用该注解，因为这只有在使用基于接口的代理时它才会生效。另外， @Transactional 注解应该只被应用到 public 方法上，这是由 Spring AOP 的本质决定的。如果你在 protected、private 或者默认可见性的方法上使用 @Transactional 注解，这将被忽略，也不会抛出任何异常。 基于 命名空间和基于 @Transactional 的事务声明方式各有优缺点。基于 的方式，其优点是与切点表达式结合，功能强大。利用切点表达式，一个配置可以匹配多个方法，而基于 @Transactional 的方式必须在每一个需要使用事务的方法或者类上用 @Transactional 标注，尽管可能大多数事务的规则是一致的，但是对 @Transactional 而言，也无法重用，必须逐个指定。另一方面，基于 @Transactional 的方式使用起来非常简单明了，没有学习成本。开发人员可以根据需要，任选其中一种使用，甚至也可以根据需要混合使用这两种方式。 如果不是对遗留代码进行维护，则不建议再使用基于 TransactionInterceptor 以及基于TransactionProxyFactoryBean 的声明式事务管理方式，但是，学习这两种方式非常有利于对底层实现的理解。 虽然上面共列举了四种声明式事务管理方式，但是这样的划分只是为了便于理解，其实后台的实现方式是一样的，只是用户使用的方式不同而已。 结束语本教程的知识点大致总结如下： 基于 TransactionDefinition、PlatformTransactionManager、TransactionStatus 编程式事务管理是 Spring 提供的最原始的方式，通常我们不会这么写，但是了解这种方式对理解 Spring 事务管理的本质有很大作用。 基于 TransactionTemplate 的编程式事务管理是对上一种方式的封装，使得编码更简单、清晰。 基于 TransactionInterceptor 的声明式事务是 Spring 声明式事务的基础，通常也不建议使用这种方式，但是与前面一样，了解这种方式对理解 Spring 声明式事务有很大作用。 基于 TransactionProxyFactoryBean 的声明式事务是上中方式的改进版本，简化的配置文件的书写，这是 Spring 早期推荐的声明式事务管理方式，但是在 Spring 2.0 中已经不推荐了。 基于 和 命名空间的声明式事务管理是目前推荐的方式，其最大特点是与 Spring AOP 结合紧密，可以充分利用切点表达式的强大支持，使得管理事务更加灵活。 基于 @Transactional 的方式将声明式事务管理简化到了极致。开发人员只需在配置文件中加上一行启用相关后处理 Bean 的配置，然后在需要实施事务管理的方法或者类上使用 @Transactional 指定事务规则即可实现事务管理，而且功能也不必其他方式逊色。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis配置]]></title>
    <url>%2Fposts%2Fc0b7%2F</url>
    <content type="text"><![CDATA[Redis.config配置详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380# 当配置中需要配置内存大小时，可以使用 1k, 5GB, 4M 等类似的格式，其转换方式如下(不区分大小写)## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## 内存配置大小写是一样的.比如 1gb 1Gb 1GB 1gB# daemonize no 默认情况下，redis不是在后台运行的，如果需要在后台运行，把该项的值更改为yesdaemonize yes# 当redis在后台运行的时候，Redis默认会把pid文件放在/var/run/redis.pid，你可以配置到其他地址。# 当运行多个redis服务时，需要指定不同的pid文件和端口pidfile /var/run/redis.pid # 指定redis运行的端口，默认是6379port 6379# 指定redis只接收来自于该IP地址的请求，如果不进行设置，那么将处理所有请求，# 在生产环境中最好设置该项bind 127.0.0.1 192.168.3.222# Specify the path for the unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 755# 设置客户端连接时的超时时间，单位为秒。当客户端在这段时间内没有发出任何指令，那么关闭该连接# 0是关闭此设置timeout 0# 指定日志记录级别# Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose# debug 记录很多信息，用于开发和测试# varbose 有用的信息，不像debug会记录那么多# notice 普通的verbose，常用于生产环境# warning 只有非常重要或者严重的信息会记录到日志loglevel debug# 配置log文件地址# 默认值为stdout，标准输出，若后台模式会输出到/dev/null#logfile stdoutlogfile /var/log/redis/redis.log# To enable logging to the system logger, just set &apos;syslog-enabled&apos; to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# 可用数据库数# 默认值为16，默认数据库为0，数据库范围在0-（database-1）之间databases 16################################ 快照 ################################## 保存数据到磁盘，格式如下:## save ## 指出在多长时间内，有多少次更新操作，就将数据同步到数据文件rdb。# 相当于条件触发抓取快照，这个可以多个条件配合## 比如默认配置文件中的设置，就设置了三个条件## save 900 1 900秒内至少有1个key被改变# save 300 10 300秒内至少有300个key被改变# save 60 10000 60秒内至少有10000个key被改变save 900 1save 300 10save 60 10000# 存储至本地数据库时（持久化到rdb文件）是否压缩数据，默认为yesrdbcompression yes# 本地持久化数据库文件名，默认值为dump.rdbdbfilename dump.rdb# 工作目录## 数据库镜像备份的文件放置的路径。# 这里的路径跟文件名要分开配置是因为redis在进行备份时，先会将当前数据库的状态写入到一个临时文件中，等备份完成时，# 再把该该临时文件替换为上面所指定的文件，而这里的临时文件和上面所配置的备份文件都会放在这个指定的路径当中。## AOF文件也会存放在这个目录下面## 注意这里必须制定一个目录而不是文件dir ./################################# 复制 ################################## 主从复制. 设置该数据库为其他数据库的从数据库.# 设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步## slaveof # 当master服务设置了密码保护时(用requirepass制定的密码)# slav服务连接master的密码## masterauth # 当从库同主机失去连接或者复制正在进行，从机库有两种运行方式：## 1) 如果slave-serve-stale-data设置为yes(默认设置)，从库会继续相应客户端的请求## 2) 如果slave-serve-stale-data是指为no，出去INFO和SLAVOF命令之外的任何请求都会返回一个# 错误&quot;SYNC with master in progress&quot;#slave-serve-stale-data yes# 从库会按照一个时间间隔向主库发送PINGs.可以通过repl-ping-slave-period设置这个时间间隔，默认是10秒## repl-ping-slave-period 10# repl-timeout 设置主库批量数据传输时间或者ping回复时间间隔，默认值是60秒# 一定要确保repl-timeout大于repl-ping-slave-period# repl-timeout 60################################## 安全 #################################### 设置客户端连接后进行任何其他指定前需要使用的密码。# 警告：因为redis速度相当快，所以在一台比较好的服务器下，一个外部的用户可以在一秒钟进行150K次的密码尝试，这意味着你需要指定非常非常强大的密码来防止暴力破解## requirepass foobared# 命令重命名.## 在一个共享环境下可以重命名相对危险的命令。比如把CONFIG重名为一个不容易猜测的字符。## 举例:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## 如果想删除一个命令，直接把它重命名为一个空字符&quot;&quot;即可，如下：## rename-command CONFIG &quot;&quot;################################### 约束 ##################################### 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，# 如果设置 maxclients 0，表示不作限制。# 当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息## maxclients 128# 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key# Redis同时也会移除空的list对象## 当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作## 注意：Redis新的vm机制，会把Key存放内存，Value会存放在swap区## maxmemory的设置比较适合于把redis当作于类似memcached的缓存来使用，而不适合当做一个真实的DB。# 当把Redis当做一个真实的数据库使用的时候，内存使用将是一个很大的开销# maxmemory # 当内存达到最大值的时候Redis会选择删除哪些数据？有五种方式可供选择## volatile-lru -&gt; 利用LRU算法移除设置过过期时间的key (LRU:最近使用 Least Recently Used )# allkeys-lru -&gt; 利用LRU算法移除任何key# volatile-random -&gt; 移除设置过过期时间的随机key# allkeys-&gt;random -&gt; remove a random key, any key# volatile-ttl -&gt; 移除即将过期的key(minor TTL)# noeviction -&gt; 不移除任何可以，只是返回一个写错误## 注意：对于上面的策略，如果没有合适的key可以移除，当写的时候Redis会返回一个错误## 写命令包括: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## 默认是:## maxmemory-policy volatile-lru# LRU 和 minimal TTL 算法都不是精准的算法，但是相对精确的算法(为了节省内存)，随意你可以选择样本大小进行检测。# Redis默认的灰选择3个样本进行检测，你可以通过maxmemory-samples进行设置## maxmemory-samples 3############################## AOF ################################ 默认情况下，redis会在后台异步的把数据库镜像备份到磁盘，但是该备份是非常耗时的，而且备份也不能很频繁，如果发生诸如拉闸限电、拔插头等状况，那么将造成比较大范围的数据丢失。# 所以redis提供了另外一种更加高效的数据库备份及灾难恢复方式。# 开启append only模式之后，redis会把所接收到的每一次写操作请求都追加到appendonly.aof文件中，当redis重新启动时，会从该文件恢复出之前的状态。# 但是这样会造成appendonly.aof文件过大，所以redis还支持了BGREWRITEAOF指令，对appendonly.aof 进行重新整理。# 你可以同时开启asynchronous dumps 和 AOFappendonly no# AOF文件名称 (默认: &quot;appendonly.aof&quot;)# appendfilename appendonly.aof# Redis支持三种同步AOF文件的策略:## no: 不进行同步，系统去操作 . Faster.# always: always表示每次有写操作都进行同步. Slow, Safest.# everysec: 表示对写操作进行累积，每秒同步一次. Compromise.## 默认是&quot;everysec&quot;，按照速度和安全折中这是最好的。# 如果想让Redis能更高效的运行，你也可以设置为&quot;no&quot;，让操作系统决定什么时候去执行# 或者相反想让数据更安全你也可以设置为&quot;always&quot;## 如果不确定就用 &quot;everysec&quot;.# appendfsync alwaysappendfsync everysec# appendfsync no# AOF策略设置为always或者everysec时，后台处理进程(后台保存或者AOF日志重写)会执行大量的I/O操作# 在某些Linux配置中会阻止过长的fsync()请求。注意现在没有任何修复，即使fsync在另外一个线程进行处理## 为了减缓这个问题，可以设置下面这个参数no-appendfsync-on-rewrite## This means that while another child is saving the durability of Redis is# the same as &quot;appendfsync none&quot;, that in pratical terms means that it is# possible to lost up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# AOF 自动重写# 当AOF文件增长到一定大小的时候Redis能够调用 BGREWRITEAOF 对日志文件进行重写## 它是这样工作的：Redis会记住上次进行些日志后文件的大小(如果从开机以来还没进行过重写，那日子大小在开机的时候确定)## 基础大小会同现在的大小进行比较。如果现在的大小比基础大小大制定的百分比，重写功能将启动# 同时需要指定一个最小大小用于AOF重写，这个用于阻止即使文件很小但是增长幅度很大也去重写AOF文件的情况# 设置 percentage 为0就关闭这个特性auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb################################## SLOW LOG #################################### Redis Slow Log 记录超过特定执行时间的命令。执行时间不包括I/O计算比如连接客户端，返回结果等，只是命令执行时间## 可以通过两个参数设置slow log：一个是告诉Redis执行超过多少时间被记录的参数slowlog-log-slower-than(微妙)，# 另一个是slow log 的长度。当一个新命令被记录的时候最早的命令将被从队列中移除# 下面的时间以微妙微单位，因此1000000代表一分钟。# 注意制定一个负数将关闭慢日志，而设置为0将强制每个命令都会记录slowlog-log-slower-than 10000# 对日志长度没有限制，只是要注意它会消耗内存# 可以通过 SLOWLOG RESET 回收被慢日志消耗的内存slowlog-max-len 1024################################ VM ################################## WARNING! Virtual Memory is deprecated in Redis 2.4### The use of Virtual Memory is strongly discouraged.# Virtual Memory allows Redis to work with datasets bigger than the actual# amount of RAM needed to hold the whole dataset in memory.# In order to do so very used keys are taken in memory while the other keys# are swapped into a swap file, similarly to what operating systems do# with memory pages.## To enable VM just set &apos;vm-enabled&apos; to yes, and set the following three# VM parameters accordingly to your needs.vm-enabled no# vm-enabled yes# This is the path of the Redis swap file. As you can guess, swap files# can&apos;t be shared by different Redis instances, so make sure to use a swap# file for every redis process you are running. Redis will complain if the# swap file is already in use.## The best kind of storage for the Redis swap file (that&apos;s accessed at random)# is a Solid State Disk (SSD).## *** WARNING *** if you are using a shared hosting the default of putting# the swap file under /tmp is not secure. Create a dir with access granted# only to Redis user and configure Redis to create the swap file there.vm-swap-file /tmp/redis.swap# vm-max-memory configures the VM to use at max the specified amount of# RAM. Everything that deos not fit will be swapped on disk *if* possible, that# is, if there is still enough contiguous space in the swap file.## With vm-max-memory 0 the system will swap everything it can. Not a good# default, just specify the max amount of RAM you can in bytes, but it&apos;s# better to leave some margin. For instance specify an amount of RAM# that&apos;s more or less between 60 and 80% of your free RAM.vm-max-memory 0# Redis swap files is split into pages. An object can be saved using multiple# contiguous pages, but pages can&apos;t be shared between different objects.# So if your page is too big, small objects swapped out on disk will waste# a lot of space. If you page is too small, there is less space in the swap# file (assuming you configured the same number of total swap file pages).## If you use a lot of small objects, use a page size of 64 or 32 bytes.# If you use a lot of big objects, use a bigger page size.# If unsure, use the default :)vm-page-size 32# Number of total memory pages in the swap file.# Given that the page table (a bitmap of free/used pages) is taken in memory,# every 8 pages on disk will consume 1 byte of RAM.## The total swap size is vm-page-size * vm-pages## With the default of 32-bytes memory pages and 134217728 pages Redis will# use a 4 GB swap file, that will use 16 MB of RAM for the page table.## It&apos;s better to use the smallest acceptable value for your application,# but the default is large in order to work in most conditions.vm-pages 134217728# Max number of VM I/O threads running at the same time.# This threads are used to read/write data from/to swap file, since they# also encode and decode objects from disk to memory or the reverse, a bigger# number of threads can help with big objects even if they can&apos;t help with# I/O itself as the physical device may not be able to couple with many# reads/writes operations at the same time.## The special value of 0 turn off threaded I/O and enables the blocking# Virtual Memory implementation.vm-max-threads 4############################### ADVANCED CONFIG ################################ 当hash中包含超过指定元素个数并且最大的元素没有超过临界时，# hash将以一种特殊的编码方式（大大减少内存使用）来存储，这里可以设置这两个临界值# Redis Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，# 这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap,# 当成员数量增大时会自动转成真正的HashMap,此时encoding为ht。hash-max-zipmap-entries 512hash-max-zipmap-value 64# list数据类型多少节点以下会采用去指针的紧凑存储格式。# list数据类型节点值大小小于多少字节会采用紧凑存储格式。list-max-ziplist-entries 512list-max-ziplist-value 64# set数据类型内部数据如果全部是数值型，且包含多少节点以下会采用紧凑格式存储。set-max-intset-entries 512# zsort数据类型多少节点以下会采用去指针的紧凑存储格式。# zsort数据类型节点值大小小于多少字节会采用紧凑存储格式。zset-max-ziplist-entries 128zset-max-ziplist-value 64# Redis将在每100毫秒时使用1毫秒的CPU时间来对redis的hash表进行重新hash，可以降低内存的使用## 当你的使用场景中，有非常严格的实时性需要，不能够接受Redis时不时的对请求有2毫秒的延迟的话，把这项配置为no。## 如果没有这么严格的实时性要求，可以设置为yes，以便能够尽可能快的释放内存activerehashing yes################################## INCLUDES #################################### 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件# include /path/to/local.conf# include /path/to/other.conf]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Postman获取上一次请求返回]]></title>
    <url>%2Fposts%2Fa72a%2F</url>
    <content type="text"><![CDATA[Postman从上一个接口的 response 中获取数据作为下一个接口的参数进行请求]]></content>
      <categories>
        <category>Postman</category>
      </categories>
      <tags>
        <tag>Postman</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transport API]]></title>
    <url>%2Fposts%2F311b%2F</url>
    <content type="text"><![CDATA[Transport API 的核心是 Channel 接口，用于所有的出站操作，见下图 如上图所示，每个 Channel 都会分配一个 ChannelPipeline 和ChannelConfig。ChannelConfig 负责设置并存储 Channel 的配置，并允许在运行期间更新它们。传输一般有特定的配置设置，可能实现了 ChannelConfig. 的子类型。 ChannelPipeline 容纳了使用的 ChannelHandler 实例，这些ChannelHandler 将处理通道传递的“入站”和“出站”数据以及事件。ChannelHandler 的实现允许你改变数据状态和传输数据。 现在我们可以使用 ChannelHandler 做下面一些事情： 传输数据时，将数据从一种格式转换到另一种格式 异常通知 Channel 变为 active（活动） 或 inactive（非活动） 时获得通知* Channel 被注册或注销时从 EventLoop 中获得通知 通知用户特定事件 Intercepting Filter（拦截过滤器） ChannelPipeline 实现了常用的 Intercepting Filter（拦截过滤器）设计模式。UNIX管道是另一例子：命令链接在一起，一个命令的输出连接到 的下一行中的输入。 你还可以在运行时根据需要添加 ChannelHandler 实例到ChannelPipeline 或从 ChannelPipeline 中删除，这能帮助我们构建高度灵活的 Netty 程序。例如，你可以支持STARTTLS协议，只需通过加入适当的 ChannelHandler（这里是 SslHandler）到的ChannelPipeline 中，当被请求这个协议时。 此外，访问指定的 ChannelPipeline 和 ChannelConfig，你能在Channel 自身上进行操作。Channel 提供了很多方法，如下列表： Table 4.1 Channel main methods 方法名称 描述 eventLoop() 返回分配给Channel的EventLoop pipeline() 返回分配给Channel的ChannelPipeline isActive() 返回Channel是否激活，已激活说明与远程连接对等 localAddress() 返回已绑定的本地SocketAddress remoteAddress() 返回已绑定的远程SocketAddress write() 写数据到远程客户端，数据通过ChannelPipeline传输过去 flush() 刷新先前的数据 writeAndFlush(…) 一个方便的方法用户调用write(…)而后调用 flush() 后面会越来越熟悉这些方法，现在只需要记住我们的操作都是在相同的接口上运行，Netty 的高灵活性让你可以以不同的传输实现进行重构。 写数据到远程已连接客户端可以调用Channel.write()方法，如下代码： Listing 4.5 Writing to a channel 123456789101112131415Channel channel = ...; // 获取channel的引用ByteBuf buf = Unpooled.copiedBuffer("your data", CharsetUtil.UTF_8); //1ChannelFuture cf = channel.writeAndFlush(buf); //2cf.addListener(new ChannelFutureListener() &#123; //3 @Override public void operationComplete(ChannelFuture future) &#123; if (future.isSuccess()) &#123; //4 System.out.println("Write successful"); &#125; else &#123; System.err.println("Write error"); //5 future.cause().printStackTrace(); &#125; &#125;&#125;); 1.创建 ByteBuf 保存写的数据 2.写数据，并刷新 3.添加 ChannelFutureListener 即可写操作完成后收到通知， 4.写操作没有错误完成 5.写操作完成时出现错误 Channel 是线程安全(thread-safe)的，它可以被多个不同的线程安全的操作，在多线程环境下，所有的方法都是安全的。正因为 Channel 是安全的，我们存储对Channel的引用，并在学习的时候使用它写入数据到远程已连接的客户端，使用多线程也是如此。下面的代码是一个简单的多线程例子： Listing 4.6 Using the channel from many threads 12345678910111213141516final Channel channel = ...; // 获取channel的引用final ByteBuf buf = Unpooled.copiedBuffer("your data", CharsetUtil.UTF_8).retain(); //1Runnable writer = new Runnable() &#123; //2 @Override public void run() &#123; channel.writeAndFlush(buf.duplicate()); &#125;&#125;;Executor executor = Executors.newCachedThreadPool();//3//写进一个线程executor.execute(writer); //4//写进另外一个线程executor.execute(writer); //5 1.创建一个 ByteBuf 保存写的数据 2.创建 Runnable 用于写数据到 channel 3.获取 Executor 的引用使用线程来执行任务 4.手写一个任务，在一个线程中执行 5.手写另一个任务，在另一个线程中执行]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transport 使用情况]]></title>
    <url>%2Fposts%2F37c%2F</url>
    <content type="text"><![CDATA[前面说了，并不是所有传输都支持核心协议，这会限制你的选择，具体看下表 Transport TCP UDP SCTP* UDT NIO X X X X OIO X X X X *指目前仅在 Linux 上的支持。 在 Linux 上启用 SCTP 注意 SCTP 需要 kernel 支持，举例 Ubuntu： 1sudo apt-get install libsctp1 Fedora 使用 yum: 1sudo yum install kernel-modules-extra.x86_64 lksctp-tools.x86_64 虽然只有SCTP具有这些特殊的要求，对应的特定的传输也有推荐的配置。想想也是，一个服务器平台可能会需要支持较高的数量的并发连接比单个客户端的话。 下面是你可能遇到的用例: OIO-在低连接数、需要低延迟时、阻塞时使用 NIO-在高连接数时使用 Local-在同一个JVM内通信时使用 Embedded-测试ChannelHandler时使用]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty入门应用]]></title>
    <url>%2Fposts%2F4bc2%2F</url>
    <content type="text"><![CDATA[Nettey时间服务器服务端TimeServer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.lanniuh.netty.nio;import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioServerSocketChannel;/** * Created by linjian * 16/8/5.** */public class TimeServer &#123; public static void main(String[] args) &#123; int port = 8080; if (args != null &amp;&amp; args.length &gt; 0) &#123; try &#123; port = Integer.valueOf(args[0]); &#125; catch (NumberFormatException e) &#123; &#125; &#125; new TimeServer().bind(port); &#125; public void bind(int port) &#123; //配置服务端NIO线程组 EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class).option(ChannelOption.SO_BACKLOG,1024).childHandler(new ChildChannelHandler()); //绑定端口,同步等待成功 ChannelFuture f = b.bind(port).sync(); //等待服务端监听端口关闭 f.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; //优雅退出,释放线程池资源 bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125; private class ChildChannelHandler extends ChannelInitializer &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; socketChannel.pipeline().addLast(new TimeServerHandler()); &#125; &#125;&#125; NioEventLoopGroup是个线程组，包含了一组NIO线程，专门用于网络时间的处理，实际上她们就是Reactor线程组。这里创建两个的原因是一个用于服务端接受客户端的连接，另一个用于进行SocketChannel的网络读写。 ServerBootstrap是Netty用于启动NIO服务端的辅助启动类，目的是降低服务端的开发复杂度。 12ServerBootstrap b = new ServerBootstrap();b.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class).option(ChannelOption.SO_BACKLOG,1024).childHandler(new ChildChannelHandler()); 调用ServerBootstrap的group方法，将两个NIO线程组当作入参传递到ServerBootstrap中，接着设置创建的Channel为NioServerSocketChannel，然后配置NioServerSocketChannel的TCP参数，最后绑定I/O事件的处理类ChildChannelHandler，它的作用类似于Reactor模式中的handler类，主要用于处理网络I/O事件，例如记录日至、对消息进行编解码等。 服务端启动辅助类配置完成之后，调用它的bind方法绑定监听端口，随后，调用它的同步阻塞方法sync等待绑定操作完成。完成之后Netty会返回一个Channel Future，主要用于异步操作的通知回调。 Netty时间服务器服务端TimeServerHandler12345678910111213141516171819202122232425262728293031323334353637package com.lanniuh.netty.nio;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerAdapter;import io.netty.channel.ChannelHandlerContext;import java.util.Date;/** * Created by linjian * 16/8/5. */public class TimeServerHandler extends ChannelHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, "utf-8"); System.out.println("The time server receive order: " + body); String currentTime = "QUERY TIME ORDER".equalsIgnoreCase(body) ? (new Date(System.currentTimeMillis())).toString() : "BAD ORDER"; ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes()); ctx.write(resp); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush(); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); &#125;&#125; Netty时间服务器客户端TimeClient123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.lanniuh.netty.nio;import io.netty.bootstrap.Bootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelOption;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioSocketChannel;/** * Created by linjian * 16/8/5. */public class TimeClient &#123; public static void main(String[] args) throws InterruptedException &#123; int port = 8080; if (args != null &amp;&amp; args.length &gt; 0) &#123; try &#123; port = Integer.valueOf(args[0]); &#125; catch (NumberFormatException e) &#123; &#125; &#125; new TimeClient().connect(port,"127.0.0.1"); &#125; public void connect(int port,String host) throws InterruptedException &#123; //配置客户端NIO线程组 EventLoopGroup group = new NioEventLoopGroup(); try &#123; Bootstrap b = new Bootstrap(); b.group(group).channel(NioSocketChannel.class).option(ChannelOption.TCP_NODELAY,true).handler(new ChannelInitializer() &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; socketChannel.pipeline().addLast(new TimeClientHandler()); &#125; &#125;); //发起异步连接操作 ChannelFuture f = b.connect(host,port).sync(); //等待客户端链路关闭 f.channel().closeFuture().sync(); &#125;finally &#123; //优雅退出,释放NIO线程组 group.shutdownGracefully(); &#125; &#125;&#125; Netty时间服务器客户端TimeClientHandler1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.lanniuh.netty.nio;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerAdapter;import io.netty.channel.ChannelHandlerContext;import java.util.logging.Logger;/** * Created by linjian * 16/8/5. */public class TimeClientHandler extends ChannelHandlerAdapter &#123; private final ByteBuf firstMessage; private static final Logger logger = Logger.getLogger(TimeClientHandler.class.getName()); public TimeClientHandler() &#123; byte[] req = "QUERY TIME ORDER".getBytes(); firstMessage = Unpooled.buffer(req.length); firstMessage.writeBytes(req); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ctx.writeAndFlush(firstMessage); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, "utf-8"); System.out.println("Now is :" + body); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; //释放资源 logger.warning("Unexcepted exception from downstream : " + cause.getMessage()); ctx.close(); &#125;&#125; 重点关注三个方法：channelActive、channelRead和exceptionCaught。当客户端和服务端TCP链路建立成功之后，Netty的NIO线程会调用activeChannel方法，发送查询时间的指令给服务端，调用ChannelHandlerContext的writeAndFlush方法将请求消息发送给服务端。 当服务端返回应答消息时，channelRead方法被调用。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty快速入门]]></title>
    <url>%2Fposts%2F8500%2F</url>
    <content type="text"><![CDATA[BOOTSTRAPNetty 应用程序通过设置 bootstrap（引导）类的开始，该类提供了一个 用于应用程序网络层配置的容器。 CHANNEL底层网络传输 API 必须提供给应用 I/O操作的接口，如读，写，连接，绑定等等。对于我们来说，这是结构几乎总是会成为一个“socket”。 Netty 中的接口 Channel 定义了与 socket 丰富交互的操作集：bind, close, config, connect, isActive, isOpen, isWritable, read, write 等等。 Netty 提供大量的 Channel 实现来专门使用。这些包括 AbstractChannel，AbstractNioByteChannel，AbstractNioChannel，EmbeddedChannel， LocalServerChannel，NioSocketChannel 等等。 CHANNELHANDLERChannelHandler 支持很多协议，并且提供用于数据处理的容器。我们已经知道 ChannelHandler 由特定事件触发。 ChannelHandler 可专用于几乎所有的动作，包括将一个对象转为字节（或相反），执行过程中抛出的异常处理。 常用的一个接口是 ChannelInboundHandler，这个类型接收到入站事件（包括接收到的数据）可以处理应用程序逻辑。当你需要提供响应时，你也可以从 ChannelInboundHandler 冲刷数据。一句话，业务逻辑经常存活于一个或者多个 ChannelInboundHandler。 CHANNELPIPELINEChannelPipeline 提供了一个容器给 ChannelHandler 链并提供了一个API 用于管理沿着链入站和出站事件的流动。每个 Channel 都有自己的ChannelPipeline，当 Channel 创建时自动创建的。 ChannelHandler 是如何安装在 ChannelPipeline？ 主要是实现了ChannelHandler 的抽象 ChannelInitializer。ChannelInitializer子类 通过 ServerBootstrap 进行注册。当它的方法 initChannel() 被调用时，这个对象将安装自定义的 ChannelHandler 集到 pipeline。当这个操作完成时，ChannelInitializer 子类则 从 ChannelPipeline 自动删除自身。 EVENTLOOPEventLoop 用于处理 Channel 的 I/O 操作。一个单一的 EventLoop通常会处理多个 Channel 事件。一个 EventLoopGroup 可以含有多于一个的 EventLoop 和 提供了一种迭代用于检索清单中的下一个。 CHANNELFUTURENetty 所有的 I/O 操作都是异步。因为一个操作可能无法立即返回，我们需要有一种方法在以后确定它的结果。出于这个目的，Netty 提供了接口 ChannelFuture,它的 addListener 方法注册了一个 ChannelFutureListener ，当操作完成时，可以被通知（不管成功与否）。 更多关于 ChannelFuture 想想一个 ChannelFuture 对象作为一个未来执行操作结果的占位符。何时执行取决于几个因素，因此不可能预测与精确。但我们可以肯定的是，它会被执行。此外，所有的操作返回 ChannelFuture 对象和属于同一个 Channel 将在以正确的顺序被执行，在他们被调用后。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ChannelHandler 和 ChannelPipeline]]></title>
    <url>%2Fposts%2F306%2F</url>
    <content type="text"><![CDATA[ChannelPipeline 是 ChannelHandler 链的容器。 在许多方面的 ChannelHandler 是在您的应用程序的核心，尽管有时它 可能并不明显。ChannelHandler 支持广泛的用途，使它难以界定。因此，最好是把它当作一个通用的容器，处理进来的事件（包括数据）并且通过ChannelPipeline。下图展示了 ChannelInboundHandler 和 ChannelOutboundHandler 继承自父接口 ChannelHandler。 Figure 3.3 ChannelHandler class hierarchy Netty 中有两个方向的数据流，图3.4 显示的入站(ChannelInboundHandler)和出站(ChannelOutboundHandler)之间有一个明显的区别：若数据是从用户应用程序到远程主机则是“出站(outbound)”，相反若数据时从远程主机到用户应用程序则是“入站(inbound)”。 为了使数据从一端到达另一端，一个或多个 ChannelHandler 将以某种方式操作数据。这些 ChannelHandler 会在程序的“引导”阶段被添加ChannelPipeline中，并且被添加的顺序将决定处理数据的顺序。 Figure 3.4 ChannelPipeline with inbound and outbound ChannelHandlers 图 3.4 同样展示了进站和出站的处理器都可以被安装在相同的 pipeline 。本例子中，如果消息或任何其他入站事件被读到，将从 pipeline 头部开始，传递到第一个 ChannelInboundHandler。该处理器可能会或可能不会实际修改数据，取决于其特定的功能，在这之后 该数据将被传递到链中的下一个 ChannelInboundHandler。最后，将数据 到达 pipeline 的尾部，此时所有处理结束。 数据的出站运动（即，数据被“写入”）在概念上是相同的。在这种情况下的数据从尾部流过 ChannelOutboundHandlers 的链，直到它到达头部。超过这点，出站数据将到达的网络传输，在这里显示为一个 socket。通常，这将触发一个写入操作。 更多 Inbound 、 Outbound Handler 在当前的链（chain）中，事件可以通过 ChanneHandlerContext 传递给下一个 handler。Netty 为此提供了抽象基类ChannelInboundHandlerAdapter 和 hannelOutboundHandlerAdapter，用来处理你想要的事件。 这些类提供的方法的实现，可以简单地通过调用 ChannelHandlerContext 上的相应方法将事件传递给下一个 handler。在实际应用中，您可以按需覆盖相应的方法即可。 所以，如果出站和入站操作是不同的，当 ChannelPipeline 中有混合处理器时将发生什么？虽然入站和出站处理器都扩展了 ChannelHandler，Netty 的 ChannelInboundHandler 的实现 和 ChannelOutboundHandler 之间的是有区别的，从而保证数据传递只从一个处理器到下一个处理器保证正确的类型。 当 ChannelHandler 被添加到的 ChannelPipeline 它得到一个 ChannelHandlerContext，它代表一个 ChannelHandler 和 ChannelPipeline 之间的“绑定”。它通常是安全保存对此对象的引用，除了当协议中的使用的是不面向连接（例如，UDP）。而该对象可以被用来获得 底层 Channel,它主要是用来写出站数据。 还有，实际上，在 Netty 发送消息有两种方式。您可以直接写消息给 Channel 或写入 ChannelHandlerContext 对象。主要的区别是， 前一种方法会导致消息从 ChannelPipeline的尾部开始，而 后者导致消息从 ChannelPipeline 下一个处理器开始。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Channel, Event 和 I/O]]></title>
    <url>%2Fposts%2F62ad%2F</url>
    <content type="text"><![CDATA[Channel, Event 和 I/ONetty 是一个非阻塞、事件驱动的网络框架。Netty 实际上是使用 Threads（多线程）处理 I/O 事件，对于熟悉多线程编程的读者可能会需要关注同步代码。这样的方式不好，因为同步会影响程序的性能，Netty 的设计保证程序处理事件不会有同步。图 Figure 3.1 展示了，你不需要在 Channel 之间共享 ChannelHandler 实例的原因： 该图显示，一个 EventLoopGroup 具有一个或多个 EventLoop。想象 EventLoop 作为一个 Thread 给 Channel 执行工作。 （事实上，一个 EventLoop 是势必为它的生命周期一个线程。） 当创建一个 Channel，Netty 通过 一个单独的 EventLoop 实例来注册该 Channel（并同样是一个单独的 Thread）的通道的使用寿命。这就是为什么你的应用程序不需要同步 Netty 的 I/O操作;所有 Channel 的 I/O 始终用相同的线程来执行。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是 Bootstrapping 为什么要用]]></title>
    <url>%2Fposts%2Fa4c0%2F</url>
    <content type="text"><![CDATA[Bootstrapping（引导） 是 Netty 中配置程序的过程，当你需要连接客户端或服务器绑定指定端口时需要使用 Bootstrapping。 如前面所述，Bootstrapping 有两种类型，一种是用于客户端的Bootstrap，一种是用于服务端的ServerBootstrap。不管程序使用哪种协议，无论是创建一个客户端还是服务器都需要使用“引导”。 面向连接 vs. 无连接 请记住，这个讨论适用于 TCP 协议，它是“面向连接”的。这样协议保证该连接的端点之间的消息的有序输送。无连接协议发送的消息，无法保证顺序和成功性 两种 Bootstrapping 之间有一些相似之处，也有一些不同。Bootstrap 和 ServerBootstrap 之间的差异如下： Table 3.1 Comparison of Bootstrap classes 分类 Bootstrap ServerBootstrap 网络功能 连接到远程主机和端口 绑定本地端口 EventLoopGroup 数量 1 2 Bootstrap用来连接远程主机，有1个EventLoopGroup ServerBootstrap用来绑定本地端口，有2个EventLoopGroup 事件组(Groups)，传输(transports)和处理程序(handlers)分别在本章后面讲述，我们在这里只讨论两种”引导”的差异(Bootstrap和ServerBootstrap)。第一个差异很明显，“ServerBootstrap”监听在服务器监听一个端口轮询客户端的“Bootstrap”或DatagramChannel是否连接服务器。通常需要调用“Bootstrap”类的connect()方法，但是也可以先调用bind()再调用connect()进行连接，之后使用的Channel包含在bind()返回的ChannelFuture中。 一个 ServerBootstrap 可以认为有2个 Channel 集合，第一个集合包含一个单例 ServerChannel，代表持有一个绑定了本地端口的 socket；第二集合包含所有创建的 Channel，处理服务器所接收到的客户端进来的连接。下图形象的描述了这种情况： 与 ServerChannel 相关 EventLoopGroup 分配一个 EventLoop 是 负责创建 Channels 用于传入的连接请求。一旦连接接受，第二个EventLoopGroup 分配一个 EventLoop 给它的 Channel。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[包含的 Transport]]></title>
    <url>%2Fposts%2F47b%2F</url>
    <content type="text"><![CDATA[Netty 自带了一些传输协议的实现，虽然没有支持所有的传输协议，但是其自带的已足够我们来使用。Netty应用程序的传输协议依赖于底层协议，本节我们将学习Netty中的传输协议。 Netty中的传输方式有如下几种： Table 4.1 Provided transports 方法名称 包 描述 NIO io.netty.channel.socket.nio 基于java.nio.channels的工具包，使用选择器作为基础的方法。 OIO io.netty.channel.socket.oio 基于java.net的工具包，使用阻塞流。 Local io.netty.channel.local 用来在虚拟机之间本地通信。 Embedded io.netty.channel.embedded 嵌入传输，它允许在没有真正网络的传输中使用 ChannelHandler，可以非常有用的来测试ChannelHandler的实现。 NIO-Nonblocking I/ONIO传输是目前最常用的方式，它通过使用选择器提供了完全异步的方式操作所有的 I/O，NIO 从Java 1.4才被提供。 NIO 中，我们可以注册一个通道或获得某个通道的改变的状态，通道状态有下面几种改变： 一个新的 Channel 被接受并已准备好 Channel 连接完成 Channel 中有数据并已准备好读取 Channel 发送数据出去 处理完改变的状态后需重新设置他们的状态，用一个线程来检查是否有已准备好的 Channel，如果有则执行相关事件。在这里可能只同时一个注册的事件而忽略其他的。选择器所支持的操作在 SelectionKey 中定义，具体如下： Table 4.2 Selection operation bit-set 方法名称 描述 OP_ACCEPT 有新连接时得到通知 OP_CONNECT 连接完成后得到通知 OP_REA 准备好读取数据时得到通知 OP_WRITE 写入更多数据到通道时得到通知，大部分时间 这是可能的，但有时 socket 缓冲区完全填满了。这通常发生在你写数据的速度太快了超过了远程节点的处理能力。 Figure 4.2 Selecting and Processing State Changes 1.新信道注册 WITH 选择器 2.选择处理的状态变化的通知 3.以前注册的通道 4.Selector.select（）方法阻塞，直到新的状态变化接收或配置的超时 已过 5.检查是否有状态变化 6.处理所有的状态变化 7.在选择器操作的同一个线程执行其他任务 有一种功能，目前仅适用于 NIO 传输叫什么 “zero-file-copy （零文件拷贝）”，这使您能够快速，高效地通过移动数据到从文件系统传输内容 网络协议栈而无需复制从内核空间到用户空间。这可以使 FT P或 HTTP 协议有很大的不同。 然而，并非所有的操作系统都支持此功能。此外，你不能用它实现数据加密或压缩文件系统 - 仅支持文件的原生内容。另一方面，传送的文件原本已经加密的是完全有效的。 接下来，我们将讨论的是 OIO ，它提供了一个阻塞传输。 OIO-Old blocking I/ONetty 中，该 OIO 传输代表了一种妥协。它通过了 Netty 的通用 API 访问但不是异步，而是构建在 java.net 的阻塞实现上。任何人下面讨论这一点可能会认为，这个协议并没有很大优势。但它确实有它有效的用途。 假设你需要的端口使用该做阻塞调用库（例如JDBC）。它可能不适合非阻塞。相反，你可以在短期内使用 OIO 传输，后来移植到纯异步的传输上。让我们看看它是如何工作的。 在 java.net API，你通常有一个线程接受新的连接到达监听在ServerSocket，并创建一个新的线程来处理新的 Socket 。这是必需的，因为在一个特定的 socket的每个 I/O 操作可能会阻塞在任何时间。在一个线程处理多个 socket 易造成阻塞操作，一个 socket 占用了所有的其他人。 鉴于此，你可能想知道 Netty 是如何用相同的 API 来支持 NIO 的异步传输。这里的 Netty 利用了 SO_TIMEOUT 标志，可以设置在一个 Socket。这 timeout 指定最大 毫秒 数量 用于等待 I/O 的操作完成。如果操作在指定的时间内失败，SocketTimeoutException 会被抛出。 Netty中捕获该异常并继续处理循环。在接下来的事件循环运行，它再次尝试。像 Netty 的异步架构来支持 OIO 的话，这其实是唯一的办法。当SocketTimeoutException 抛出时，执行 stack trace。 Figure 4.3 OIO-Processing logic 1.线程分配给 Socket 2.Socket 连接到远程 3.读操作（可能会阻塞） 4.读完成 5.处理可读的字节 6.执行提交到 socket 的其他任务 7.再次尝试读 同个 JVM 内的本地 Transport 通信Netty 提供了“本地”传输，为运行在同一个 Java 虚拟机上的服务器和客户之间提供异步通信。此传输支持所有的 Netty 常见的传输实现的 API。 在此传输中，与服务器 Channel 关联的 SocketAddress 不是“绑定”到一个物理网络地址中，而是在服务器是运行时它被存储在注册表中，当 Channel 关闭时它会注销。由于该传输不是“真正的”网络通信，它不能与其他传输实现互操作。因此，客户端是希望连接到使用本地传输的的服务器时，要注意正确的用法。除此限制之外，它的使用是与其他的传输是相同的。 内嵌 TransportNetty中 还提供了可以嵌入 ChannelHandler 实例到其他的 ChannelHandler 的传输，使用它们就像辅助类，增加了灵活性的方法，使您可以与你的 ChannelHandler 互动。 该嵌入技术通常用于测试 ChannelHandler 的实现，但它也可用于将功能添加到现有的 ChannelHandler 而无需更改代码。嵌入传输的关键是Channel 的实现，称为“EmbeddedChannel”。 第10章描述了使用 EmbeddedChannel 来测试 ChannelHandlers。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[案例研究:Transport 的迁移]]></title>
    <url>%2Fposts%2Ff2e%2F</url>
    <content type="text"><![CDATA[为了让你想象 Transport 如何工作，我会从一个简单的应用程序开始，这个应用程序什么都不做，只是接受客户端连接并发送“Hi!”字符串消息到客户端，发送完了就断开连接。 没有用 Netty 实现 I/O 和 NIO我们将不用 Netty 实现只用 JDK API 来实现 I/O 和 NIO。下面这个例子，是使用阻塞 IO 实现的例子： Listing 4.1 Blocking networking without Netty 1234567891011121314151617181920212223242526272829303132333435public class PlainOioServer &#123; public void serve(int port) throws IOException &#123; final ServerSocket socket = new ServerSocket(port); //1 try &#123; for (;;) &#123; final Socket clientSocket = socket.accept(); //2 System.out.println("Accepted connection from " + clientSocket); new Thread(new Runnable() &#123; //3 @Override public void run() &#123; OutputStream out; try &#123; out = clientSocket.getOutputStream(); out.write("Hi!\r\n".getBytes(Charset.forName("UTF-8"))); //4 out.flush(); clientSocket.close(); //5 &#125; catch (IOException e) &#123; e.printStackTrace(); try &#123; clientSocket.close(); &#125; catch (IOException ex) &#123; // ignore on close &#125; &#125; &#125; &#125;).start(); //6 &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 1.绑定服务器到指定的端口。 2.接受一个连接。 3.创建一个新的线程来处理连接。 4.将消息发送到连接的客户端。 5.一旦消息被写入和刷新时就 关闭连接。 6.启动线程。 上面的方式可以工作正常，但是这种阻塞模式在大连接数的情况就会有很严重的问题，如客户端连接超时，服务器响应严重延迟，性能无法扩展。为了解决这种情况，我们可以使用异步网络处理所有的并发连接，但问题在于 NIO 和 OIO 的 API 是完全不同的，所以一个用OIO开发的网络应用程序想要使用NIO重构代码几乎是重新开发。 下面代码是使用 NIO 实现的例子： Listing 4.2 Asynchronous networking without Netty 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class PlainNioServer &#123; public void serve(int port) throws IOException &#123; ServerSocketChannel serverChannel = ServerSocketChannel.open(); serverChannel.configureBlocking(false); ServerSocket ss = serverChannel.socket(); InetSocketAddress address = new InetSocketAddress(port); ss.bind(address); //1 Selector selector = Selector.open(); //2 serverChannel.register(selector, SelectionKey.OP_ACCEPT); //3 final ByteBuffer msg = ByteBuffer.wrap("Hi!\r\n".getBytes()); for (;;) &#123; try &#123; selector.select(); //4 &#125; catch (IOException ex) &#123; ex.printStackTrace(); // handle exception break; &#125; Set readyKeys = selector.selectedKeys(); //5 Iterator iterator = readyKeys.iterator(); while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); iterator.remove(); try &#123; if (key.isAcceptable()) &#123; //6 ServerSocketChannel server = (ServerSocketChannel)key.channel(); SocketChannel client = server.accept(); client.configureBlocking(false); client.register(selector, SelectionKey.OP_WRITE | SelectionKey.OP_READ, msg.duplicate()); //7 System.out.println( "Accepted connection from " + client); &#125; if (key.isWritable()) &#123; //8 SocketChannel client = (SocketChannel)key.channel(); ByteBuffer buffer = (ByteBuffer)key.attachment(); while (buffer.hasRemaining()) &#123; if (client.write(buffer) == 0) &#123; //9 break; &#125; &#125; client.close(); //10 &#125; &#125; catch (IOException ex) &#123; key.cancel(); try &#123; key.channel().close(); &#125; catch (IOException cex) &#123; // 在关闭时忽略 &#125; &#125; &#125; &#125; &#125;&#125; 1.绑定服务器到制定端口 2.打开 selector 处理 channel 3.注册 ServerSocket 到 ServerSocket ，并指定这是专门意接受 连接。 4.等待新的事件来处理。这将阻塞，直到一个事件是传入。 5.从收到的所有事件中 获取 SelectionKey 实例。 6.检查该事件是一个新的连接准备好接受。 7.接受客户端，并用 selector 进行注册。 8.检查 socket 是否准备好写数据。 9.将数据写入到所连接的客户端。如果网络饱和，连接是可写的，那么这个循环将写入数据，直到该缓冲区是空的。 10.关闭连接。 如你所见，即使它们实现的功能是一样，但是代码完全不同。下面我们将用Netty 来实现相同的功能。 采用 Netty 实现 I/O 和 NIO下面代码是使用Netty作为网络框架编写的一个阻塞 IO 例子： Listing 4.3 Blocking networking with Netty 12345678910111213141516171819202122232425262728293031public class NettyOioServer &#123; public void server(int port) throws Exception &#123; final ByteBuf buf = Unpooled.unreleasableBuffer( Unpooled.copiedBuffer("Hi!\r\n", Charset.forName("UTF-8"))); EventLoopGroup group = new OioEventLoopGroup(); try &#123; ServerBootstrap b = new ServerBootstrap(); //1 b.group(group) //2 .channel(OioServerSocketChannel.class) .localAddress(new InetSocketAddress(port)) .childHandler(new ChannelInitializer() &#123;//3 @Override public void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123; //4 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ctx.writeAndFlush(buf.duplicate()).addListener(ChannelFutureListener.CLOSE);//5 &#125; &#125;); &#125; &#125;); ChannelFuture f = b.bind().sync(); //6 f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully().sync(); //7 &#125; &#125;&#125; 1.创建一个 ServerBootstrap 2.使用 OioEventLoopGroup 允许阻塞模式 3.指定 ChannelInitializer 将给每个接受的连接调用 4.添加的 ChannelHandler 拦截事件，并允许他们作出反应 5.写信息到客户端，并添加 ChannelFutureListener 当一旦消息写入就关闭连接 6.绑定服务器来接受连接 7.释放所有资源 下面代码是使用 Netty NIO 实现。 Netty NIO 版本下面是 Netty NIO 的代码，只是改变了一行代码，就从 OIO 传输 切换到了 NIO。 Listing 4.4 Asynchronous networking with Netty 123456789101112131415161718192021222324252627282930313233public class NettyNioServer &#123; public void server(int port) throws Exception &#123; final ByteBuf buf = Unpooled.unreleasableBuffer( Unpooled.copiedBuffer("Hi!\r\n", Charset.forName("UTF-8"))); NioEventLoopGroup bossGroup = new NioEventLoopGroup(1); NioEventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap b = new ServerBootstrap(); //1 b.group(bossGroup, workerGroup) //2 .channel(NioServerSocketChannel.class) .localAddress(new InetSocketAddress(port)) .childHandler(new ChannelInitializer() &#123; //3 @Override public void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123; //4 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ctx.writeAndFlush(buf.duplicate()) //5 .addListener(ChannelFutureListener.CLOSE); &#125; &#125;); &#125; &#125;); ChannelFuture f = b.bind().sync(); //6 f.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully().sync(); //7 workerGroup.shutdownGracefully().sync(); &#125; &#125;&#125; 1.创建一个 ServerBootstrap 2.使用 NioEventLoopGroup 允许非阻塞模式（NIO） 3.指定 ChannelInitializer 将给每个接受的连接调用 4.添加的 ChannelInboundHandlerAdapter() 接收事件并进行处理 5.写信息到客户端，并添加 ChannelFutureListener 当一旦消息写入就关闭连接 6.绑定服务器来接受连接 7.释放所有资源 因为 Netty 使用相同的 API 来实现每个传输，它并不关心你使用什么来实现。Netty 通过操作接口Channel 、ChannelPipeline 和 ChannelHandler来实现。 现在你了解到了用 基于 Netty 传输的好处。下面就来看下传输的 API.]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[STR_TO_DATE]]></title>
    <url>%2Fposts%2Feed8%2F</url>
    <content type="text"><![CDATA[STR_TO_DATE(str,format)This is the inverse of theDATE_FORMAT()function. It takes a string str and a format string format .STR_TO_DATE()returns aDATETIMEvalue if the format string contains both date and time parts, or aDATEorTIMEvalue if the string contains only date or time parts. If the date, time, or datetime value extracted from str is illegal,STR_TO_DATE()returnsNULLand produces a warning. The server scans str attempting to match format to it. The format string can contain literal characters and format specifiers beginning with%. Literal characters in format must match literally in str . Format specifiers in format must match a date or time part in str . For the specifiers that can be used in format, see theDATE_FORMAT()function description. 1234mysql&gt; SELECT STR_TO_DATE('01,5,2013','%d,%m,%Y'); -&gt; '2013-05-01'mysql&gt; SELECT STR_TO_DATE('May 1, 2013','%M %d,%Y'); -&gt; '2013-05-01]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql锁机制]]></title>
    <url>%2Fposts%2F33e8%2F</url>
    <content type="text"><![CDATA[行级锁 劣势：开销大，加锁慢；会出现死锁； 优势：锁定粒度小，发生锁冲突的概率最低，并发度也最高。 页级锁 开销和加锁时间介于表锁和行锁之间；会出现死锁； 锁定粒度介于表锁和行锁之间，并发处理能力一般。只需了解一下。 表级锁 劣势：锁定粒度大，发生锁冲突的概率最高，并发度最低。 优势：开销小，加锁快；不会出现死锁； 共享读锁：lock table tableName read; 独占写锁：lock table tableName write; 批量解锁：unlock tables; MySQL 事务属性事务是由一组SQL语句组成的逻辑处理单元，事务具有ACID属性。 原子性（Atomicity）：事务是一个原子操作单元。在当时原子是不可分割的最小元素，其对数据的修改，要么全部成功，要么全部都不成功。 一致性（Consistent）：事务开始到结束的时间段内，数据都必须保持一致状态。 隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的”独立”环境执行。 持久性（Durable）：事务完成后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 事务常见问题更新丢失（Lost Update） 原因：当多个事务选择同一行操作，并且都是基于最初选定的值，由于每个事务都不知道其他事务的存在，就会发生更新覆盖的问题。类比github提交冲突。 脏读（Dirty Reads） 原因：事务A读取了事务B已经修改但尚未提交的数据。若事务B回滚数据，事务A的数据存在不一致性的问题。 不可重复读（Non-Repeatable Reads） 原因：事务A第一次读取最初数据，第二次读取事务B已经提交的修改或删除数据。导致两次读取数据不一致。不符合事务的隔离性。 幻读（Phantom Reads） 原因：事务A根据相同条件第二次查询到事务B提交的新增数据，两次数据结果集不一致。不符合事务的隔离性。 幻读和脏读有点类似 脏读是事务B里面修改了数据， 幻读是事务B里面新增了数据。 事务的隔离级别数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大。这是因为事务隔离实质上是将事务在一定程度上”串行”进行，这显然与”并发”是矛盾的。根据自己的业务逻辑，权衡能接受的最大副作用。从而平衡了”隔离” 和 “并发”的问题。MySQL默认隔离级别是可重复读。脏读，不可重复读，幻读，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。 隔离级别 读数据一致性 脏读 不可重复 读 幻读 未提交读(Read uncommitted) 最低级别 是 是 是 已提交读(Read committed) 语句级 否 是 是 可重复读(Repeatable read) 事务级 否 否 是 可序列化(Serializable) 最高级别，事务级 否 否 否 查看当前数据库的事务隔离级别：show variables like ‘tx_isolation’; 123456mysql&gt; show variables like 'tx_isolation';+---------------+-----------------+| Variable_name | Value |+---------------+-----------------+| tx_isolation | REPEATABLE-READ |+---------------+-----------------+ 间隙锁当我们用范围条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做”间隙(GAP)”。InnoDB也会对这个”间隙”加锁，这种锁机制就是所谓的间隙锁(Next-Key锁)。 12345678Transaction-Amysql&gt; update innodb_lock set k=66 where id &gt;=6;Query OK, 1 row affected (0.63 sec)mysql&gt; commit;Transaction-Bmysql&gt; insert into innodb_lock (id,k,v) values(7,'7','7000');Query OK, 1 row affected (18.99 sec) 危害(坑)：若执行的条件是范围过大，则InnoDB会将整个范围内所有的索引键值全部锁定，很容易对性能造成影响。 锁模式共享锁（S）：也称读锁，允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 排他锁（X)：也称写锁，独占锁，允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁。 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。 InnoDB的行锁加锁方法意向锁是InnoDB自动加的，不需用户干预。对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)；对于普通SELECT语句，InnoDB不会加任何锁；事务可以通过以下语句显示给记录集加共享锁或排他锁。 共享锁（S）：SELECT * FROM table_name WHERE … LOCK IN SHARE MODE。 排他锁（X)：SELECT * FROM table_name WHERE … FOR UPDATE。 InnoDB锁的互斥与兼容关系锁和锁之间的关系，要么是相容的，要么是互斥的。 锁a和锁b相容是指：操作同样一组数据时，如果事务t1获取了锁a,另一个事务t2还可以获取锁b； 锁a和锁b互斥是指：操作同样一组数据时，如果事务t1获取了锁a，另一个事务t2在t1释放锁a之前无法获取锁b。 (y表示兼容，n表示不兼容) - X S IX IS X n n n n S n y n y IX n n y y IS n y y y InnoDB行锁实现方式InnoDB行锁是通过给索引上的索引项加锁 来实现的，这一点MySQL与Oracle不同，后者是通过在数据块中对相应数据行加锁来实现的。 InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！ 在不通过索引条件查询的时候，InnoDB确实使用的是表锁，而不是行锁。 由于MySQL的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然是访问不同行的记录，但是如果是使用相同的索引键，是会出现锁冲突的。 当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，另外，不论是使用主键索引、唯一索引或普通索引，InnoDB都会使用行锁来对数据加锁。 即便在条件中使用了索引字段，但是否使用索引来检索数据是由MySQL通过判断不同执行计划的代价来决定的，如果MySQL认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下InnoDB将使用表锁，而不是行锁。 InnoDB间隙锁（Next-Key锁）当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁）。 InnoDB使用间隙锁的目的 一方面是为了防止幻读，以满足相关隔离级别的要求. 一方面是满足其恢复和复制的需要. InnoDB什么时候使用表锁对于InnoDB表，在绝大部分情况下都应该使用行级锁，因为事务和行锁往往是我们之所以选择InnoDB表的理由。但在个别特殊事务中，也可以考虑使用表级锁。 第一种情况是：事务需要更新大部分或全部数据，表又比较大，如果使用默认的行锁，不仅这个事务执行效率低，而且可能造成其他事务长时间锁等待和锁冲突，这种情况下可以考虑使用表锁来提高该事务的执行速度。 第二种情况是：事务涉及多个表，比较复杂，很可能引起死锁，造成大量事务回滚。这种情况也可以考虑一次性锁定事务涉及的表，从而避免死锁、减少数据库因事务回滚带来的开销。 InnoDB使用表锁注意事项 使用LOCK TABLES虽然可以给InnoDB加表级锁，但必须说明的是，表锁不是由InnoDB存储引擎层管理的，而是由其上一层──MySQL Server负责的，仅当autocommit=0、innodb_table_locks=1（默认设置）时，InnoDB层才能知道MySQL加的表锁，MySQL Server也才能感知InnoDB加的行锁，这种情况下，InnoDB才能自动识别涉及表级锁的死锁；否则，InnoDB将无法自动检测并处理这种死锁。 在用 LOCK TABLES对InnoDB表加锁时要注意，要将AUTOCOMMIT设为0，否则MySQL不会给表加锁；事务结束前，不要用UNLOCK TABLES释放表锁，因为UNLOCK TABLES会隐含地提交事务；COMMIT或ROLLBACK并不能释放用LOCK TABLES加的表级锁，必须用UNLOCK TABLES释放表锁。 死锁MyISAM表锁是deadlock free的，这是因为MyISAM总是一次获得所需的全部锁，要么全部满足，要么等待，因此不会出现死锁。但在InnoDB中，除单个SQL组成的事务外，锁是逐步获得的，这就决定了在InnoDB中发生死锁是可能的。 发生死锁后，InnoDB一般都能自动检测到，并使一个事务释放锁并回退，另一个事务获得锁，继续完成事务。但在涉及外部锁，或涉及表锁的情况下，InnoDB并不能完全自动检测到死锁，这需要通过设置锁等待超时参数 innodb_lock_wait_timeout来解决。需要说明的是，这个参数并不是只用来解决死锁问题，在并发访问比较高的情况下，如果大量事务因无法立即获得所需的锁而挂起，会占用大量计算机资源，造成严重性能问题，甚至拖跨数据库。我们通过设置合适的锁等待超时阈值，可以避免这种情况发生。 分析行锁定通过检查InnoDB_row_lock 状态变量分析系统上的行锁的争夺情况 show status like ‘innodb_row_lock%’ 123456789+-------------------------------+-------+| Variable_name | Value |+-------------------------------+-------+| Innodb_row_lock_current_waits | 0 || Innodb_row_lock_time | 0 || Innodb_row_lock_time_avg | 0 || Innodb_row_lock_time_max | 0 || Innodb_row_lock_waits | 0 |+-------------------------------+-------+ innodb_row_lock_current_waits: 当前正在等待锁定的数量 innodb_row_lock_time: 从系统启动到现在锁定总时间长度；非常重要的参数 innodb_row_lock_time_avg: 每次等待所花平均时间；非常重要的参数 innodb_row_lock_time_max: 从系统启动到现在等待最常的一次所花的时间; innodb_row_lock_waits: 系统启动后到现在总共等待的次数；非常重要的参数。直接决定优化的方向和策略。 分析表锁定可以通过检查table_locks_waited 和 table_locks_immediate 状态变量分析系统上的表锁定：show status like ‘table_locks%’ 1234567mysql&gt; show status like 'table_locks%';+----------------------------+-------+| Variable_name | Value |+----------------------------+-------+| Table_locks_immediate | 104 || Table_locks_waited | 0 |+----------------------------+-------+ table_locks_immediate: 表示立即释放表锁数。table_locks_waited: 表示需要等待的表锁数。此值越高则说明存在着越严重的表级锁争用情况。 此外，MyISAM的读写锁调度是写优先，这也是MyISAM不适合做写为主表的存储引擎。因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永久阻塞。 避免死锁的常用方法 在应用中，如果不同的程序会并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会。在下面的例子中，由于两个session访问两个表的顺序不同，发生死锁的机会就非常高！但如果以相同的顺序来访问，死锁就可以避免。 在程序以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低出现死锁的可能。 在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，而不应先申请共享锁，更新时再申请排他锁，因为当用户申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁。 在REPEATABLE-READ隔离级别下，如果两个线程同时对相同条件记录用SELECT…FOR UPDATE加排他锁，在没有符合该条件记录情况下，两个线程都会加锁成功。程序发现记录尚不存在，就试图插入一条新记录，如果两个线程都这么做，就会出现死锁。这种情况下，将隔离级别改成READ COMMITTED，就可避免问题。 当隔离级别为READ COMMITTED时，如果两个线程都先执行SELECT…FOR UPDATE，判断是否存在符合条件的记录，如果没有，就插入记录。此时，只有一个线程能插入成功，另一个线程会出现锁等待，当第1个线程提交后，第2个线程会因主键重出错，但虽然这个线程出错了，却会获得一个排他锁！这时如果有第3个线程又来申请排他锁，也会出现死锁。 如果出现死锁，可以用SHOW INNODB STATUS命令来确定最后一个死锁产生的原因。返回结果中包括死锁相关事务的详细信息，如引发死锁的SQL语句，事务已经获得的锁，正在等待什么锁，以及被回滚的事务等。据此可以分析死锁产生的原因和改进措施。]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql批量SQL插入性能优化]]></title>
    <url>%2Fposts%2F7a85%2F</url>
    <content type="text"><![CDATA[对于一些数据量较大的系统，数据库面临的问题除了查询效率低下，还有就是数据入库时间长。特别像报表系统，每天花费在数据导入上的时间可能会长达几个小时或十几个小时之久。因此，优化数据库插入性能是很有意义的。 经过对MySQL InnoDB的一些性能测试，发现一些可以提高insert效率的方法，供大家参考参考。 一条SQL语句插入多条数据常用的插入语句如： 1234INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES ('0', 'userid_0', 'content_0', 0);INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES ('1', 'userid_1', 'content_1', 1); 修改成： 12INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES ('0', 'userid_0', 'content_0', 0), ('1', 'userid_1', 'content_1', 1); 修改后的插入操作能够提高程序的插入效率。这里第二种SQL执行效率高的主要原因是合并后日志量（MySQL的binlog和innodb的事务让日志）减少了，降低日志刷盘的数据量和频率，从而提高效率。通过合并SQL语句，同时也能减少SQL语句解析的次数，减少网络传输的IO。 这里提供一些测试对比数据，分别是进行单条数据的导入与转化成一条SQL语句进行导入，分别测试1百、1千、1万条数据记录。 在事务中进行插入处理。把插入修改成： 1234567START TRANSACTION;INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES ('0', 'userid_0', 'content_0', 0);INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES ('1', 'userid_1', 'content_1', 1);...COMMIT; 使用事务可以提高数据的插入效率，这是因为进行一个INSERT操作时，MySQL内部会建立一个事务，在事务内才进行真正插入处理操作。通过使用事务可以减少创建事务的消耗，所有插入都在执行后才进行提交操作。 这里也提供了测试对比，分别是不使用事务与使用事务在记录数为1百、1千、1万的情况。 数据有序插入。数据有序的插入是指插入记录在主键上是有序排列，例如datetime是记录的主键： 123456INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES ('1', 'userid_1', 'content_1', 1);INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES ('0', 'userid_0', 'content_0', 0);INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES ('2', 'userid_2', 'content_2',2); 修改成： 123456INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES ('0', 'userid_0', 'content_0', 0);INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES ('1', 'userid_1', 'content_1', 1);INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES ('2', 'userid_2', 'content_2',2); 由于数据库插入时，需要维护索引数据，无序的记录会增大维护索引的成本。我们可以参照InnoDB使用的B+tree索引，如果每次插入记录都在索引的最后面，索引的定位效率很高，并且对索引调整较小；如果插入的记录在索引中间，需要B+tree进行分裂合并等处理，会消耗比较多计算资源，并且插入记录的索引定位效率会下降，数据量较大时会有频繁的磁盘操作。 下面提供随机数据与顺序数据的性能对比，分别是记录为1百、1千、1万、10万、100万。 从测试结果来看，该优化方法的性能有所提高，但是提高并不是很明显。 性能综合测试这里提供了同时使用上面三种方法进行INSERT效率优化的测试。 从测试结果可以看到，合并数据+事务的方法在较小数据量时，性能提高是很明显的，数据量较大时（1千万以上），性能会急剧下降，这是由于此时数据量超过了innodb_buffer的容量，每次定位索引涉及较多的磁盘读写操作，性能下降较快。而使用合并数据+事务+有序数据的方式在数据量达到千万级以上表现依旧是良好，在数据量较大时，有序数据索引定位较为方便，不需要频繁对磁盘进行读写操作，所以可以维持较高的性能。 注意事项： SQL语句是有长度限制，在进行数据合并在同一SQL中务必不能超过SQL长度限制，通过max_allowed_packet配置可以修改，默认是1M，测试时修改为8M。 事务需要控制大小，事务太大可能会影响执行的效率。MySQL有innodb_log_buffer_size配置项，超过这个值会把innodb的数据刷到磁盘中，这时，效率会有所下降。所以比较好的做法是，在数据达到这个这个值前进行事务提交。]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql高性能优化规范建议]]></title>
    <url>%2Fposts%2Fd664%2F</url>
    <content type="text"><![CDATA[数据库命令规范 所有数据库对象名称必须使用小写字母并用下划线分割 所有数据库对象名称禁止使用mysql保留关键字（如果表名中包含关键字查询时，需要将其用单引号括起来） 数据库对象的命名要能做到见名识意，并且最后不要超过32个字符 临时库表必须以tmp_为前缀并以日期为后缀，备份表必须以bak_为前缀并以日期(时间戳)为后缀 所有存储相同数据的列名和列类型必须一致（一般作为关联列，如果查询时关联列类型不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低） 数据库基本设计规范1. 所有表必须使用Innodb存储引擎没有特殊要求（即Innodb无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用Innodb存储引擎（mysql5.5之前默认使用Myisam，5.6以后默认的为Innodb） Innodb 支持事务，支持行级锁，更好的恢复性，高并发下性能更好 2. 数据库和表的字符集统一使用UTF8兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储emoji表情的需要，字符集需要采用utf8mb4字符集 3. 所有表和字段都需要添加注释使用comment从句添加表和列的备注 从一开始就进行数据字典的维护 4. 尽量控制单表数据量的大小，建议控制在500万以内500万并不是Mysql数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题 可以用历史数据归档（应用于日志数据），分库分表（应用于业务数据）等手段来控制数据量大小 5. 谨慎使用Mysql分区表分区表在物理上表现为多个文件，在逻辑上表现为一个表 谨慎选择分区键，跨分区查询效率可能更低 建议采用物理分表的方式管理大数据 6. 尽量做到冷热数据分离，减小表的宽度Mysql限制每个表最多存储4096列，并且每一行数据的大小不能超过65535字节 减少磁盘IO,保证热数据的内存缓存命中率（表越宽，把表装载进内存缓冲池时所占用的内存也就越大,也会消耗更多的IO） 更有效的利用缓存，避免读入无用的冷数据 经常一起使用的列放到一个表中（避免更多的关联操作） 7. 禁止在表中建立预留字段预留字段的命名很难做到见名识义 预留字段无法确认存储的数据类型，所以无法选择合适的类型 对预留字段类型的修改，会对表进行锁定 8. 禁止在数据库中存储图片，文件等大的二进制数据通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机IO操作，文件很大时，IO操作很耗时 通常存储于文件服务器，数据库只存储文件地址信息 9. 禁止在线上做数据库压力测试10. 禁止从开发环境，测试环境直接连接生成环境数据库数据库字段设计规范1. 优先选择符合存储需要的最小的数据类型原因是：列的字段越大，建立索引时所需要的空间也就越大，这样一页中所能存储的索引节点的数量也就越少也越少，在遍历时所需要的IO次数也就越多， 索引的性能也就越差 方法： 将字符串转换成数字类型存储，如：将IP地址转换成整形数据 mysql提供了两个方法来处理ip地址 inet_aton 把ip转为无符号整型(4-8位) inet_ntoa 把整型的ip转为地址 插入数据前，先用inet_aton把ip地址转为整型，可以节省空间 显示数据时，使用inet_ntoa把整型的ip地址转为地址显示即可。 对于非负型的数据（如自增ID、整型IP）来说，要优先使用无符号整型来存储 因为：无符号相对于有符号可以多出一倍的存储空间 SIGNED INT -21474836482147483647 UNSIGNED INT 04294967295 VARCHAR(N)中的N代表的是字符数，而不是字节数 使用UTF8存储255个汉字 Varchar(255)=765个字节 过大的长度会消耗更多的内存 2. 避免使用TEXT、BLOB数据类型，最常见的TEXT类型可以存储64k的数据 建议把BLOB或是TEXT列分离到单独的扩展表中 Mysql内存临时表不支持TEXT、BLOB这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行 而且对于这种数据，Mysql还是要进行二次查询，会使sql性能变得很差，但是不是说一定不能使用这样的数据类型 如果一定要使用，建议把BLOB或是TEXT列分离到单独的扩展表中，查询时一定不要使用select * 而只需要取出必要的列，不需要TEXT列的数据时不要对该列进行查询 TEXT或BLOB类型只能使用前缀索引 因为MySQL对索引字段长度是有限制的，所以TEXT类型只能使用前缀索引，并且TEXT列上是不能有默认值的 3. 避免使用ENUM类型修改ENUM值需要使用ALTER语句 ENUM类型的ORDER BY操作效率低，需要额外操作 禁止使用数值作为ENUM的枚举值 4. 尽可能把所有列定义为NOT NULL原因： 索引NULL列需要额外的空间来保存，所以要占用更多的空间 进行比较和计算时要对NULL值做特别的处理 5. 使用TIMESTAMP（4个字节）或DATETIME类型（8个字节）存储时间TIMESTAMP 存储的时间范围 1970-01-01 00:00:01 ~ 2038-01-19-03:14:07 TIMESTAMP 占用4字节和INT相同，但比INT可读性高 超出TIMESTAMP取值范围的使用DATETIME类型存储 经常会有人用字符串存储日期型的数据（不正确的做法） 缺点1：无法用日期函数进行计算和比较 缺点2：用字符串存储日期要占用更多的空间 6. 同财务相关的金额类数据必须使用decimal类型 非精准浮点：float,double 精准浮点：decimal Decimal类型为精准浮点数，在计算时不会丢失精度 占用空间由定义的宽度决定，每4个字节可以存储9位数字，并且小数点要占用一个字节 可用于存储比bigint更大的整型数据 索引设计规范1. 限制每张表上的索引数量，建议单张表索引不超过5个索引并不是越多越好！索引可以提高效率同样可以降低效率 索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率 因为mysql优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个 索引都可以用于查询，就会增加mysql优化器生成执行计划的时间，同样会降低查询性能 2. 禁止给表中的每一列都建立单独的索引5.6版本之前，一个sql只能使用到一个表中的一个索引，5.6以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好 3. 每个Innodb表必须有个主键Innodb是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的 每个表都可以有多个索引，但是表的存储顺序只能有一种 Innodb是按照主键索引的顺序来组织表的 不要使用更新频繁的列作为主键，不适用多列主键（相当于联合索引） 不要使用UUID,MD5,HASH,字符串列作为主键（无法保证数据的顺序增长） 主键建议使用自增ID值 常见索引列建议 出现在SELECT、UPDATE、DELETE语句的WHERE从句中的列 包含在ORDER BY、GROUP BY、DISTINCT中的字段 并不要将符合1和2中的字段的列都建立一个索引， 通常将1、2中的字段建立联合索引效果更好 多表join的关联列 如何选择索引列的顺序建立索引的目的是：希望通过索引进行数据查找，减少随机IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数） 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO性能也就越好） 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引） 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间）重复索引示例：primary key(id)、index(id)、unique index(id) 冗余索引示例：index(a,b,c)、index(a,b)、index(a) 对于频繁的查询优先考虑使用覆盖索引覆盖索引：就是包含了所有查询字段(where,select,ordery by,group by包含的字段)的索引 覆盖索引的好处: 避免Innodb表进行索引的二次查询 Innodb是以聚集索引的顺序来存储的，对于Innodb来说，二级索引在叶子节点中所保存的是行的主键信息， 如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据 而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了IO操作，提升了查询效率 可以把随机IO变成顺序IO加快查询效率 由于覆盖索引是按键值的顺序存储的，对于IO密集型的范围查找来说，对比随机从磁盘读取每一行的数据IO要少的多， 因此利用覆盖索引在访问时也可以把磁盘的随机读取的IO转变成索引查找的顺序IO 索引SET规范尽量避免使用外键约束不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引 外键可用于保证数据的参照完整性，但建议在业务端实现 外键会影响父表和子表的写操作从而降低性能 数据库SQL开发规范1. 建议使用预编译语句进行数据库操作预编译语句可以重复使用这些计划，减少SQL编译所需要的时间，还可以解决动态SQL所带来的SQL注入的问题 只传参数，比传递SQL语句更高效 相同语句可以一次解析，多次使用，提高处理效率 2. 避免数据类型的隐式转换隐式转换会导致索引失效 如: select name,phone from customer where id = ‘111’; 3. 充分利用表上已经存在的索引避免使用双%号的查询条件。如 a like ‘%123%’，（如果无前置%,只有后置%，是可以用到列上的索引的） 一个SQL只能利用到复合索引中的一列进行范围查询如 有 a,b,c列的联合索引，在查询条件中有a列的范围查询，则在b,c列上的索引将不会被用到， 在定义联合索引时，如果a列要用到范围查找的话，就要把a列放到联合索引的右侧 使用left join 或 not exists 来优化not in 操作因为not in 也通常会使用索引失效 4. 数据库设计时，应该要对以后扩展进行考虑5. 程序连接不同的数据库使用不同的账号，进制跨库查询为数据库迁移和分库分表留出余地 降低业务耦合度 避免权限过大而产生的安全风险 6. 禁止使用SELECT * 必须使用SELECT &lt;字段列表&gt; 查询原因： 消耗更多的CPU和IO以网络带宽资源 无法使用覆盖索引 可减少表结构变更带来的影响 7. 禁止使用不含字段列表的INSERT语句如： insert into values (‘a’,’b’,’c’); 应使用 insert into t(c1,c2,c3) values (‘a’,’b’,’c’); 8. 避免使用子查询，可以把子查询优化为join操作通常子查询在in子句中，且子查询中为简单SQL(不包含union、group by、order by、limit从句)时,才可以把子查询转化为关联查询进行优化 子查询性能差的原因： 子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响 特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大 由于子查询会产生大量的临时表也没有索引，所以会消耗过多的CPU和IO资源，产生大量的慢查询 9. 避免使用JOIN关联太多的表对于Mysql来说，是存在关联缓存的，缓存的大小可以由join_buffer_size参数进行设置 在Mysql中，对于同一个SQL多关联（join）一个表，就会多分配一个关联缓存，如果在一个SQL中关联的表越多， 所占用的内存也就越大 如果程序中大量的使用了多表关联的操作，同时join_buffer_size设置的也不合理的情况下，就容易造成服务器内存溢出的情况， 就会影响到服务器数据库性能的稳定性 同时对于关联操作来说，会产生临时表操作，影响查询效率 Mysql最多允许关联61个表，建议不超过5个 10. 减少同数据库的交互次数数据库更适合处理批量操作 合并多个相同的操作到一起，可以提高处理效率 11. 对应同一列进行or判断时，使用in代替orin 的值不要超过500个 in 操作可以更有效的利用索引，or大多数情况下很少能利用到索引 12. 禁止使用order by rand() 进行随机排序会把表中所有符合条件的数据装载到内存中，然后在内存中对所有数据根据随机生成的值进行排序，并且可能会对每一行都生成一个随机值，如果满足条件的数据集非常大， 就会消耗大量的CPU和IO及内存资源 推荐在程序中获取一个随机值，然后从数据库中获取数据的方式 13. WHERE从句中禁止对列进行函数转换和计算对列进行函数转换或计算时会导致无法使用索引 不推荐： where date(create_time)=’20190101’ 推荐： where create_time &gt;= ‘20190101’ and create_time &lt; ‘20190102’ 14. 在明显不会有重复值时使用UNION ALL 而不是UNIONUNION 会把两个结果集的所有数据放到临时表中后再进行去重操作 UNION ALL 不会再对结果集进行去重操作 15. 拆分复杂的大SQL为多个小SQL大SQL:逻辑上比较复杂，需要占用大量CPU进行计算的SQL MySQL 一个SQL只能使用一个CPU进行计算 SQL拆分后可以通过并行执行来提高处理效率 数据库操作行为规范超100万行的批量写（UPDATE、DELETE、INSERT）操作，要分批多次进行操作1. 大批量操作可能会造成严重的主从延迟主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间， 而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况 2. binlog日志为row格式时会产生大量的日志大批量写操作会产生大量日志，特别是对于row格式二进制数据而言，由于在row格式中会记录每一行数据的修改，我们一次修改的数据越多， 产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因 3. 避免产生大事务操作大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对MySQL的性能产生非常大的影响 特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批 对于大表使用pt-online-schema-change修改表结构 避免大表修改产生的主从延迟 避免在对表字段进行修改时进行锁表 对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的 pt-online-schema-change它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器 把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉 把原来一个DDL操作，分解成多个小的批次进行 禁止为程序使用的账号赋予super权限当达到最大连接数限制时，还运行1个有super权限的用户连接 super权限只能留给DBA处理问题的账号使用 对于程序连接数据库账号，遵循权限最小原则]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql登录命令]]></title>
    <url>%2Fposts%2F1c43%2F</url>
    <content type="text"><![CDATA[1234567mysql -hlocalhost -uroot -p-h数据库主机-u用户-p密码-P端口号（大写P）mysql -h localhost -u joinhealth -P 3306 -p 查看当前数据库列表–显示数据库1show databases; 选择数据库1use database_name;]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql大表更新sql的优化策略]]></title>
    <url>%2Fposts%2F5951%2F</url>
    <content type="text"><![CDATA[问题sql背景：项目有6个表的要根据pid字段要写入对应的brand_id字段。但是这个其中有两个表是千万级别的。我的worker运行之后，线上的mysql主从同步立刻延迟了！运行了一个多小时之后，居然延迟到了40分钟，而且只更新了十几万行数据。 问题sql如下： 1234UPDATE $tableName$SET brand_id = #newBrandId#WHERE pid = #pid# AND brand_id = 0 项目组的mysql专家帮我分析了下，因为pid字段没有索引，mysql引擎要逐行扫描出与传入的pid值相等的列，然后更新数据，也就是要扫描完1000W+行磁盘数据才能执行完这个sql。因为是update操作，没有用到索引，于是导致这个sql会占用表锁，其它的sql只能等这个sql执行完成之后才能开始执行。更严重的是，这个千万级的表里面有多少个不同的pid，我就要执行多少个这样的sql。 同事给我的建议的根据id字段进行sql代码层次的横向分表。每次更新1000行的数据，这样mysql引擎就不用每次在扫全表了，数据库压力是之前的万分之一。而且id作为主键，是有索引的，这个时候占用的是这1000行数据的行级锁，不会影响其它的数据。有索引能大大优化查询性能，优化后的sql如下： 12345UPDATE $tableName$SET brand_id = #newBrandId#WHERE pid = #pid# AND brand_id = 0 AND id BETWEEN #startNum# AND #endNum# 仅仅用了id限区间的语句，将一个千万级的大表代码层次上进行横向切割。重新上线worker后，mysql主从没有任何延迟！而且经过监控，短短10分钟就更新了十几万数据，效率是之前的6倍！更重要的是数据库负载均衡，应用健康运行。]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql查看数据库和表的占用空间大小]]></title>
    <url>%2Fposts%2Ffd8f%2F</url>
    <content type="text"><![CDATA[1234567SELECT * FROM information_schema.TABLES WHERE TABLE_SCHEMA = '&#123;db_name&#125;' AND table_name = '&#123;tab_name&#125;'; 1234567SELECT concat(round(sum( data_length / 1024 / 1024 ),2 ),'MB' ) FROM information_schema.PARTITIONS WHERE table_schema = 'db_name' AND table_name = 'tab_name';]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql导入数据命令]]></title>
    <url>%2Fposts%2F75ff%2F</url>
    <content type="text"><![CDATA[1source d:/myprogram/database/db.sql]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[multi-statement not allow]]></title>
    <url>%2Fposts%2Fb698%2F</url>
    <content type="text"><![CDATA[sql injection violation, multi-statement not allow数据库连接url加上支持批量的参数allowMultiQueries=true 如果用了druid，并且使用了wall-filter，需要配置]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LEFT]]></title>
    <url>%2Fposts%2Fda34%2F</url>
    <content type="text"><![CDATA[LEFT(str,len)Returns the leftmost len characters from the string str, orNULLif any argument isNULL. 12mysql&gt; SELECT LEFT('foobarbar', 5); -&gt; 'fooba' This function is multibyte safe.]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Left Join 没有使用索引问题]]></title>
    <url>%2Fposts%2F5540%2F</url>
    <content type="text"><![CDATA[问题：生产环境sql执行很慢分析sql执行 t_user_user表并没有使用到索引 思路：索引用不上的原因可能是字符集不相同查看表字段字符集： 1SHOW FULL COLUMNS FROM &#123;tab_name&#125;; 发现两张表的关联字段字符集不相同，修改字段字符集编码 重新分析sql，发现已使用索引]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GROUP_CONCAT]]></title>
    <url>%2Fposts%2Fad29%2F</url>
    <content type="text"><![CDATA[GROUP_CONCAT(expr)This function returns a string result with the concatenated non-NULLvalues from a group. It returnsNULLif there are no non-NULLvalues. The full syntax is as follows: 1234GROUP_CONCAT([DISTINCT] expr [,expr ...] [ORDER BY &#123;unsigned_integer | col_name | expr&#125; [ASC | DESC] [,col_name ...]] [SEPARATOR str_val]) 1234mysql&gt; SELECT student_name, GROUP_CONCAT(test_score) FROM student GROUP BY student_name; Or: 12345mysql&gt; SELECT student_name, GROUP_CONCAT(DISTINCT test_score ORDER BY test_score DESC SEPARATOR ' ') FROM student GROUP BY student_name; In MySQL, you can get the concatenated values of expression combinations. To eliminate duplicate values, use theDISTINCTclause. To sort values in the result, use theORDER BYclause. To sort in reverse order, add theDESC(descending) keyword to the name of the column you are sorting by in theORDER BYclause. The default is ascending order; this may be specified explicitly using theASCkeyword. The default separator between values in a group is comma (,). To specify a separator explicitly, useSEPARATORfollowed by the string literal value that should be inserted between group values. To eliminate the separator altogether, specifySEPARATOR &#39;&#39;. The result is truncated to the maximum length that is given by thegroup_concat_max_lensystem variable, which has a default value of 1024. The value can be set higher, although the effective maximum length of the return value is constrained by the value ofmax_allowed_packet. The syntax to change the value ofgroup_concat_max_lenat runtime is as follows, wherevalis an unsigned integer: 1SET [GLOBAL | SESSION] group_concat_max_len = val; The return value is a nonbinary or binary string, depending on whether the arguments are nonbinary or binary strings. The result type isTEXTorBLOBunlessgroup_concat_max_lenis less than or equal to 512, in which case the result type isVARCHARorVARBINARY.]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deadlock死锁]]></title>
    <url>%2Fposts%2F7509%2F</url>
    <content type="text"><![CDATA[查看死锁信息 1show engine innodb status]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DATE_FORMAT]]></title>
    <url>%2Fposts%2F415c%2F</url>
    <content type="text"><![CDATA[DATE_FORMAT(date,format)Formats the date value according to the format string. The following specifiers may be used in the format string. The % character is required before format specifier characters. Specifier Description %a Abbreviated weekday name (Sun..Sat) %b Abbreviated month name (Jan..Dec) %c Month, numeric (0..12) %D Day of the month with English suffix (0th,1st,2nd,3rd, …) %d Day of the month, numeric (00..31) %e Day of the month, numeric (0..31) %f Microseconds (000000..999999) %H Hour (00..23) %h Hour (01..12) %I Hour (01..12) %i Minutes, numeric (00..59) %j Day of year (001..366) %k Hour (0..23) %l Hour (1..12) %M Month name (January..December) %m Month, numeric (00..12) %p AMorPM %r Time, 12-hour (hh:mm:ssfollowed byAMorPM) %S Seconds (00..59) %s Seconds (00..59) %T Time, 24-hour (hh:mm:ss) %U Week (00..53), where Sunday is the first day of the week;WEEK()mode 0 %u Week (00..53), where Monday is the first day of the week;WEEK()mode 1 %V Week (01..53), where Sunday is the first day of the week;WEEK()mode 2; used with%X %v Week (01..53), where Monday is the first day of the week;WEEK()mode 3; used with%x %W Weekday name (Sunday..Saturday) %w Day of the week (0=Sunday..6=Saturday) %X Year for the week where Sunday is the first day of the week, numeric, four digits; used with%V %x Year for the week, where Monday is the first day of the week, numeric, four digits; used with%v %Y Year, numeric, four digits %y Year, numeric (two digits) %% A literal%character %x x, for any “x” not listed above]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务隔离级别设置]]></title>
    <url>%2Fposts%2Fa91b%2F</url>
    <content type="text"><![CDATA[123456#查询事务隔离级别SELECT @@GLOBAL.tx_isolation,@@tx_isolation;#设置事务隔离级别 read committedSET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;SET GLOBAL TRANSACTION ISOLATION LEVEL READ COMMITTED;]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql启动]]></title>
    <url>%2Fposts%2Fa027%2F</url>
    <content type="text"><![CDATA[Mysql服务启动 启动1service mysqld start 停止1service mysqld stop 重启1service mysqld restart]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[根据日期查询优化技巧]]></title>
    <url>%2Fposts%2Fda9b%2F</url>
    <content type="text"><![CDATA[问题：数据量达百万级别根据日期查询效率特别慢。create_time是datetime类型，转换为日期再匹配，需要查询出所有行进行过滤。 优化方案：利用在create_time字段上建立索引，使用BETWEEN…AND…，查询极快 时间查询优化解决方案： 尽量避免thisTime &gt; startTime and thisTime &lt; endTime这样的语句 使用索引，以查询时间的列建立索引 使用BETWEEN AND可能效果更好]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据量 Mybatis 分页插件Count语句优化]]></title>
    <url>%2Fposts%2Fa039%2F</url>
    <content type="text"><![CDATA[问题当在大数量的情况下，进行分页查询，统计总数时，会自动count一次，这个语句是在我们的查询语句的基础上嵌套一层，如： 1SELECT COUNT(*) FROM (主sql) 这样在数据量大的情况下，会出问题，很容易cpu就跑满了 优化在mapper.xml中自定义count查询，使用自定义的查询速度会快些 增加countSuffixcount 查询后缀配置参数，该参数是针对PageInterceptor配置的，默认值为_COUNT。 分页插件会优先通过当前查询的 msId +countSuffix查找手写的分页查询。 如果存在就使用手写的 count 查询，如果不存在，仍然使用之前的方式自动创建 count 查询。 例如，如果存在下面两个查询： 12345678select a.id,b.countryname,a.countrycode from country aleft join country b on a.id = b.idorder by a.idselect count(distinct a.id) from country aleft join country b on a.id = b.id 上面的countSuffix使用的默认值_COUNT，分页插件会自动获取到selectLeftjoin_COUNT查询，这个查询需要自己保证结果数正确。 返回值的类型必须是resultType=&quot;Long&quot;，入参使用的和selectLeftjoin查询相同的参数，所以在 SQL 中要按照selectLeftjoin的入参来使用。 因为selectLeftjoin_COUNT方法是自动调用的，所以不需要在接口提供相应的方法，如果需要单独调用，也可以提供。 上面方法执行输出的部分日志如下： 123456789101112DEBUG [main] - ==&gt; Preparing: select count(distinct a.id) from country a left join country b on a.id = b.id DEBUG [main] - ==&gt; Parameters: TRACE [main] - &lt;== Columns: C1TRACE [main] - &lt;== Row: 183DEBUG [main] - &lt;== Total: 1DEBUG [main] - Cache Hit Ratio [com.github.pagehelper.mapper.CountryMapper]: 0.0DEBUG [main] - ==&gt; Preparing: select a.id,b.countryname,a.countrycode from country a left join country b on a.id = b.id order by a.id LIMIT 10 DEBUG [main] - ==&gt; Parameters: TRACE [main] - &lt;== Columns: ID, COUNTRYNAME, COUNTRYCODETRACE [main] - &lt;== Row: 1, Angola, AOTRACE [main] - &lt;== Row: 2, Afghanistan, AFTRACE [main] - &lt;== Row: 3, Albania, AL 此功能pagehelper5.0.4版本以上支持，所以要升级pagehelper版本 升级后单元测试报错1Cause: java.lang.ClassCastException: com.github.pagehelper.PageHelper cannot be cast to org.apache.ibatis.plugin.Interceptor; 原因1235.*使用新的分页com.github.pagehelper.PageInterceptorhelperDialect 分页插件会自动检测当前的数据库链接，自动选择合适的分页方式。 你可以配置`helperDialect`属性来指定分页插件使用哪种方言。 解决方案1修改mybatis-Config.xml 1234567Caused by: com.github.pagehelper.PageException: java.lang.NoSuchMethodException: org.apache.ibatis.reflection.MetaObject.forObject(java.lang.Object) at com.github.pagehelper.util.MetaObjectUtil.(MetaObjectUtil.java:49) ... 57 moreCaused by: java.lang.NoSuchMethodException: org.apache.ibatis.reflection.MetaObject.forObject(java.lang.Object) at java.lang.Class.getDeclaredMethod(Class.java:2017) at com.github.pagehelper.util.MetaObjectUtil.(MetaObjectUtil.java:47) ... 57 more 原因1mybatis版本太低导致分页插件拦截器里面反射失败 解决方案1升级mybatis 版本为：3.4.4 、升级mybatis-spring版本为：1.3.0]]></content>
      <categories>
        <category>Mybatis</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis缓存]]></title>
    <url>%2Fposts%2F2ec3%2F</url>
    <content type="text"><![CDATA[前言MyBatis是常见的Java数据库访问层框架。在日常工作中，开发人员多数情况下是使用MyBatis的默认缓存配置，但是MyBatis缓存机制有一些不足之处，在使用中容易引起脏数据，形成一些潜在的隐患。个人在业务开发中也处理过一些由于MyBatis缓存引发的开发问题，带着个人的兴趣，希望从应用及源码的角度为读者梳理MyBatis缓存机制。 本次分析中涉及到的代码和数据库表均放在GitHub上，地址：mybatis-cache-demo。 目录本文按照以下顺序展开。 一级缓存介绍及相关配置。 一级缓存工作流程及源码分析。 一级缓存总结。 二级缓存介绍及相关配置。 二级缓存源码分析。 二级缓存总结。 全文总结。 一级缓存一级缓存介绍在应用运行过程中，我们有可能在一次数据库会话中，执行多次查询条件完全相同的SQL，MyBatis提供了一级缓存的方案优化这部分场景，如果是相同的SQL语句，会优先命中一级缓存，避免直接对数据库进行查询，提高性能。具体执行过程如下图所示。 每个SqlSession中持有了Executor，每个Executor中有一个LocalCache。当用户发起查询时，MyBatis根据当前执行的语句生成MappedStatement，在Local Cache进行查询，如果缓存命中的话，直接返回结果给用户，如果缓存没有命中的话，查询数据库，结果写入Local Cache，最后返回结果给用户。具体实现类的类关系图如下图所示。 一级缓存配置我们来看看如何使用MyBatis一级缓存。开发者只需在MyBatis的配置文件中，添加如下语句，就可以使用一级缓存。共有两个选项，SESSION或者STATEMENT，默认是SESSION级别，即在一个MyBatis会话中执行的所有语句，都会共享这一个缓存。一种是STATEMENT级别，可以理解为缓存只对当前执行的这一个Statement有效。 一级缓存实验接下来通过实验，了解MyBatis一级缓存的效果，每个单元测试后都请恢复被修改的数据。 首先是创建示例表student，创建对应的POJO类和增改的方法，具体可以在entity包和mapper包中查看。 123456CREATE TABLE `student` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(200) COLLATE utf8_bin DEFAULT NULL, `age` tinyint(3) unsigned DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8 COLLATE=utf8_bin; 在以下实验中，id为1的学生名称是凯伦。 实验1开启一级缓存，范围为会话级别，调用三次getStudentById，代码如下所示： 1234567public void getStudentById() throws Exception &#123; SqlSession sqlSession = factory.openSession(true); // 自动提交事务 StudentMapper studentMapper = sqlSession.getMapper(StudentMapper.class); System.out.println(studentMapper.getStudentById(1)); System.out.println(studentMapper.getStudentById(1)); System.out.println(studentMapper.getStudentById(1)); &#125; 执行结果： 我们可以看到，只有第一次真正查询了数据库，后续的查询使用了一级缓存。 实验2增加了对数据库的修改操作，验证在一次数据库会话中，如果对数据库发生了修改操作，一级缓存是否会失效。 123456789@Testpublic void addStudent() throws Exception &#123; SqlSession sqlSession = factory.openSession(true); // 自动提交事务 StudentMapper studentMapper = sqlSession.getMapper(StudentMapper.class); System.out.println(studentMapper.getStudentById(1)); System.out.println("增加了" + studentMapper.addStudent(buildStudent()) + "个学生"); System.out.println(studentMapper.getStudentById(1)); sqlSession.close();&#125; 执行结果： 我们可以看到，在修改操作后执行的相同查询，查询了数据库，一级缓存失效。 实验3开启两个SqlSession，在sqlSession1中查询数据，使一级缓存生效，在sqlSession2中更新数据库，验证一级缓存只在数据库会话内部共享。 1234567891011121314@Testpublic void testLocalCacheScope() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); System.out.println("studentMapper读取数据: " + studentMapper.getStudentById(1)); System.out.println("studentMapper读取数据: " + studentMapper.getStudentById(1)); System.out.println("studentMapper2更新了" + studentMapper2.updateStudentName("小岑",1) + "个学生的数据"); System.out.println("studentMapper读取数据: " + studentMapper.getStudentById(1)); System.out.println("studentMapper2读取数据: " + studentMapper2.getStudentById(1));&#125; sqlSession2更新了id为1的学生的姓名，从凯伦改为了小岑，但session1之后的查询中，id为1的学生的名字还是凯伦，出现了脏数据，也证明了之前的设想，一级缓存只在数据库会话内部共享。 一级缓存工作流程&amp;源码分析那么，一级缓存的工作流程是怎样的呢？我们从源码层面来学习一下。 工作流程一级缓存执行的时序图，如下图所示。 源码分析接下来将对MyBatis查询相关的核心类和一级缓存的源码进行走读。这对后面学习二级缓存也有帮助。 SqlSession： 对外提供了用户和数据库之间交互需要的所有方法，隐藏了底层的细节。默认实现类是DefaultSqlSession。 Executor：SqlSession向用户提供操作数据库的方法，但和数据库操作有关的职责都会委托给Executor。 如下图所示，Executor有若干个实现类，为Executor赋予了不同的能力，大家可以根据类名，自行学习每个类的基本作用。 在一级缓存的源码分析中，主要学习BaseExecutor的内部实现。 BaseExecutor：BaseExecutor是一个实现了Executor接口的抽象类，定义若干抽象方法，在执行的时候，把具体的操作委托给子类进行执行。 1234protected abstract int doUpdate(MappedStatement ms, Object parameter) throws SQLException;protected abstract List doFlushStatements(boolean isRollback) throws SQLException;protected abstract List doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException;protected abstract Cursor doQueryCursor(MappedStatement ms, Object parameter, RowBounds rowBounds, BoundSql boundSql) throws SQLException; 在一级缓存的介绍中提到对Local Cache的查询和写入是在Executor内部完成的。在阅读BaseExecutor的代码后发现Local Cache是BaseExecutor内部的一个成员变量，如下代码所示。 123public abstract class BaseExecutor implements Executor &#123;protected ConcurrentLinkedQueue deferredLoads;protected PerpetualCache localCache; Cache： MyBatis中的Cache接口，提供了和缓存相关的最基本的操作，如下图所示： 有若干个实现类，使用装饰器模式互相组装，提供丰富的操控缓存的能力，部分实现类如下图所示： BaseExecutor成员变量之一的PerpetualCache，是对Cache接口最基本的实现，其实现非常简单，内部持有HashMap，对一级缓存的操作实则是对HashMap的操作。如下代码所示： 123public class PerpetualCache implements Cache &#123; private String id; private Map cache = new HashMap(); 在阅读相关核心类代码后，从源代码层面对一级缓存工作中涉及到的相关代码，出于篇幅的考虑，对源码做适当删减，读者朋友可以结合本文，后续进行更详细的学习。 为执行和数据库的交互，首先需要初始化SqlSession，通过DefaultSqlSessionFactory开启SqlSession： 12345private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; ............ final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit);&#125; 在初始化SqlSesion时，会使用Configuration类创建一个全新的Executor，作为DefaultSqlSession构造函数的参数，创建Executor代码如下所示： 123456789101112131415161718public Executor newExecutor(Transaction transaction, ExecutorType executorType) &#123; executorType = executorType == null ? defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Executor executor; if (ExecutorType.BATCH == executorType) &#123; executor = new BatchExecutor(this, transaction); &#125; else if (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125; // 尤其可以注意这里，如果二级缓存开关开启的话，是使用CahingExecutor装饰BaseExecutor的子类 if (cacheEnabled) &#123; executor = new CachingExecutor(executor); &#125; executor = (Executor) interceptorChain.pluginAll(executor); return executor;&#125; SqlSession创建完毕后，根据Statment的不同类型，会进入SqlSession的不同方法中，如果是Select语句的话，最后会执行到SqlSession的selectList，代码如下所示： 12345@Overridepublic List selectList(String statement, Object parameter, RowBounds rowBounds) &#123; MappedStatement ms = configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER);&#125; SqlSession把具体的查询职责委托给了Executor。如果只开启了一级缓存的话，首先会进入BaseExecutor的query方法。代码如下所示： 123456@Overridepublic List query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException &#123; BoundSql boundSql = ms.getBoundSql(parameter); CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql); return query(ms, parameter, rowBounds, resultHandler, key, boundSql);&#125; 在上述代码中，会先根据传入的参数生成CacheKey，进入该方法查看CacheKey是如何生成的，代码如下所示： 1234567CacheKey cacheKey = new CacheKey();cacheKey.update(ms.getId());cacheKey.update(rowBounds.getOffset());cacheKey.update(rowBounds.getLimit());cacheKey.update(boundSql.getSql());//后面是update了sql中带的参数cacheKey.update(value); 在上述的代码中，将MappedStatement的Id、SQL的offset、SQL的limit、SQL本身以及SQL中的参数传入了CacheKey这个类，最终构成CacheKey。以下是这个类的内部结构： 123456789101112131415private static final int DEFAULT_MULTIPLYER = 37;private static final int DEFAULT_HASHCODE = 17;private int multiplier;private int hashcode;private long checksum;private int count;private List updateList;public CacheKey() &#123; this.hashcode = DEFAULT_HASHCODE; this.multiplier = DEFAULT_MULTIPLYER; this.count = 0; this.updateList = new ArrayList();&#125; 首先是成员变量和构造函数，有一个初始的hachcode和乘数，同时维护了一个内部的updatelist。在CacheKey的update方法中，会进行一个hashcode和checksum的计算，同时把传入的参数添加进updatelist中。如下代码所示： 123456789public void update(Object object) &#123; int baseHashCode = object == null ? 1 : ArrayUtil.hashCode(object); count++; checksum += baseHashCode; baseHashCode *= count; hashcode = multiplier * hashcode + baseHashCode; updateList.add(object);&#125; 同时重写了CacheKey的equals方法，代码如下所示： 123456789101112@Overridepublic boolean equals(Object object) &#123; ............. for (int i = 0; i &lt; updateList.size(); i++) &#123; Object thisObject = updateList.get(i); Object thatObject = cacheKey.updateList.get(i); if (!ArrayUtil.equals(thisObject, thatObject)) &#123; return false; &#125; &#125; return true;&#125; 除去hashcode、checksum和count的比较外，只要updatelist中的元素一一对应相等，那么就可以认为是CacheKey相等。只要两条SQL的下列五个值相同，即可以认为是相同的SQL。 Statement Id + Offset + Limmit + Sql + Params BaseExecutor的query方法继续往下走，代码如下所示： 1234567list = resultHandler == null ? (List) localCache.getObject(key) : null;if (list != null) &#123; // 这个主要是处理存储过程用的。 handleLocallyCachedOutputParameters(ms, key, parameter, boundSql); &#125; else &#123; list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);&#125; 如果查不到的话，就从数据库查，在queryFromDatabase中，会对localcache进行写入。 在query方法执行的最后，会判断一级缓存级别是否是STATEMENT级别，如果是的话，就清空缓存，这也就是STATEMENT级别的一级缓存无法共享localCache的原因。代码如下所示： 123if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) &#123; clearLocalCache();&#125; 在源码分析的最后，我们确认一下，如果是insert/delete/update方法，缓存就会刷新的原因。 SqlSession的insert方法和delete方法，都会统一走update的流程，代码如下所示： 12345678@Overridepublic int insert(String statement, Object parameter) &#123; return update(statement, parameter); &#125; @Override public int delete(String statement) &#123; return update(statement, null);&#125; update方法也是委托给了Executor执行。BaseExecutor的执行方法如下所示： 123456789@Overridepublic int update(MappedStatement ms, Object parameter) throws SQLException &#123; ErrorContext.instance().resource(ms.getResource()).activity("executing an update").object(ms.getId()); if (closed) &#123; throw new ExecutorException("Executor was closed."); &#125; clearLocalCache(); return doUpdate(ms, parameter);&#125; 每次执行update前都会清空localCache。 至此，一级缓存的工作流程讲解以及源码分析完毕。 总结 MyBatis一级缓存的生命周期和SqlSession一致。 MyBatis一级缓存内部设计简单，只是一个没有容量限定的HashMap，在缓存的功能性上有所欠缺。 MyBatis的一级缓存最大范围是SqlSession内部，有多个SqlSession或者分布式的环境下，数据库写操作会引起脏数据，建议设定缓存级别为Statement。 二级缓存二级缓存介绍在上文中提到的一级缓存中，其最大的共享范围就是一个SqlSession内部，如果多个SqlSession之间需要共享缓存，则需要使用到二级缓存。开启二级缓存后，会使用CachingExecutor装饰Executor，进入一级缓存的查询流程前，先在CachingExecutor进行二级缓存的查询，具体的工作流程如下所示。 二级缓存开启后，同一个namespace下的所有操作语句，都影响着同一个Cache，即二级缓存被多个SqlSession共享，是一个全局的变量。 当开启缓存后，数据的查询执行的流程就是 二级缓存 -&gt; 一级缓存 -&gt; 数据库。 二级缓存配置要正确的使用二级缓存，需完成如下配置的。 在MyBatis的配置文件中开启二级缓存。 在MyBatis的映射XML中配置cache或者 cache-ref 。 cache标签用于声明这个namespace使用二级缓存，并且可以自定义配置。 type：cache使用的类型，默认是PerpetualCache，这在一级缓存中提到过。 eviction： 定义回收的策略，常见的有FIFO，LRU。 flushInterval： 配置一定时间自动刷新缓存，单位是毫秒。 size： 最多缓存对象的个数。 readOnly： 是否只读，若配置可读写，则需要对应的实体类能够序列化。 blocking： 若缓存中找不到对应的key，是否会一直blocking，直到有对应的数据进入缓存。 cache-ref代表引用别的命名空间的Cache配置，两个命名空间的操作使用的是同一个Cache。 二级缓存实验接下来我们通过实验，了解MyBatis二级缓存在使用上的一些特点。 在本实验中，id为1的学生名称初始化为点点。 实验1测试二级缓存效果，不提交事务，sqlSession1查询完数据后，sqlSession2相同的查询是否会从缓存中获取数据。 1234567891011@Testpublic void testCacheWithoutCommitOrClose() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); System.out.println("studentMapper读取数据: " + studentMapper.getStudentById(1)); System.out.println("studentMapper2读取数据: " + studentMapper2.getStudentById(1));&#125; 执行结果： 我们可以看到，当sqlsession没有调用commit()方法时，二级缓存并没有起到作用。 实验2测试二级缓存效果，当提交事务时，sqlSession1查询完数据后，sqlSession2相同的查询是否会从缓存中获取数据。 123456789101112@Testpublic void testCacheWithCommitOrClose() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); System.out.println("studentMapper读取数据: " + studentMapper.getStudentById(1)); sqlSession1.commit(); System.out.println("studentMapper2读取数据: " + studentMapper2.getStudentById(1));&#125; 从图上可知，sqlsession2的查询，使用了缓存，缓存的命中率是0.5。 实验3测试update操作是否会刷新该namespace下的二级缓存。 123456789101112131415161718@Testpublic void testCacheWithUpdate() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); SqlSession sqlSession3 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); StudentMapper studentMapper3 = sqlSession3.getMapper(StudentMapper.class); System.out.println("studentMapper读取数据: " + studentMapper.getStudentById(1)); sqlSession1.commit(); System.out.println("studentMapper2读取数据: " + studentMapper2.getStudentById(1)); studentMapper3.updateStudentName("方方",1); sqlSession3.commit(); System.out.println("studentMapper2读取数据: " + studentMapper2.getStudentById(1));&#125; 我们可以看到，在sqlSession3更新数据库，并提交事务后，sqlsession2的StudentMapper namespace下的查询走了数据库，没有走Cache。 实验4验证MyBatis的二级缓存不适应用于映射文件中存在多表查询的情况。 通常我们会为每个单表创建单独的映射文件，由于MyBatis的二级缓存是基于namespace的，多表查询语句所在的namspace无法感应到其他namespace中的语句对多表查询中涉及的表进行的修改，引发脏数据问题。 123456789101112131415161718@Testpublic void testCacheWithDiffererntNamespace() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); SqlSession sqlSession3 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); ClassMapper classMapper = sqlSession3.getMapper(ClassMapper.class); System.out.println("studentMapper读取数据: " + studentMapper.getStudentByIdWithClassInfo(1)); sqlSession1.close(); System.out.println("studentMapper2读取数据: " + studentMapper2.getStudentByIdWithClassInfo(1)); classMapper.updateClassName("特色一班",1); sqlSession3.commit(); System.out.println("studentMapper2读取数据: " + studentMapper2.getStudentByIdWithClassInfo(1));&#125; 执行结果： 在这个实验中，我们引入了两张新的表，一张class，一张classroom。class中保存了班级的id和班级名，classroom中保存了班级id和学生id。我们在StudentMapper中增加了一个查询方法getStudentByIdWithClassInfo，用于查询学生所在的班级，涉及到多表查询。在ClassMapper中添加了updateClassName，根据班级id更新班级名的操作。 当sqlsession1的studentmapper查询数据后，二级缓存生效。保存在StudentMapper的namespace下的cache中。当sqlSession3的classMapper的updateClassName方法对class表进行更新时，updateClassName不属于StudentMapper的namespace，所以StudentMapper下的cache没有感应到变化，没有刷新缓存。当StudentMapper中同样的查询再次发起时，从缓存中读取了脏数据。 实验5为了解决实验4的问题呢，可以使用Cache ref，让ClassMapper引用StudenMapper命名空间，这样两个映射文件对应的SQL操作都使用的是同一块缓存了。 执行结果： 不过这样做的后果是，缓存的粒度变粗了，多个Mapper namespace下的所有操作都会对缓存使用造成影响。 二级缓存源码分析MyBatis二级缓存的工作流程和前文提到的一级缓存类似，只是在一级缓存处理前，用CachingExecutor装饰了BaseExecutor的子类，在委托具体职责给delegate之前，实现了二级缓存的查询和写入功能，具体类关系图如下图所示。 源码分析源码分析从CachingExecutor的query方法展开，源代码走读过程中涉及到的知识点较多，不能一一详细讲解，读者朋友可以自行查询相关资料来学习。 CachingExecutor的query方法，首先会从MappedStatement中获得在配置初始化时赋予的Cache。 1Cache cache = ms.getCache(); 本质上是装饰器模式的使用，具体的装饰链是： SynchronizedCache -&gt; LoggingCache -&gt; SerializedCache -&gt; LruCache -&gt; PerpetualCache。 以下是具体这些Cache实现类的介绍，他们的组合为Cache赋予了不同的能力。 SynchronizedCache：同步Cache，实现比较简单，直接使用synchronized修饰方法。 LoggingCache：日志功能，装饰类，用于记录缓存的命中率，如果开启了DEBUG模式，则会输出命中率日志。 SerializedCache：序列化功能，将值序列化后存到缓存中。该功能用于缓存返回一份实例的Copy，用于保存线程安全。 LruCache：采用了Lru算法的Cache实现，移除最近最少使用的Key/Value。 PerpetualCache： 作为为最基础的缓存类，底层实现比较简单，直接使用了HashMap。 然后是判断是否需要刷新缓存，代码如下所示： 1flushCacheIfRequired(ms); 在默认的设置中SELECT语句不会刷新缓存，insert/update/delte会刷新缓存。进入该方法。代码如下所示： 123456private void flushCacheIfRequired(MappedStatement ms) &#123; Cache cache = ms.getCache(); if (cache != null &amp;&amp; ms.isFlushCacheRequired()) &#123; tcm.clear(cache); &#125;&#125; MyBatis的CachingExecutor持有了TransactionalCacheManager，即上述代码中的tcm。 TransactionalCacheManager中持有了一个Map，代码如下所示： 1private Map transactionalCaches = new HashMap(); 这个Map保存了Cache和用TransactionalCache包装后的Cache的映射关系。 TransactionalCache实现了Cache接口，CachingExecutor会默认使用他包装初始生成的Cache，作用是如果事务提交，对缓存的操作才会生效，如果事务回滚或者不提交事务，则不对缓存产生影响。 在TransactionalCache的clear，有以下两句。清空了需要在提交时加入缓存的列表，同时设定提交时清空缓存，代码如下所示： 12345@Overridepublic void clear() &#123; clearOnCommit = true; entriesToAddOnCommit.clear();&#125; CachingExecutor继续往下走，ensureNoOutParams主要是用来处理存储过程的，暂时不用考虑。 12if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123; ensureNoOutParams(ms, parameterObject, boundSql); 之后会尝试从tcm中获取缓存的列表。 1List list = (List) tcm.getObject(cache, key); 在getObject方法中，会把获取值的职责一路传递，最终到PerpetualCache。如果没有查到，会把key加入Miss集合，这个主要是为了统计命中率。 1234Object object = delegate.getObject(key);if (object == null) &#123; entriesMissedInCache.add(key);&#125; CachingExecutor继续往下走，如果查询到数据，则调用tcm.putObject方法，往缓存中放入值。 1234if (list == null) &#123; list = delegate. query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); tcm.putObject(cache, key, list); // issue #578 and #116&#125; tcm的put方法也不是直接操作缓存，只是在把这次的数据和key放入待提交的Map中。 1234@Overridepublic void putObject(Object key, Object object) &#123; entriesToAddOnCommit.put(key, object);&#125; 从以上的代码分析中，我们可以明白，如果不调用commit方法的话，由于TranscationalCache的作用，并不会对二级缓存造成直接的影响。因此我们看看Sqlsession的commit方法中做了什么。代码如下所示： 1234@Overridepublic void commit(boolean force) &#123; try &#123; executor.commit(isCommitOrRollbackRequired(force)); 因为我们使用了CachingExecutor，首先会进入CachingExecutor实现的commit方法。 12345@Overridepublic void commit(boolean required) throws SQLException &#123; delegate.commit(required); tcm.commit();&#125; 会把具体commit的职责委托给包装的Executor。主要是看下tcm.commit()，tcm最终又会调用到TrancationalCache。 1234567public void commit() &#123; if (clearOnCommit) &#123; delegate.clear(); &#125; flushPendingEntries(); reset();&#125; 看到这里的clearOnCommit就想起刚才TrancationalCache的clear方法设置的标志位，真正的清理Cache是放到这里来进行的。具体清理的职责委托给了包装的Cache类。之后进入flushPendingEntries方法。代码如下所示： 123456private void flushPendingEntries() &#123; for (Map.Entry entry : entriesToAddOnCommit.entrySet()) &#123; delegate.putObject(entry.getKey(), entry.getValue()); &#125; ................&#125; 在flushPendingEntries中，将待提交的Map进行循环处理，委托给包装的Cache类，进行putObject的操作。 后续的查询操作会重复执行这套流程。如果是insert|update|delete的话，会统一进入CachingExecutor的update方法，其中调用了这个函数，代码如下所示： 1private void flushCacheIfRequired(MappedStatement ms) 在二级缓存执行流程后就会进入一级缓存的执行流程，因此不再赘述。 总结 MyBatis的二级缓存相对于一级缓存来说，实现了SqlSession之间缓存数据的共享，同时粒度更加的细，能够到namespace级别，通过Cache接口实现类不同的组合，对Cache的可控性也更强。 MyBatis在多表查询时，极大可能会出现脏数据，有设计上的缺陷，安全使用二级缓存的条件比较苛刻。 在分布式环境下，由于默认的MyBatis Cache实现都是基于本地的，分布式环境下必然会出现读取到脏数据，需要使用集中式缓存将MyBatis的Cache接口实现，有一定的开发成本，直接使用Redis、Memcached等分布式缓存可能成本更低，安全性也更高。 全文总结本文对介绍了MyBatis一二级缓存的基本概念，并从应用及源码的角度对MyBatis的缓存机制进行了分析。最后对MyBatis缓存机制做了一定的总结，个人建议MyBatis缓存特性在生产环境中进行关闭，单纯作为一个ORM框架使用可能更为合适。]]></content>
      <categories>
        <category>Mybatis</category>
      </categories>
      <tags>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用自定义count SQL]]></title>
    <url>%2Fposts%2F4247%2F</url>
    <content type="text"><![CDATA[pagehelper 5.1.8 mybatis 3.4.4 mybatis-spring 1.3.0 PageHelper 5.0.4 增加手写 count 查询支持增加countSuffixcount 查询后缀配置参数，该参数是针对PageInterceptor配置的，默认值为_COUNT。 分页插件会优先通过当前查询的 msId +countSuffix查找手写的分页查询。 如果存在就使用手写的 count 查询，如果不存在，仍然使用之前的方式自动创建 count 查询。 例如，如果存在下面两个查询： 123456789&lt;select id="selectLeftjoin" resultType="com.github.pagehelper.model.Country"&gt; select a.id,b.countryname,a.countrycode from country a left join country b on a.id = b.id order by a.id&lt;/select&gt;&lt;select id="selectLeftjoin_COUNT" resultType="Long"&gt; select count(distinct a.id) from country a left join country b on a.id = b.id&lt;/select&gt; 上面的countSuffix使用的默认值_COUNT，分页插件会自动获取到selectLeftjoin_COUNT查询，这个查询需要自己保证结果数正确。 返回值的类型必须是resultType=&quot;Long&quot;，入参使用的和selectLeftjoin查询相同的参数，所以在 SQL 中要按照selectLeftjoin的入参来使用。 因为selectLeftjoin_COUNT方法是自动调用的，所以不需要在接口提供相应的方法，如果需要单独调用，也可以提供。 上面方法执行输出的部分日志如下： 123456789101112DEBUG [main] - ==&gt; Preparing: select count(distinct a.id) from country a left join country b on a.id = b.id DEBUG [main] - ==&gt; Parameters: TRACE [main] - &lt;== Columns: C1TRACE [main] - &lt;== Row: 183DEBUG [main] - &lt;== Total: 1DEBUG [main] - Cache Hit Ratio [com.github.pagehelper.mapper.CountryMapper]: 0.0DEBUG [main] - ==&gt; Preparing: select a.id,b.countryname,a.countrycode from country a left join country b on a.id = b.id order by a.id LIMIT 10 DEBUG [main] - ==&gt; Parameters: TRACE [main] - &lt;== Columns: ID, COUNTRYNAME, COUNTRYCODETRACE [main] - &lt;== Row: 1, Angola, AOTRACE [main] - &lt;== Row: 2, Afghanistan, AFTRACE [main] - &lt;== Row: 3, Albania, AL]]></content>
      <categories>
        <category>Mybatis</category>
      </categories>
      <tags>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadPoolExecutor线程池参数设置技巧]]></title>
    <url>%2Fposts%2Fff4c%2F</url>
    <content type="text"><![CDATA[一、ThreadPoolExecutor的重要参数 corePoolSize：核心线程数 核心线程会一直存活，及时没有任务需要执行 当线程数小于核心线程数时，即使有线程空闲，线程池也会优先创建新线程处理 设置allowCoreThreadTimeout=true（默认false）时，核心线程会超时关闭 queueCapacity：任务队列容量（阻塞队列） 当核心线程数达到最大时，新任务会放在队列中排队等待执行 maxPoolSize：最大线程数 当线程数&gt;=corePoolSize，且任务队列已满时。线程池会创建新线程来处理任务 当线程数=maxPoolSize，且任务队列已满时，线程池会拒绝处理任务而抛出异常 keepAliveTime：线程空闲时间 当线程空闲时间达到keepAliveTime时，线程会退出，直到线程数量=corePoolSize 如果allowCoreThreadTimeout=true，则会直到线程数量=0 allowCoreThreadTimeout：允许核心线程超时 rejectedExecutionHandler：任务拒绝处理器 两种情况会拒绝处理任务： 当线程数已经达到maxPoolSize，切队列已满，会拒绝新任务 当线程池被调用shutdown()后，会等待线程池里的任务执行完毕，再shutdown。如果在调用shutdown()和线程池真正shutdown之间提交任务，会拒绝新任务 线程池会调用rejectedExecutionHandler来处理这个任务。如果没有设置默认是AbortPolicy，会抛出异常 ThreadPoolExecutor类有几个内部实现类来处理这类情况： AbortPolicy 丢弃任务，抛运行时异常 CallerRunsPolicy 执行任务 DiscardPolicy 忽视，什么都不会发生 DiscardOldestPolicy 从队列中踢出最先进入队列（最后一个执行）的任务 实现RejectedExecutionHandler接口，可自定义处理器 二、ThreadPoolExecutor执行顺序：线程池按以下行为执行任务 当线程数小于核心线程数时，创建线程。 当线程数大于等于核心线程数，且任务队列未满时，将任务放入任务队列。 当线程数大于等于核心线程数，且任务队列已满 若线程数小于最大线程数，创建线程 若线程数等于最大线程数，抛出异常，拒绝任务 三、如何设置参数 默认值 corePoolSize=1 queueCapacity=Integer.MAX_VALUE maxPoolSize=Integer.MAX_VALUE keepAliveTime=60s allowCoreThreadTimeout=false rejectedExecutionHandler=AbortPolicy() 如何来设置 需要根据几个值来决定 tasks ：每秒的任务数，假设为500~1000 taskcost：每个任务花费时间，假设为0.1s responsetime：系统允许容忍的最大响应时间，假设为1s 做几个计算 corePoolSize = 每秒需要多少个线程处理？ threadcount = tasks/(1/taskcost) =taskstaskcout = (500~1000)0.1 = 50~100 个线程。corePoolSize设置应该大于50 根据8020原则，如果80%的每秒任务数小于800，那么corePoolSize设置为80即可 queueCapacity = (coreSizePool/taskcost)*responsetime 计算可得 queueCapacity = 80/0.1*1 = 80。意思是队列里的线程可以等待1s，超过了的需要新开线程来执行 切记不能设置为Integer.MAX_VALUE，这样队列会很大，线程数只会保持在corePoolSize大小，当任务陡增时，不能新开线程来执行，响应时间会随之陡增。 maxPoolSize = (max(tasks)- queueCapacity)/(1/taskcost) 计算可得 maxPoolSize = (1000-80)/10 = 92 （最大任务数-队列容量）/每个线程每秒处理能力 = 最大线程数 rejectedExecutionHandler：根据具体情况来决定，任务不重要可丢弃，任务重要则要利用一些缓冲机制来处理 keepAliveTime和allowCoreThreadTimeout采用默认通常能满足 以上都是理想值，实际情况下要根据机器性能来决定。如果在未达到最大线程数的情况机器cpu load已经满了，则需要通过升级硬件（呵呵）和优化代码，降低taskcost来处理。]]></content>
      <categories>
        <category>MultiThread</category>
      </categories>
      <tags>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LockSupport]]></title>
    <url>%2Fposts%2F8785%2F</url>
    <content type="text"><![CDATA[用例1：子线程等待主线程发放许可！ 123456789101112131415161718192021222324252627public static void main(String[] args) &#123; Thread thread = new Thread()&#123; public void run()&#123; System.out.println("子线程 -&gt; 测试通行许可！"); LockSupport.park(); System.out.println("子线程 -&gt; 已通行！"); &#125; &#125;; thread.start(); System.out.println("主线程 -&gt; 休眠1秒！"); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("主线程 -&gt; 发放通行许可于子线程！"); LockSupport.unpark(thread); /* 运行结果： 主线程 -&gt; 休眠1秒！ 子线程 -&gt; 测试通行许可！ 主线程 -&gt; 发放通行许可于子线程！ 子线程 -&gt; 已通行！ */&#125; 用例2：主线程提前发放许可给子线程！123456789101112131415161718192021222324252627public static void main(String[] args) &#123; Thread thread = new Thread()&#123; public void run()&#123; System.out.println("子线程 -&gt; 休眠1秒！"); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("子线程 -&gt; 测试通行许可！"); LockSupport.park(); System.out.println("子线程 -&gt; 已通行！"); &#125; &#125;; thread.start(); System.out.println("主线程 -&gt; 提前发放通行许可于子线程！"); LockSupport.unpark(thread); /* 运行结果： 主线程 -&gt; 提前发放通行许可于子线程！ 子线程 -&gt; 休眠1秒！ 子线程 -&gt; 测试通行许可！ 子线程 -&gt; 已通行！ */&#125; 用例3：子线程传递数据给主线程。1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; Thread thread = new Thread()&#123; public void run()&#123; System.out.println("子线程 -&gt; 测试通行许可！并提供通行证：A"); LockSupport.park(new String("A")); System.out.println("子线程 -&gt; 已通行！"); &#125; &#125;; thread.start(); System.out.println("主线程 -&gt; 休眠1秒！"); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("主线程 -&gt; 检查并处理子线程的通行证：" + LockSupport.getBlocker(thread)); System.out.println("主线程 -&gt; 许可子线程通行！"); LockSupport.unpark(thread); /* 运行结果： 主线程 -&gt; 休眠1秒！ 子线程 -&gt; 测试通行许可！并提供通行证：A 主线程 -&gt; 检查并处理子线程的通行证：A 主线程 -&gt; 许可子线程通行！ 子线程 -&gt; 已通行！ */&#125; 全部操作： park()/park(Object) 等待通行准许。 parkNanos(long)/parkNanos(Object, long) 在指定运行时间（即相对时间）内，等待通行准许。 parkUntil(long)/parkUntil(Object, long) 在指定到期时间（即绝对时间）内，等待通行准许。 unpark(Thread) 发放通行准许或提前发放。（注：不管提前发放多少次，只用于一次性使用。） getBlocker(Thread) 进入等待通行准许时，所提供的对象。 主要用途：当前线程需要唤醒另一个线程，但是只确定它会进入阻塞，但不确定它是否已经进入阻塞，因此不管是否已经进入阻塞，还是准备进入阻塞，都将发放一个通行准许。 正确用法： 把LockSupport视为一个sleep()来用，只是sleep()是定时唤醒，LockSupport既可以定时唤醒，也可以由其它线程唤醒。]]></content>
      <categories>
        <category>MultiThread</category>
      </categories>
      <tags>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AtomicInteger]]></title>
    <url>%2Fposts%2Fe8f0%2F</url>
    <content type="text"><![CDATA[AtomicIntegerAtomicInteger，一个提供原子操作的Integer的类。在Java语言中，++i和i++操作并不是线程安全的，在使用的时候，不可避免的会用到synchronized关键字。而AtomicInteger则通过一种线程安全的加减操作接口。 123456789101112131415161718192021222324class Counter &#123; private volatile int count = 0; public synchronized void increment() &#123; count++; // 若要线程安全执行执行count++，需要加锁 &#125; public int getCount() &#123; return count; &#125;&#125;class Counter &#123; private AtomicInteger count = new AtomicInteger(); public void increment() &#123; count.incrementAndGet(); &#125; // 使用AtomicInteger之后，不需要加锁，也可以实现线程安全。 public int getCount() &#123; return count.get(); &#125;&#125; AtomicInteger常用接口: 1234567891011121314//获取当前的值 public final int get() //取当前的值，并设置新的值 public final int getAndSet(int newValue) //获取当前的值，并自增 public final int getAndIncrement() //获取当前的值，并自减 public final int getAndDecrement() //获取当前的值，并加上预期的值 public final int getAndAdd(int delta)]]></content>
      <categories>
        <category>MultiThread</category>
      </categories>
      <tags>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是CPU密集型、IO密集型]]></title>
    <url>%2Fposts%2F93d6%2F</url>
    <content type="text"><![CDATA[CPU密集型（CPU-bound） 12345CPU密集型也叫计算密集型，指的是系统的硬盘、内存性能相对CPU要好很多，此时，系统运作大部分的状况是CPU Loading 100%，CPU要读/写I/O(硬盘/内存)，I/O在很短的时间就可以完成，而CPU还有许多运算要处理，CPU Loading很高。在多重程序系统中，大部份时间用来做计算、逻辑判断等CPU动作的程序称之CPU bound。例如一个计算圆周率至小数点一千位以下的程序，在执行的过程当中绝大部份时间用在三角函数和开根号的计算，便是属于CPU bound的程序。CPU bound的程序一般而言CPU占用率相当高。这可能是因为任务本身不太需要访问I/O设备，也可能是因为程序是多线程实现因此屏蔽掉了等待I/O的时间。 IO密集型（I/O bound）123IO密集型指的是系统的CPU性能相对硬盘、内存要好很多，此时，系统运作，大部分的状况是CPU在等I/O (硬盘/内存) 的读/写操作，此时CPU Loading并不高。I/O bound的程序一般在达到性能极限时，CPU占用率仍然较低。这可能是因为任务本身需要大量I/O操作，而pipeline做得不是很好，没有充分利用处理器能力。 CPU密集型 vs IO密集型1234567891011我们可以把任务分为计算密集型和IO密集型。计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。总之，计算密集型程序适合C语言多线程，I/O密集型适合脚本语言开发的多线程。]]></content>
      <categories>
        <category>MultiThread</category>
      </categories>
      <tags>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven PurgingLocalRepository]]></title>
    <url>%2Fposts%2F63e4%2F</url>
    <content type="text"><![CDATA[Purging local repository dependenciesThe purpose of thedependency:purge-local-repositorygoal is to purge (delete and optionally re-resolve) artifacts from the local maven repository. This page describes some of the configuration options available to the plugin. 1mvn dependency:purge-local-repository Dependency includes/excludes12345678910111213141516171819202122232425262728&lt;project&gt; [...] &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;purge-local-dependencies&lt;/id&gt; &lt;phase&gt;process-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;purge-local-repository&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt;[groupId1]:[artifactId1]&lt;/exclude&gt; &lt;exclude&gt;[groupId2]:[artifactId2]&lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; [...]&lt;/project&gt;]]></content>
      <categories>
        <category>Maven</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven maven.test.skip和skipTests的区别]]></title>
    <url>%2Fposts%2F9298%2F</url>
    <content type="text"><![CDATA[-DskipTests，不执行测试用例，但编译测试用例类生成相应的class文件至target/test-classes下。 -Dmaven.test.skip=true，不执行测试用例，也不编译测试用例类。 也可以在pom.xml文件中修改 12345678910111213141516&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugin&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;configuration&gt; &lt;skip&gt;true&lt;/skip&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;configuration&gt; &lt;skip&gt;true&lt;/skip&gt; &lt;/configuration&gt; &lt;/plugin&gt;]]></content>
      <categories>
        <category>Maven</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven Deploy]]></title>
    <url>%2Fposts%2F92b0%2F</url>
    <content type="text"><![CDATA[pom.xml配置123456789101112&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;name&gt;Internal Releases&lt;/name&gt; &lt;url&gt;http://121.40.185.58:8081/nexus/content/repositories/thirdparty&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;name&gt;Internal Releases&lt;/name&gt; &lt;url&gt;http://121.40.185.58:8081/nexus/content/repositories/thirdparty&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; setting.xml文件配置12345&lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt;]]></content>
      <categories>
        <category>Maven</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lambda表达式]]></title>
    <url>%2Fposts%2F8609%2F</url>
    <content type="text"><![CDATA[# 语法123(parameters) -&gt; expression或(parameters) -&gt;&#123; statements; &#125; 可选类型声明：不需要声明参数类型，编译器可以统一识别参数值。 可选的参数圆括号：一个参数无需定义圆括号，但多个参数需要定义圆括号。 可选的大括号：如果主体包含了一个语句，就不需要使用大括号。 可选的返回关键字：如果主体只有一个表达式返回值则编译器会自动返回值，大括号需要指定明表达式返回了一个数值。 Lambda 表达式实例1234567891011121314// 1. 不需要参数,返回值为 5 () -&gt; 5 // 2. 接收一个参数(数字类型),返回其2倍的值 x -&gt; 2 * x // 3. 接受2个参数(数字),并返回他们的差值 (x, y) -&gt; x – y // 4. 接收2个int型整数,返回他们的和 (int x, int y) -&gt; x + y // 5. 接受一个 string 对象,并在控制台打印,不返回任何值(看起来像是返回void) (String s) -&gt; System.out.print(s) 变量作用域lambda 表达式只能引用标记了 final 的外层局部变量，这就是说不能在 lambda 内部修改定义在域外的局部变量，否则会编译错误。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka消息事务]]></title>
    <url>%2Fposts%2F76c2%2F</url>
    <content type="text"><![CDATA[数据传输事务的定义 最多一次：消息不会被重复发送，最多被传输一次，但也有可能一次不传输。 最少一次：消息不会被漏发送，最少被传输一次，但也有可能被重复发送。 精确的一次（Exactly once）：不会漏传输也不会重复传输，每个消息都被传输一次而且仅仅被传输一次，这是大家所期望的。 事务保证 内部重试问题：Preceducer幂等处理 多分区原子写入]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka介绍]]></title>
    <url>%2Fposts%2Fd84c%2F</url>
    <content type="text"><![CDATA[LinkedIn 开源 分布式数据同步系统Databus 高性能计算引擎Cubert Java异步处理框架ParSeq Kafka流处理平台 A streaming platform has three key capabilities: Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system. Store streams of records in a fault-tolerant durable way. Process streams of records as they occur. Kafka is generally used for two broad classes of applications: Building real-time streaming data pipelines that reliably get data between systems or applications Building real-time streaming applications that transform or react to the streams of data]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka基本概念]]></title>
    <url>%2Fposts%2F6dd5%2F</url>
    <content type="text"><![CDATA[Producer消息和数据的生产者，向Kafka的一个topic发布消息的进程/代码/服务 Consumer消息和数据的消费者，订阅数据topic并且处理其发布的消息的进程/代码/服务 ConsumerGroup逻辑概念，对于同一个topic，会广播给不同的group，一个group中，只有一个consumer可以消费该消息 Broker物理概念，Kafka集群中的每个Kafka节点 Topic逻辑概念，Kafka消息的类别，对数据进行区分、隔离 Partition物理概念，Kafka下数据存储的基本单元。一个Topic数据，会被分散存储到多个Partition，每一个Partition是有序的 每一个Topic被切分为多个Partitions 消费者数目少于或等于Partition的数目 Broker Group中的每一个Broker保存Topic的一个或多个Partitions Consumer Group中的仅有一个Consumer读取Topic的一个或多个Partitions，并且是唯一的Consumer Replication同一个Partition可能会有多个Replica，多个Replica之间数据是一样的 当集群中有Broker挂掉的情况，系统可以主动地使用Replicas提供服务 系统默认设置每一个Topic的replication系数为1，可以在创建Topic时单独设置 Replication的基本单位是Topic的Partition 所有的读和写都从Leader进，Followers只是做备份 Followers必须能够及时复制Leader的数据 增加容错性与可扩展性 Replication Leader一个Partition的多个Replica上，需要一个Leader负责该Partition上与Producer和Consumer交互 ReplicaManager负责管理当前broker所有分区和副本的信息，处理KafkaController发起的一些请求，副本状态的切换、添加/读取消息等]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[REST POST&PUT]]></title>
    <url>%2Fposts%2F90ac%2F</url>
    <content type="text"><![CDATA[REST – PUT vs POSTIt has been observed that many people struggle to choose betweenHTTP PUT vs POSTmethods when designing a system. Though,RFC 2616has been very clear in differentiating between the two – yet complex wordings are a source of confusion for many of us. Let’s try to solve the puzzlewhen to use PUT or POST. Let’s compare them for better understanding. PUT POST RFC-2616 clearly mention that PUT method requests for the enclosed entity be stored under the supplied Request-URI. If the Request-URI refers to an already existing resource – an update operation will happen, otherwise create operation should happen if Request-URI is a valid resource URI (assuming client is allowed to determine resource identifier). The POST method is used to request that the origin server accept the entity enclosed in the request as a new subordinate of the resource identified by the Request-URI in the Request-Line. It essentially means that POST request-URI should be of a collection URI. PUT method is idempotent. So if you send retry a request multiple times, that should be equivalent to single request modification. POST is NOT idempotent. So if you retry the request N times, you will end up having N resources with N different URIs created on server. Use PUT when you want to modify a singular resource which is already a part of resources collection. PUT replaces the resource in its entirety. Use PATCH if request updates part of the resource. Use POST when you want to add a child resource under resources collection. PUT is idempotent, so you can cache the response. Responses to this method are not cacheable, unless the response includes appropriate Cache-Control or Expires header fields. However, the 303 (See Other) response can be used to direct the user agent to retrieve a cacheable resource. Generally, in practice, always use PUT for UPDATE operations. Always use POST for CREATE operations. PUT /questions/{question-id} | ThePOSTmethod is used to request that the origin server accept the entity enclosed in the request as a new subordinate of the resource identified by the Request-URI in the Request-Line. It essentially means thatPOSTrequest-URI should be of a collection URI. POST /questions | | PUTmethod isidempotent. So if you send retry a request multiple times, that should be equivalent to single request modification. | POSTis NOT idempotent. So if you retry the request N times, you will end up having N resources with N different URIs created on server. | | UsePUTwhen you want to modify a singular resource which is already a part of resources collection. PUT replaces the resource in its entirety. Use PATCH if request updates part of the resource. | UsePOSTwhen you want to add a child resource under resources collection. | | PUTis idempotent, so you can cache the response. | Responses to this method are notcacheable, unless the response includes appropriate Cache-Control or Expires header fields. However, the 303 (See Other) response can be used to direct the user agent to retrieve a cacheable resource. | | Generally, in practice, always usePUTfor UPDATE operations. | Always usePOSTfor CREATE operations. | PUT vs POST : An ExampleLet’s say we are designing a network application. Let’s list down few URIs and their purpose to get better understanding when to usePOSTand when to usePUToperations. GET /device-management/devices : Get all devices POST /device-management/devices : Create a new device GET /device-management/devices/{id} : Get the device information identified by “id” PUT /device-management/devices/{id} : Update the device information identified by “id” DELETE /device-management/devices/{id} : Delete device by “id” Follow the similar URI design practices for other resources as well.]]></content>
      <tags>
        <tag>Restful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OKHttpUtil]]></title>
    <url>%2Fposts%2F5275%2F</url>
    <content type="text"><![CDATA[OKHttpUtil 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329package com.jdyun.weixin.util;import java.io.IOException;import java.io.UnsupportedEncodingException;import java.net.MalformedURLException;import java.net.URL;import java.security.NoSuchAlgorithmException;import java.security.SecureRandom;import java.util.Map;import java.util.concurrent.TimeUnit;import javax.net.ssl.HostnameVerifier;import javax.net.ssl.SSLContext;import javax.net.ssl.SSLSession;import javax.net.ssl.SSLSocketFactory;import javax.net.ssl.TrustManager;import javax.net.ssl.X509TrustManager;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.stereotype.Component;import java.security.KeyManagementException;import java.security.cert.X509Certificate;import okhttp3.ConnectionPool;import okhttp3.FormBody;import okhttp3.OkHttpClient;import okhttp3.Request;import okhttp3.RequestBody;import okhttp3.Response;import okhttp3.FormBody.Builder;import okhttp3.MediaType;/*** Description: 用okhttp进行get和post的访问* Copyright: jdy groups 2019年1月29日* Company: jdyun * @author jdyun* @date 2018年1月29日* @version 1.0*/@Component public class OKHttpUtil &#123; protected static Logger logger=LoggerFactory.getLogger(OKHttpUtil.class); //log protected static Logger interfaceLog = LoggerFactory.getLogger("interfaceLog");//interfaceLog private static OkHttpClient httpClient; @Value("$&#123;client.backurl&#125;") String backurl; @Value("$&#123;client.name&#125;") String name; @Value("$&#123;client.secretkey&#125;") String secretkey; /** * 绕过验证 * * @return * @throws NoSuchAlgorithmException * @throws KeyManagementException */ @Bean //获取这个SSLSocketFactory public static SSLSocketFactory getSSLSocketFactory() &#123; try &#123; SSLContext sslContext = SSLContext.getInstance("SSL"); sslContext.init(null, getTrustManager(), new SecureRandom()); return sslContext.getSocketFactory(); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; @Bean //获取TrustManager private static TrustManager[] getTrustManager() &#123; TrustManager[] trustAllCerts = new TrustManager[]&#123; new X509TrustManager() &#123; @Override public void checkClientTrusted(X509Certificate[] chain, String authType) &#123; &#125; @Override public void checkServerTrusted(X509Certificate[] chain, String authType) &#123; &#125; @Override public X509Certificate[] getAcceptedIssuers() &#123; return new X509Certificate[]&#123;&#125;; &#125; &#125; &#125;; return trustAllCerts; &#125; //获取HostnameVerifier public static HostnameVerifier getHostnameVerifier() &#123; HostnameVerifier hostnameVerifier = new HostnameVerifier() &#123; @Override public boolean verify(String s, SSLSession sslSession) &#123; return true; &#125; &#125;; return hostnameVerifier; &#125; public OKHttpUtil()&#123; if(httpClient == null)&#123; ConnectionPool pool = new ConnectionPool(5, 50, TimeUnit.MINUTES); // 配置请求的超时设置 设置连接池属性 设置ssl绕过验证 httpClient = new OkHttpClient.Builder() // .connectTimeout(1, TimeUnit.MINUTES) // .followRedirects(true) // .readTimeout(1, TimeUnit.MINUTES) // .retryOnConnectionFailure(false) // .writeTimeout(3, TimeUnit.MINUTES) // .connectionPool(pool) // .sslSocketFactory(getSSLSocketFactory()) .hostnameVerifier(getHostnameVerifier()) .build(); &#125; &#125; private void config(Request.Builder builder) &#123; builder.addHeader("User-Agent", "Mozilla/5.0"); builder.addHeader("Accept","text/html,application/xhtml+xml,application/xml,application/json;q=0.9,*/*;q=0.8"); builder.addHeader("Accept-Language", "zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3");//"en-US,en;q=0.5"); builder.addHeader("Accept-Charset", "ISO-8859-1,utf-8,gbk,gb2312;q=0.7,*;q=0.7"); builder.addHeader("Connection", "close"); builder.addHeader("secretkey", secretkey); builder.addHeader("name", name); &#125; /** *httpGet * Description:get请求 * Copyright: jdyun 2019年1月30日 * Company: * @author jdyun-ceo * @date 2019年1月30日 下午7:39:44 * @version 1.0 * @param strurl * @return */ public String httpGet(Map paramurl,String url) throws MalformedURLException, UnsupportedEncodingException, NoSuchAlgorithmException&#123; String strUrl=backurl+url; String addres = ""; int i = 0; for (Map.Entry entry : paramurl.entrySet()) &#123; if(i == 0)&#123; addres += "?"+entry.getKey() +"="+entry.getValue(); &#125;else&#123; addres += "&amp;"+entry.getKey() +"="+entry.getValue(); &#125; i++; &#125; strUrl += addres; URL u=new URL(strUrl); Request.Builder builder =new Request.Builder(); config(builder); String rtn="&#123;\"status\":\"fail\"&#125;"; try &#123; Request request = builder.url(u).get() .build(); Response response = httpClient.newCall(request).execute(); if(response.isSuccessful())&#123; rtn = response.body().string(); interfaceLog.info("系统"+url+"接口result===============&gt;&gt;"+rtn); &#125; &#125; catch (Exception e) &#123; logger.error("系统"+url+"error===============&gt;&gt;"+e); &#125;finally&#123; &#125; return rtn; &#125; /** * * httpPost * Description: post提交表单 * Copyright: 2018年1月30日 * Company: * @author email:bleach.song@.cn * @date 2018年1月30日 下午7:39:29 * @version 1.0 * @param map * @param strurl * @return */ public String httpPost(Map map,String strurl) &#123; // 获取配置文件验证参数 strurl=backurl+strurl; String result = null; Request.Builder builder =new Request.Builder(); config(builder); Builder body = new FormBody.Builder(); for (Map.Entry entry : map.entrySet()) &#123; body.add(entry.getKey(), entry.getValue()==null?"":entry.getValue().toString()); &#125; FormBody formbody = body.build(); try &#123; Request request = builder.url(strurl).post(formbody) .build(); Response response = httpClient.newCall(request).execute(); if(response.isSuccessful())&#123; result = response.body().string(); interfaceLog.info("系统"+strurl+"接口result===============&gt;&gt;"+result); &#125; &#125; catch (IllegalArgumentException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; &#125; return result; &#125; /** * * httpGetForWechat * Description:微信用 * Copyright: 2018年8月29日 * Company: * @author jdyun * @date 2019年8月29日 下午3:56:02 * @version 1.0 * @param paramurl * @param url * @return * @throws MalformedURLException * @throws UnsupportedEncodingException * @throws NoSuchAlgorithmException */ public String httpGetForJSTicket(String paramurl,String url) throws MalformedURLException, UnsupportedEncodingException, NoSuchAlgorithmException&#123; url += paramurl; URL u=new URL(url); Request.Builder builder =new Request.Builder(); config(builder); String rtn="&#123;\"status\":\"fail\"&#125;"; try &#123; Request request = builder.url(u).get() .build(); Response response = httpClient.newCall(request).execute(); if(response.isSuccessful())&#123; rtn = response.body().string(); interfaceLog.info("httpGetForJSTicketrtn===============&gt;&gt;"+rtn); &#125; &#125; catch (Exception e) &#123; logger.error("httpGetForJSTicketerror===============&gt;&gt;"+e); &#125; return rtn; &#125; public String httpGetWebtoken(String paramurl,String url) throws MalformedURLException, UnsupportedEncodingException, NoSuchAlgorithmException&#123; url += paramurl; URL u=new URL(url); Request.Builder builder =new Request.Builder(); config(builder); String rtn="&#123;\"status\":\"fail\"&#125;"; try &#123; Request request = builder.url(u).get().build(); Response response = httpClient.newCall(request).execute(); Boolean para = response.isSuccessful(); interfaceLog.info("para===============&gt;&gt;"+para); if(para)&#123; rtn = response.body().string(); interfaceLog.info("httpGetWebtokenrtn===============&gt;&gt;"+rtn); &#125; &#125; catch (Exception e) &#123; logger.error("httpGetWebtokenerror===============&gt;&gt;"+e); &#125; return rtn; &#125; /** * * gettoken * Copyright: 2018年9月12日 * Company: * @author jdyun * @date 2018年9月12日 上午10:48:23 * @version 1.0 * @param url * @return * @throws MalformedURLException * @throws UnsupportedEncodingException * @throws NoSuchAlgorithmException */ public String gettoken(String url) throws MalformedURLException, UnsupportedEncodingException, NoSuchAlgorithmException&#123; URL u=new URL(url); Request.Builder builder =new Request.Builder(); config(builder); String rtn="&#123;\"status\":\"fail\"&#125;"; try &#123; Request request = builder.url(u).get() .build(); Response response = httpClient.newCall(request).execute(); if(response.isSuccessful())&#123; rtn = response.body().string(); interfaceLog.info("本地token===============&gt;&gt;"+rtn); &#125; &#125; catch (Exception e) &#123; logger.error("gettokenerror===============&gt;&gt;"+e); &#125;finally&#123; &#125; return rtn; &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvisualvm 工具使用]]></title>
    <url>%2Fposts%2F950d%2F</url>
    <content type="text"><![CDATA[VisualVM 是Netbeans的profile子项目，已在JDK6.0 update 7 中自带(java启动时不需要特定参数，监控工具在bin/jvisualvm.exe)。 https://visualvm.dev.java.net/介绍VisualVM，能够监控线程，内存情况，查看方法的CPU时间和内存中的对 象，已被GC的对象，反向查看分配的堆栈(如100个String对象分别由哪几个对象分配出来的). 从界面上看还是比较简洁的，左边是树形结构，自动显示当前本机所运行的Java程序，还可以添加远程的Java VM，其中括号里面的PID指的是进程ID。OverView界面显示VM启动参数以及该VM对应的一些属性。Monitor界面则是监控Java堆大小，Permgen大小，Classes和线程数量。jdk不同版本中界面会不太一致，如有的含cpu监控，有的则不含（jdk1.6.0_10未包含）。 官网上关于jvisualvm的用法介绍 http://docs.oracle.com/javase/6/docs/technotes/tools/share/jvisualvm.html 作用：JVM和监控的应用程序运行在不同的服务器上，减轻应用程序的负担，特别是HeapDump的时候，应用常能够续负担很大。 VisualVM使用简单，几乎0配置，功能还是比较丰富的，几乎囊括了其它JDK自带命令的所有功能。 内存信息 线程信息 Dump堆（本地进程） Dump线程（本地进程） 打开堆Dump。堆Dump可以用jmap来生成。 打开线程Dump 生成应用快照（包含内存信息、线程信息等等） 性能分析。 :idea: CPU分析（各个方法调用时间，检查哪些方法耗时多），内存分析（各类对象占用的内存，检查哪些类占用内存多） …… 配置本地监控不需要配置，只要打开某个JAVA程序就会自动的加入到本地监控中。 远程监控： 本机的VisualVM就必须和远程的JVM要进行通信, Visualvm目前支持两种remote connection方式，分别是jstatd和JMX方式。 远程监控某个中间件时，需要修改中间件的启动文件，添加上关于jmx等的信息。 1、远程监控tomcat，jmx方式。 12345678910111213服务器 上的 tomcat 配置 jvm 启动参数。在 tomcat 的 catalina.bat 中添 加如下参数: JAVA_OPTS=&quot;$JAVA_OPTS -Djava.rmi.server.hostname=192.168.0.237 -Dcom.sun.management.jmxremote.port=18999 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false&quot;上述参数未设用户名与密码登录。 客户端VisualVM配置 (我客户端用的是WinXP). a.直接反键点击Remote，选择Add Remote Host... b.在弹出的界面中输入远程机器的IP地址(192.168.0.237)，这个IP地址会加入到Remote节点下. c.反键点击这个IP地址，选择Add JMX Connection, 在弹出的界面中输入刚配置的端口号(18999), 这个连接会加入到该IP节点下. d.反键点击这个连接，选择Open. 此处参数设置与jconsole工具远程监控tomcat相同。 2、监视websphere服务器JVM的配置 http://xjsunjie.blog.51cto.com/999372/1331880 jconsole &amp; jvisualvm远程监视websphere服务器JVM的配置案 在启动文件里配置。 3、jstatd 配置。 使用http://zhouanya.blog.51cto.com/4944792/1370017 visualVM 插件 https://github.com/irockel/tda visualVM还可通过扩展增加功能。启动页面时有“visualVM 扩展入门指南”，如果需要哪些插件可看下这里的介绍。 在“工具”-》“插件”-》可用插件项中列出。 除这些可用插件外，还可以增加第三方的插件扩展功能。 Third Party Plugins: BTrace Plugin: support for creating, deploying and saving BTrace scripts directly from the VisualVM. Home » Coherence Plugin: summarized statistics and information for a JMX enabled Coherence cluster. Home » CRaSH Plugin: support for the CRaSH open source shell for the Java Platform in VisualVM. Home » OSGi Plugin: basic management of OSGi platforms via JMX. Home » TDA Plugin: Thread Dump Analyzer is a GUI for analyzing thread dumps generated by the Java VM. Home » 其他插件里： Buffer Monitor: monitoring usage of direct buffers created by ByteBuffer.allocateDirect and mapped buffers created by FileChannel.map. Buffer Pools和MBeans Browser可以通过GUI的方式查看DirectMemory的即时使用情况。 如果是JDK 7及以上版本，可以用jconsole或者jvisualvm的MBean窗口查看java.nio.BufferPool.direct属性。参考：https://www.zhihu.com/question/55033583 使用实例1、插件“Visual GC”使用。转自：http://supercharles888.blog.51cto.com/609344/1179790 安装 ”Visual GC”插件: 这个插件是jvisualvm的插件，它非常强大，可以动态的对指定的进程进行监控，并且来通过统计面板来分类显示出各项任务/事件的总时间开销： 安装方法： Tool-&gt;Plugin-&gt;Available Plugins: 重启Visual VM 之后，就可以看到这个”Visual GC”已经被正确的显示了。 实战： 用Visual VM和Visual GC来优化我们的Eclipse启动： 首先，我们启动eclipse: 在ps -ef|grep eclipse（windows则是任务管理器）中，我们可以从看到这个进程id为32561 我们从Visual VM中找到对应的process id: 我们切换到 “Visual GC”标签页，它会显示启动eclipse的所有测量数据： 分析： 从上图中我们可以很明显的看出来，主要的时间开销在以下2方面： (1)编译时间有点长，用了3.794秒，这个时间主要是用来校验eclipse平台本身的字节码了，所以我们需要关闭字节码校验，让启动时候不会去校验平台本身（也是java写的）的字节码，为了达到这个目的，我们只需要在eclipse启动参数中加上-Xverify:none 如下所示，因为我们用的是Spring Source Tool Suite,所以我们在STS.ini中增加这一行。 (2)另外一个大问题就是类加载时间，它有2部分组成，因为类有2部分组成，一是eclipse平台自带的类，二是它所使用的插件的类文件，我们可以在eclipse启动的时候关闭不必要的插件加载来减少类加载时间，方法是Preference-&gt;General-&gt;Startup and Shutdown 校验结果： 现在我们把eclipse关闭并且重新打开，这会启动一个新的进程，id为32696,我们把这次Visual GC的测量图和原来的进行比较： 从这里可以看出来，时间被明显的缩短了，编译时间从3.794秒缩短到2.155秒，提升百分比为43.1%。而类加载时间从18.424秒缩短到10.208秒，提升百分比为44.6%。 额外步骤： 对于一些其他的启动参数，比如初始内存，最大内存，Gem,Perm的参数。 2、将堆dump的文件，使用其进行查看。 1）堆dump文件。 2）将dump文件传至jvisualvm本机，点击”文件“-》”装入“，选择第一步生成的dump文件。 a.摘要标签 可查看dump的各项信息。 文件-&gt;装入-&gt;堆Dump－&gt;检查 -&gt; 查找20保留大小最大的对象，就会触发保留大小的计算，然后就可以类视图里浏览，按保留大小排序了。 对象的大小有两种统计方式： 本身大小(Shallow Size)：对象本来的大小。 保留大小(Retained Size)： 当前对象大小 + 当前对象直接或间接引用到的对象的大小总和。 看本身大小时，占大头的都是char[] ,byte[]之类的，没什么意思（用jmap -histo:live pid 看的也是本身大小）。所以需要关心的是保留大小比较大的对象，看谁在引用这些char[], byte[]。 b.类 标签 双击某个类，点击实例视图。 c.实例试图 远程监控内存泄露、解决内存溢出问题1)内存泄露、溢出的异同 同：都会导致应用程序运行出现问题，性能下降或挂起。 异： v内存泄露是导致内存溢出的原因之一；内存泄露积累起来将导致内存溢出。 v内存泄露可以通过完善代码来避免；内存溢出可以通过调整配置来减少发生频率，但无法彻底避免。 2)监测内存泄漏 ·内存泄漏是指程序中间动态分配了内存，但在程序结束时没有释放这部分内存，从而造成那部分内存不可用的情况，重启计算机可以解决，但也有可能再次发生内存泄露，内存泄露和硬件没有关系，它是由软件设计缺陷引起的。 ·内存泄漏可以分为4类： a. 常发性内存泄漏。发生内存泄漏的代码会被多次执行到，每次被执行的时候都会导致一块内存泄漏。 b.偶发性内存泄漏。发生内存泄漏的代码只有在某些特定环境或操作过程下才会发生。常发性和偶发性是相对的。对于特定的环境，偶发性的也许就变成了常发性的。所以测试环境和测试方法对检测内存泄漏至关重要。 c.一次性内存泄漏。发生内存泄漏的代码只会被执行一次，或者由于算法上的缺陷，导致总会有一块仅且一块内存发生泄漏。比如，在类的构造函数中分配内存，在析构函数中却没有释放该内存，所以内存泄漏只会发生一次。 d.隐式内存泄漏。程序在运行过程中不停的分配内存，但是直到结束的时候才释放内存。严格的说这里并没有发生内存泄漏，因为最终程序释放了所有申请的内存。但是对于一个服务器程序，需要运行几天，几周甚至几个月，不及时释放内存也可能导致最终耗尽系统的所有内存。所以，我们称这类内存泄漏为隐式内存泄漏。 3)Heap dump 分析 每隔一段时间给所检测的java应用做一次heap dump： （或者在响应应用pid上鼠标右键heap dump）弹出以下提示框： 在应用服务器将此文件下载到jvisual vm所在的机器上，file–load打开此文件，如下面三图所示： 对比上面三个截图，发现似乎有个东西在急速飙升，仔细一看是这个对象：org.eclipse.swt.graphics.Image 在第一章图中这个还没排的上，第二次dump已经上升到5181个，第三次就是7845个了。涨速相当快，而且和任务管理器里面看到的GDI数量增涨一致，就是它了。 问题到这儿就比较清楚了，回到代码里面仔细一看可以发现，是某个地方反复的用图片来创建Image对象导致的，改掉以后搞定问题。 到这里其实我想说的是，Java使用起来其实要比C++更容易导致内存泄漏。对于C++来说，每一个申请的对象都必须明确释放，任何没有释放的对象都会导致memleak，这是不可饶恕的，而且这类问题已经有很多工具和方法来解决。但是到了Java里面情况就不同了，对于Java程序员来说对象都是不需要也无法主动销毁的，所以一般的思路是：随用随new，用完即丢。很多对象具体的生命周期可能连写代码的人自己也不清楚，或者不需要清楚，只知道某个时刻垃圾收集器会去做的，不用管。但很可能某个对象在“用完即丢”的时候在另一个不容易发现的地方被保存了一个引用，那么这个对象就永远不会被回收。更加糟糕的是整个程序从设计之初就没有考虑过对象回收的问题，对于C++程序员来说memleak必然是一个设计错误，但是对Java程序员来说这只是一个疏忽，而且似乎没有什么好的办法来避免。今天看到的这个问题是因为GDI泄漏会造成严重后果才被重视，但如果仅仅是造成内存泄漏，那这个程序可能得连续跑上个十天半个月才会发现问题。最后就是，对于c++，错误的代码在测试阶段就可以快速的侦测出哪怕一个byte的memleak并加以改正，但是对于java程序，理论上没有哪个工具能够知道是不是有泄漏，因为除了作者自己以外没有人能够知道一个被引用的对象是不是应该被销毁，只有通过大量的，长期的压力测试才能发现问题，这是很危险的一件事情。 4)解决内存溢出问题 1、java.lang.OutOfMemoryError: PermGen space JVM管理两种类型的内存，堆和非堆。堆是在JVM启动时创建；非堆是留给JVM自己用的，用来存放类的信息的。它和堆不同，运行期内GC不会释放空间。如果web app用了大量的第三方jar或者应用有太多的class文件而恰好MaxPermSize设置较小，超出了也会导致这块内存的占用过多造成溢出，或者tomcat热部署时侯不会清理前面加载的环境，只会将context更改为新部署的，非堆存的内容就会越来越多。 PermGen space的全称是Permanent Generation space，是指内存的永久保存区域，这块内存主要是被JVM存放Class和Meta信息的，Class在被Loader时就会被放到PermGen space中，它和存放类实例(Instance)的Heap区域不同，GC(Garbage Collection)不会在主程序运行期对PermGen space进行清理，所以如果你的应用中有很CLASS的话，就很可能出现PermGen space错误，这种错误常见在web服务器对JSP进行pre compile的时候。如果你的WEB APP下都用了大量的第三方jar, 其大小超过了jvm默认的大小(4M)那么就会产生此错误信息了。 如上图所示，PermGen在程序运行一段时间后超出了我们指定的128MB，通过Classes视图看到，Java在运行的同时加载了大量的类到内存中。PermGen会存储Jar或者Class的描述信息；所以在class大量增加的同时PermGen超出了我们指定的范围。为了让应用稳定，我们需要探寻新的PermGen范围。检测时段时候后（如下图）发现PermGen在145MB左右稳定，那么我们就得到了稳定的新参数；这样PermGen内存溢出的问题得到解决。 2、java.lang.OutOfMemoryError: Java heap space 第一种情况是个补充，主要存在问题就是出现在这个情况中。其默认空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。如果内存剩余不到40％，JVM就会增大堆到Xmx设置的值，内存剩余超过70％，JVM就会减小堆到Xms设置的值。所以服务器的Xmx和Xms设置一般应该设置相同避免每次GC后都要调整虚拟机堆的大小。假设物理内存无限大，那么JVM内存的最大值跟操作系统有关，一般32位机是1.5g到3g之间，而64位的就不会有限制了。 注意：如果Xms超过了Xmx值，或者堆最大值和非堆最大值的总和超过了物理内存或者操作系统的最大限制都会引起服务器启动不起来。 垃圾回收GC的角色，JVM调用GC的频度还是很高的，主要两种情况下进行垃圾回收： 一个是当应用程序线程空闲；另一个是java内存堆不足时，会不断调用GC，若连续回收都解决不了内存堆不足的问题时，就会报out of memory错误。因为这个异常根据系统运行环境决定，所以无法预期它何时出现。 根据GC的机制，程序的运行会引起系统运行环境的变化，增加GC的触发机会。 为了避免这些问题，程序的设计和编写就应避免垃圾对象的内存占用和GC的开销。显示调用System.GC()只能建议JVM需要在内存中对垃圾对象进行回收，但不是必须马上回收。一个是并不能解决内存资源耗空的局面，另外也会增加GC的消耗。 如何避免内存泄漏、溢出1)尽早释放无用对象的引用。 好的办法是使用临时变量的时候，让引用变量在退出活动域后自动设置为null，暗示垃圾收集器来收集该对象，防止发生内存泄露。 程序进行字符串处理时，尽量避免使用String，而应使用StringBuffer。 因为每一个String对象都会独立占用内存一块区域，如： 1.String str = “aaa”; 2.String str2 = “bbb”; 3.String str3 = str + str2; 4.// 假如执行此次之后str , str2再不被调用，那么它们就会在内存中等待GC回收； 5.// 假如程序中存在过多的类似情况就会出现内存错误； 尽量少用静态变量。 因为静态变量是全局的，GC不会回收。 避免集中创建对象尤其是大对象，如果可以的话尽量使用流操作。 JVM会突然需要大量内存，这时会触发GC优化系统内存环境； 一个案例如下： 1.// 使用jspsmartUpload作文件上传，运行过程中经常出现java.outofMemoryError的错误， 2.// 检查之后发现问题：组件里的代码 3.m_totalBytes = m_request.getContentLength(); 4.m_binArray = new byte[m_totalBytes]; 5.// totalBytes这个变量得到的数极大，导致该数组分配了很多内存空间，而且该数组不能及时释放。 6.// 解决办法只能换一种更合适的办法，至少是不会引发outofMemoryError的方式解决。 7.// 参考：http://bbs.xml.org.cn/blog/more.asp?name=hongrui&amp;id=3747 尽量运用对象池技术以提高系统性能。 生命周期长的对象拥有生命周期短的对象时容易引发内存泄漏，例如大集合对象拥有大数据量的业务对象的时候，可以考虑分块进行处理，然后解决一块释放一块的策略。 不要在经常调用的方法中创建对象，尤其是忌讳在循环中创建对象。 可以适当的使用hashtable，vector 创建一组对象容器，然后从容器中去取那些对象，而不用每次new之后又丢弃。 优化配置。 a.设置-Xms、-Xmx相等； b.设置NewSize、MaxNewSize相等； c.设置Heap size, PermGen space: 使用过程中遇到的一些问题与疑问问题1：从服务器dump堆内存后文件比较大（3.5G左右），加载文件、查看实例对象都很慢，还提示配置xmx大小。在windows上如何配置xmx大小？ 表明给VisualVM分配的堆内存不够，找到${visualvm}/etc/visualvm.conf （如：C:\Program Files\Java\jdk1.6.0_10\lib\visualvm\etc）这个文件，修改 default_options=”-J-Xms24m -J-Xmx192m“ 为 default_options=”-J-Xms24m -J-Xmx1024m”（ 再重启VisualVM就行了。 对于“堆 dump”来说，在远程监控jvm的时候，VisualVM是没有这个功能的，只有本地监控的时候才有。另外，就算是本地监控，它在dump和得到实例的 速度那是相当的慢的。所以鉴于这几个原因，不建议用VisualVM，而是用jmap加上Mat来分析内存情况。 问题2： 参考资料： 1、http://www.kankanews.com/ICkengine/archives/106440.shtml 2、http://freewind.me/blog/20111023/479.html 3、http://supercharles888.blog.51cto.com/609344/1179790 4、http://zhouanya.blog.51cto.com/4944792/1370017]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK升级]]></title>
    <url>%2Fposts%2Fd721%2F</url>
    <content type="text"><![CDATA[修改pom.xml1234567891011121314151617&lt;properties&gt; &lt;project.build.jdk&gt;1.8&lt;/project.build.jdk&gt;&lt;/properties&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 修改Project Structure 修改idea tomcat jdk]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Idea</tag>
        <tag>Java</tag>
        <tag>JDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java字符串拼接效率比较]]></title>
    <url>%2Fposts%2F3e2c%2F</url>
    <content type="text"><![CDATA[字符串拼接的三种方法 加号 concat方法 StringBuilder（或StringBuffer）的append方法 程序例子 1234567891011121314151617181920212223242526public class Main &#123; public static void main(String[] args) &#123; String str = "a"; long time = System.currentTimeMillis(); for (int i = 0; i &lt; 50000; i++) &#123; str += "c"; &#125; System.out.println("加号所花费的时间："); System.out.println(System.currentTimeMillis()-time); String str2 = "a"; time = System.currentTimeMillis(); for (int i = 0; i &lt; 50000; i++) &#123; str2.concat("c"); &#125; System.out.println("cancat方法所花费的时间："); System.out.println(System.currentTimeMillis()-time); time = System.currentTimeMillis(); StringBuilder stringBuilder = new StringBuilder("a"); for (int i = 0; i &lt; 50000; i++) &#123; stringBuilder.append("c"); &#125; String str3 = stringBuilder.toString(); System.out.println("StringBuilder的append方法："); System.out.println(System.currentTimeMillis()-time); &#125;&#125; 程序输出 ### append方法最快、concat次之、加号最慢“+”方法虽然编译器对字符串的加号做了优化，它会使用StringBuilder的append方法进行追加，而它最终通过toString方法转换成String字符串，上例中“+”拼接的代码即如下： 1str = new StringBuilder(str).append("JTZen9").toString(); 它与纯粹地使用StringBuilder的append方法是不同的： 每趟循环都会创建一个StringBuilder对象 每次执行完毕都会调用toString方法将其转换为字符串 所以，就耗费了更多的时间。 concat方法concat源代码： 12345678910111213141516public String concat(String str) &#123; // 追加的字符串长度为0 int otherLen = str.length(); // 如果追加的字符串长度为0，则返回原字符串本身 if (otherLen == 0) &#123; return this; &#125; // 获取原字符串的字符数组的长度 int len = value.length; // 将原字符串的字符数组放到buf数组中 char buf[] = Arrays.copyOf(value, len + otherLen); // 追加的字符串转化成字符数组，添加到buf中 str.getChars(buf, len); // 产生一个新的字符串 return new String(buf, true); &#125; 整体是一个数组的拷贝，虽然在内存中是处理都是原子性操作，速度非常快，但是，最后的return语句创建一个新String对象，也就是每次concat操作都会创建一个新的String对象，这也是限制concat方法速度的原因。 append方法append源代码： 12345678910111213141516171819202122232425262728public AbstractStringBuilder append(String str) &#123; // 如果是null值，则把null作为字符串处理 if (str == null) return appendNull(); int len = str.length(); // 追加后的字符数组长度是否超过当前值 ensureCapacityInternal(count + len); // 字符串复制到目标数组 str.getChars(0, len, value, count); count += len; return this; &#125; private AbstractStringBuilder appendNull() &#123; int c = count; ensureCapacityInternal(c + 4); final char[] value = this.value; value[c++] = 'n'; value[c++] = 'u'; value[c++] = 'l'; value[c++] = 'l'; count = c; return this; &#125; private void ensureCapacityInternal(int minimumCapacity) &#123; // overflow-conscious code if (minimumCapacity - value.length &gt; 0) expandCapacity(minimumCapacity); // 加长，并作数组拷贝 &#125; 整个append方法都在做字符数组的处理，加长，拷贝等，这些都是基本的数据处理，整个方法内并没有生成对象。只是最后toString返回一个对象而已。 题外12String str = "My name is ";str = str + "JTZen9"; 相当于 str = new StringBuilder(str).append(“JTZen9”).toString(); 也就是说，该str = str + “JTZen9”;语句执行完之后，总共有三个对象。 1String str = "My name is " + "JTZen9"; JVM会直接把str作为一个对象，即 “My name is JTZen9” 使用场景 大多数情况，我们使用“+”，符合编码习惯和我们的阅读 当在频繁进行字符串的运算（如拼接、替换、删除等），或者在系统性能临界的时候，我们可以考虑使用concat或append方法]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Final关键字]]></title>
    <url>%2Fposts%2Fc11c%2F</url>
    <content type="text"><![CDATA[final关键字的基本用法 在Java中，final关键字可以用来修饰类、方法和变量（包括成员变量和局部变量）。下面就从这三个方面来了解一下final关键字的基本用法。 1.修饰类 当用final修饰一个类时，表明这个类不能被继承。也就是说，如果一个类你永远不会让他被继承，就可以用final进行修饰。final类中的成员变量可以根据需要设为final，但是要注意final类中的所有成员方法都会被隐式地指定为final方法。 在使用final修饰类的时候，要注意谨慎选择，除非这个类真的在以后不会用来继承或者出于安全的考虑，尽量不要将类设计为final类。 2.修饰方法 下面这段话摘自《Java编程思想》第四版第143页： “使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升。在最近的Java版本中，不需要使用final方法进行这些优化了。“ 因此，如果只有在想明确禁止 该方法在子类中被覆盖的情况下才将方法设置为final的。 注：类的private方法会隐式地被指定为final方法。 3.修饰变量 修饰变量是final用得最多的地方，也是本文接下来要重点阐述的内容。首先了解一下final变量的基本语法： 对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。 举个例子： 上面的一段代码中，对变量i和obj的重新赋值都报错了。 深入理解final关键字 在了解了final关键字的基本用法之后，这一节我们来看一下final关键字容易混淆的地方。 1.类的final变量和普通变量有什么区别？ 当用final作用于类的成员变量时，成员变量（注意是类的成员变量，局部变量只需要保证在使用之前被初始化赋值即可）必须在定义时或者构造器中进行初始化赋值，而且final变量一旦被初始化赋值之后，就不能再被赋值了。 那么final变量和普通变量到底有何区别呢？下面请看一个例子： 1234567891011public class Test &#123; public static void main(String[] args) &#123; String a = "hello2"; final String b = "hello"; String d = "hello"; String c = b + 2; String e = d + 2; System.out.println((a == c)); System.out.println((a == e)); &#125;&#125; 12truefalse 大家可以先想一下这道题的输出结果。为什么第一个比较结果为true，而第二个比较结果为fasle。这里面就是final变量和普通变量的区别了，当final变量是基本数据类型以及String类型时，如果在编译期间能知道它的确切值，则编译器会把它当做编译期常量使用。也就是说在用到该final变量的地方，相当于直接访问的这个常量，不需要在运行时确定。这种和C语言中的宏替换有点像。因此在上面的一段代码中，由于变量b被final修饰，因此会被当做编译器常量，所以在使用到b的地方会直接将变量b 替换为它的 值。而对于变量d的访问却需要在运行时通过链接来进行。想必其中的区别大家应该明白了，不过要注意，只有在编译期间能确切知道final变量值的情况下，编译器才会进行这样的优化，比如下面的这段代码就不会进行优化： 12345678910111213public class Test &#123; public static void main(String[] args) &#123; String a = "hello2"; final String b = getHello(); String c = b + 2; System.out.println((a == c)); &#125; public static String getHello() &#123; return "hello"; &#125;&#125; 这段代码的输出结果为false。 2.被final修饰的引用变量指向的对象内容可变吗？ 在上面提到被final修饰的引用变量一旦初始化赋值之后就不能再指向其他的对象，那么该引用变量指向的对象的内容可变吗？看下面这个例子： 1234567891011public class Test &#123; public static void main(String[] args) &#123; final MyClass myClass = new MyClass(); System.out.println(++myClass.i); &#125;&#125;class MyClass &#123; public int i = 0;&#125; 这段代码可以顺利编译通过并且有输出结果，输出结果为1。这说明引用变量被final修饰之后，虽然不能再指向其他对象，但是它指向的对象的内容是可变的。 3.final和static 很多时候会容易把static和final关键字混淆，static作用于成员变量用来表示只保存一份副本，而final的作用是用来保证变量不可变。看下面这个例子： 12345678910111213141516public class Test &#123; public static void main(String[] args) &#123; MyClass myClass1 = new MyClass(); MyClass myClass2 = new MyClass(); System.out.println(myClass1.i); System.out.println(myClass1.j); System.out.println(myClass2.i); System.out.println(myClass2.j); &#125;&#125;class MyClass &#123; public final double i = Math.random(); public static double j = Math.random();&#125; 运行这段代码就会发现，每次打印的两个j值都是一样的，而i的值却是不同的。从这里就可以知道final和static变量的区别了。 4.匿名内部类中使用的外部局部变量为什么只能是final变量？ 这个问题请参见上一篇博文中《Java内部类详解》中的解释，在此处不再赘述。 5.关于final参数的问题 关于网上流传的”当你在方法中不需要改变作为参数的对象变量时，明确使用final进行声明，会防止你无意的修改而影响到调用方法外的变量“这句话，我个人理解这样说是不恰当的。 因为无论参数是基本数据类型的变量还是引用类型的变量，使用final声明都不会达到上面所说的效果。 看这个例子就清楚了： 上面这段代码好像让人觉得用final修饰之后，就不能在方法中更改变量i的值了。殊不知，方法changeValue和main方法中的变量i根本就不是一个变量，因为java参数传递采用的是值传递，对于基本类型的变量，相当于直接将变量进行了拷贝。所以即使没有final修饰的情况下，在方法内部改变了变量i的值也不会影响方法外的i。 再看下面这段代码： 123456789101112131415public class Test &#123; public static void main(String[] args) &#123; MyClass myClass = new MyClass(); StringBuffer buffer = new StringBuffer("hello"); myClass.changeValue(buffer); System.out.println(buffer.toString()); &#125;&#125;class MyClass &#123; void changeValue(final StringBuffer buffer) &#123; buffer.append("world"); &#125;&#125; 运行这段代码就会发现输出结果为 helloworld。很显然，用final进行修饰并没有阻止在changeValue中改变buffer指向的对象的内容。有人说假如把final去掉了，万一在changeValue中让buffer指向了其他对象怎么办。有这种想法的朋友可以自己动手写代码试一下这样的结果是什么，如果把final去掉了，然后在changeValue中让buffer指向了其他对象，也不会影响到main方法中的buffer，原因在于java采用的是值传递，对于引用变量，传递的是引用的值，也就是说让实参和形参同时指向了同一个对象，因此让形参重新指向另一个对象对实参并没有任何影响。 所以关于网上流传的final参数的说法，我个人不是很赞同。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工程实践：如何给变量取一个好的名字]]></title>
    <url>%2Fposts%2F49d%2F</url>
    <content type="text"><![CDATA[变量命名风格 变量命名风格通常会根据不同的变量类型来区分，以Java语言为例，根据变量类型不同有两种命名风格： 1）类成员变量、局部变量 类成员变量、局部变量通常采用驼峰命名风格，如下： 1String userName; 2）静态成员变量、枚举值、常量 静态成员变量、枚举值、常量通常采用所有字母大写、多个单词以英文下划线连接，如： 1234567891011121314151617public static final int MAX_YEARS = 25;​// 建议枚举类都以Enum结尾enum ColorEnum &#123; RED(0, "红色"), YELLOW(1, "黄色"), GREEN(2, "绿色"), WHITE(3, "白色"), BLACK(4, "黑色"); private int code; private String name;​ Color(int code, String name) &#123; this.code = code; this.name = name; &#125;&#125; 变量命名最高境界1在函数命名那篇中我们说的函数命名最高境界是见字如面，那么对于变量命名来说，最高境界是什么呢？ 我认为是：自解释，即&quot;代码即注释&quot;。 为什么这么说呢，因为通常来说一个函数是会有函数注释的，即使函数名字取的不好，如果注释写的比较清楚，对于后续维护人员来说也是了解函数具体功能的一种方式。 而变量则不同，在一个工程里面，变量的数量远远大于函数的数量，所以不太可能对于每个变量都去写注释，所以如果一个工程的变量命名很糟糕，那么对于后续维护人员来说将是毁灭性的打击，因为每读到一个变量，可能就需要去猜测变量的含义，我想没有哪个人愿意读到这样的代码，永远记住一点：”代码是写给人看的，不是写给机器看的”。 譬如下面这段代码的命名就非常糟糕： 1234ppn = (cpn &gt; 1) ? (cpn - 1) : cpn;npn = (cpn &lt; tpn) ? (cpn + 1) : tpn;​p = new Page(ppn, cpn, npn, tpn); 1上面这段代码估计只有原作者清楚地知道各个变量的含义是啥了， 如果修改为下面这种写法，可读性会好很多，并且一目了然，很容易知道其大概意图是计算分页信息： 1234prePageNum = (curPageNum &gt; 1) ? (curPageNum - 1) : curPageNum;nextPageNum = (curPageNum &lt; totalPageNum) ? (curPageNum + 1) : totalPageNum;​page = new Page(prePageNum, curPageNum, nextPageNum, totalPageNum); 变量命名最佳实践1）采用名词或者形容词来命名变量 变量一般情况下建议使用名词、名字组合或者形容词，因为变量一般形容的是一种事物或者事物的属性，所以用名词或者名词组合更容易让人理解，而形容词一般用于bool类型的变量。 2）避免使用单字母变量，尽量细化变量含义 在程序中，尽量避免使用单字母变量，唯一可以接受使用单字母变量的场景只有for循环，不过还是不太推荐在for循环中使用单字母变量(用pos、index比for循环的i、j、k要好很多)。 举个例子，比如下面这行代码： 123double calConeVolume(double b, double d) &#123; return Math.PI * b * b * d / 3;&#125; 123咋一看这个函数参数感觉挺清晰，但是一细看，b是什么？d又是什么？如果我要用这个函数，该怎么传参？估计大部人是一脸懵逼状，只能进去看实际的函数实现才知道b是圆锥体半径，d是圆锥体高度；那么怎么优化这段代码命名呢？其实很简单，稍微细化一下变量含义，让变量名自己去表达实际意图： 123double calConeVolume(double radius, double height) &#123; return Math.PI * radius * radius * height / 3;&#125; 3）变量命名前后用词需统一 在同一个工程或者一个场景下，变量命名风格需前后统一，比如total和sum都能表示总计的意思，那么所有需要用到”总计”含义的地方要么全部使用total、要么全部使用sum。 保持前后命名风格统一是保证工程代码良好可读性的关键保证。 4）集合变量用类型或者复数s作为后缀 在java中，有很多集合，比如List、Map、Set等，那么集合变量该怎么命名呢？ 一般可采取两种方式： 使用复数s结尾 1List students = new ArrayList&lt;&gt;(); 用集合类型作为后缀 1List studentList = new ArrayList&lt;&gt;(); 上面两种方式均可，没有比较明显的偏好，根据实际场景决定。第一种方式相对更简洁，第二种在局部作用域里面有多种相关的集合变量时区分度更大，比如： 123456List studentList = new ArrayList&lt;&gt;();Map studentMap = Maps.newHashMap();​for (Student stu : studentList) &#123; studentMap.put(stu.getId, stu);&#125; 我的建议是如果局部作用域只有一种类型的集合，那么推荐使用复数形式；如果局部作用域有多个相关的集合类型，那么推荐用类型结尾。 5）禁止使用is作为bool类型的类成员变量前置 在java中，禁止用is作为bool类型的类成员变量的前缀，因为is作为前缀会导致序列化/反序列出现问题，阿里的java代码规范中也明确提到了这一点，所以在写代码的时候最好还是遵守公认的规范，不然哪天说不定就踩坑了。 6）尽量避免使用缩写进行命名 有些时候，变量名可能有点长，不利于代码可读性，因此很多时候在写代码的时候喜欢用缩写来命名，但这个不是一个好的习惯，除非使用的缩写是大家都会使用的约定俗称的缩写。 比如下面这个命名： 1int averageStudentAge; =&gt; int avgStudentAge; 因为avg大家都知道是average的缩写，所以这么写问题不大，不会引起歧义； 但是下面这种缩写命名： 123restmpcnt 就不是好的缩写命名，因为不同的人阅读可能会有不同的理解： 123res =&gt; response、resource、resulttmp =&gt; temporary、templatecnt =&gt; count、content、context 附上一些约定俗称的缩写： 全称 缩写 identification id average avg maximum max minimum min buffer buf error err message msg image img length len library lib password pwd position pos data transfer object dto view object vo 7）抛弃掉flag变量 国内一些早期的教材上，到处充斥着各种flag风格的变量，这种命名方式对于大型工程简直就是噩梦，比如： 1234int flag = getDoctorFlag(doctorId);if (flag == 1) &#123; //....&#125; 1看到这段代码，读者会有疑问flag变量的含义是什么？flag值为1的时候又代表什么含义？是医生的值班/在岗状态、还是医生的身体状态？估计读者的内心是崩溃的。 如果优化成下面这种形式： 1234DutyStatus doctorDutyStatus = getDoctorDutyStatus(doctorId);if (doctorDutyStatus == DutyStatus.ONLINE) &#123; // ...&#125;]]></content>
      <categories>
        <category>Other</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[工程实践：给函数取一个好的名字]]></title>
    <url>%2Fposts%2F93bc%2F</url>
    <content type="text"><![CDATA[常见函数命名风格 目前来说，最常见的函数命名主要有两种风格：驼峰命名和帕斯卡命名。 驼峰命名：多个单词组成一个名称时，第一个单词全部小写，后面单词首字母大写；如： 1public void setUserName(String userName); 帕斯卡命名：多个单词组成一个名称时，每个单词的首字母大写； 1public void SetUserName(String userName); 两种命名风格都是ok的，但要保证一点，对于一个团队或者一个项目，需要根据语言本身的推荐命名方式做好约定。比如java一般都采取驼峰命名，C#采取帕斯卡命名。 函数命名最高境界 我们通常说：天下武功，唯快不破。那么对于函数命名来说最高境界是什么呢？我认为是：见字如面，顾名思义，就是看到函数的名字就知道这个函数具体做了哪些事情。 比如上面的函数： 1public void setUserName(String userName); 但是下面这个函数命名就不是一个好的命名： 1public String addCharacter(String originString, char ch);` 这个函数，一咋看，还不错，从函数字面意思看是给某个字符串添加一个字符。但是到底是在原有字符串首部添加，还是在原有字符串末尾追加呢？亦或是在某个固定位置插入呢？从函数名字完全看不出来这个函数的真正意图，只能继续往下读这个函数的具体实现才知道。 而下面这几个名字就比上面要好得多： 12public String appendCharacter(String originString, char ch); // 追加到末尾public String insertCharacter(String originString, char ch, int insertPosition); // 插入指定位置 函数命名最佳实践1）要领1：动词选取要精准 通常来说，动词决定了一个函数要采取什么”动作”。动词取的好，一个函数名字已经成功了80%。 常用动词表： 类别 单词 添加/插入/创建/初始化/加载 add、append、insert、create、initialize、load 删除/销毁 delete、remove、destroy、drop 打开/开始/启动 open、start 关闭/停止 close、stop 获取/读取/查找/查询 get、fetch、acquire、read、search、find、query 设置/重置/放入/写入/释放/刷新 set、reset、put、write、release、refresh 发送/推送 send、push 接收/拉取 receive、pull 提交/撤销/取消 submit、cancel 收集/采集/选取/选择 collect、pick、select 提取/解析 sub、extract、parse 编码/解码 encode、decode 填充/打包/压缩 fill、pack、compress 清空/拆包/解压 flush、clear、unpack、decompress 增加/减少 increase、decrease、reduce 分隔/拼接 split、join、concat 过滤/校验/检测 filter、valid、check 动词决定了函数的具体动作，而名词决定了函数具体的操作对象，对于名词，尽量使用领域词汇，不要使用生僻或者大家很少使用的词语。 2）要领2：名词使用领域词汇 举个例子：集合的容量通常用capacity、集合实际元素个数用size、字符串长度用length，这种就遵循大家的使用习惯，不要用size去形如字符串的长度。 再比如，假如使用到建造者模式，那么通常会用build作为函数名字，这个时候就不要另辟蹊径，用create来作为函数名字，使用大家约定俗成的命名习惯更容易让你的代码被别人读懂。 常用名词表： 类别 单词 容量/大小/长度 capacity、size、length 实例/上下文 instance、context 配置 config、settings 头部/前面/前一个/第一个 header、front、previous、first 尾部/后面/后一个/最后一个 tail、back、next、last 区间/区域/某一部分/范围/规模 range、interval、region、area、section、scope、scale 缓存/缓冲/会话 cache、buffer、session 本地/局部/全局 local、global 成员/元素 member、element 菜单/列表 menu、list 源/目标 source、destination、target 3）要领3：函数取名最忌讳”名不副实” 函数取名最忌讳的是”名不副实”，举个例子，假如有个Cache类，里面有个函数判断key是否过期： 12345678910111213public boolean isExpired(String key) &#123; // 当前时间戳 long curTimestamp = DateUtils.nowUnixTime(); // 获取key的存入时间戳 long storeTimestamp = getStoreTimestamp(key); if (curTimestamp - storeTimestamp &gt; MAX_EXPIRE_SECONDS) &#123; // 注意这个地方的delete是个隐藏逻辑 delete(key); return true; &#125; return false; &#125; 上面这个函数从函数字面意思看是判断key是否过期，但是！！它居然在函数里面隐藏了一段特殊逻辑：如果过期则删除掉key。这个就是典型的”名不副实”，这个是最忌讳的，会给后续的开发人员留下”巨坑”。 有两种方式去优化这段代码： 方式一：将隐藏逻辑去掉 1234567891011public boolean isExpired(String key) &#123; // 当前时间戳 long curTimestamp = DateUtils.nowUnixTime(); // 获取key的存入时间戳 long storeTimestamp = getStoreTimestamp(key); if (curTimestamp - storeTimestamp &gt; MAX_EXPIRE_SECONDS) &#123; return true; &#125; return false; &#125; 方式二：改变函数名字 1234567891011public int deleteIfExpired(String key) &#123; // 当前时间戳 long curTimestamp = DateUtils.nowUnixTime(); // 获取key的存入时间戳 long storeTimestamp = getStoreTimestamp(key); if (curTimestamp - storeTimestamp &gt; MAX_EXPIRE_SECONDS) &#123; return delete(key); &#125; return 0; &#125; 4）要领4：多查询条件的函数名字谨慎使用介词by 我们平时在写查询接口时，假如有多个查询参数怎么办？每个通过by一起连接依赖？No！这绝对不是明智的方式。假如一开始产品的需求是通过学生姓名查询学生信息，写出来的可能是这样的函数： 1public List getByName(String name); 然后突然又有一天产品提出了新的需求，希望同时可以通过姓名和电话号码来查询学生信息，那么函数可能变成这样了： 1public List getByNameAndMobile(String name, String mobile); 接着，没过多久，产品又希望根据学生年龄来查询学生信息，那么函数可能变成这样了： 1public List getByNameAndMobileAndAge(String name, String mobile, int age); 如果这样来给函数命名，那么你的噩梦大门即将打开。 通常比较好的做法是： 如果是通过主键id来查询，那么可以通过by来连接查询信息，比如： 1public Student getByStudentId(long studentId); 如果是通过其他属性来查询，并且未来会存在多个组合查询的可能性，建议进行封装，比如： 1public List getStudents(StudentSearchParam searchParam); 最后，建议大家平时在写代码过程中，不要怕在函数命名上耗费时间，一个好的函数命名在后期会大大减少你代码重构的成本，争取对函数命名做到”见字如面”~]]></content>
      <categories>
        <category>Other</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IDEA切换Git地址]]></title>
    <url>%2Fposts%2F5fd7%2F</url>
    <content type="text"><![CDATA[VCS–&gt;Git–&gt;Remotes]]></content>
      <categories>
        <category>Idea</category>
      </categories>
      <tags>
        <tag>Idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Idea常用设置]]></title>
    <url>%2Fposts%2Fc655%2F</url>
    <content type="text"><![CDATA[隐藏文件![]![] 显示文件修改 auto import Favorites(Command + 2) Bookmarks() Recent Files(Command + E) Find Action(Shift + Command + A) Search Everywhere(Shift Shift) Find in Path(Command + Shift + F) Usages(Alt + F7)]]></content>
      <categories>
        <category>Idea</category>
      </categories>
      <tags>
        <tag>Idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA常用插件]]></title>
    <url>%2Fposts%2F4d6d%2F</url>
    <content type="text"><![CDATA[插件安装设置–&gt;Plugins–&gt;Browse repositories–&gt;搜索插件–&gt;install–&gt;重启idea 常用插件Maven HelperJRebellombok.ignoreAlibaba Cloud ToolkitMybatisCodeHelperProRestfulToolkitSonarLintstackoverflow]]></content>
      <categories>
        <category>Idea</category>
      </categories>
      <tags>
        <tag>Idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git设置默认分支]]></title>
    <url>%2Fposts%2Fe5a8%2F</url>
    <content type="text"><![CDATA[要把push和pull的默认分支设置为dev，那么： 1git branch --set-upstream-to=origin/dev dev]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git基本命令]]></title>
    <url>%2Fposts%2F93c0%2F</url>
    <content type="text"><![CDATA[打算使用 Git 来对现有的项目进行管理，你只需要进入该项目目录并输入 1$ git init 实现对指定文件的跟踪 12$ git add *.c$ git add LICENSE 提交 1$ git commit -m 'initial project version' 克隆仓库 1$ git clone https://github.com/libgit2/libgit2 mylibgit 检查当前文件状态 1$ git status 忽略文件 1$ touch .gitignore 格式规范 所有空行或者以 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式可以以（/）开头防止递归。 匹配模式可以以（/）结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。 查看差异 只显示尚未暂存的改动 1$ git diff 查看已暂存的将要添加到下次提交里的内容 1$ git diff --cached 查看已经暂存起来的变化 1$ git diff --cached 提交更新 -a跳过使用暂存区域 -m提交信息 1$ git commit -m "Story 182: Fix benchmarks for speed" 移除文件 1$ git rm 查看提交历史 -p显示每次提交的内容差异 -2仅显示最近两次提交 –stat每次提交的简略的统计信息 –pretty指定使用不同于默认格式的方式展示提交历史 12$ git log$ git log --pretty=oneline]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git分支管理最佳实践]]></title>
    <url>%2Fposts%2Fcf82%2F</url>
    <content type="text"><![CDATA[Git 是目前最流行的源代码管理工具。大量的软件项目由 GitHub、Bitbucket 和 GitLab 这样的云服务平台或是私有的 Git 仓库来管理。在使用 Git 时通常会遇到的一个问题是采用何种分支管理实践，即如何管理仓库中作用不同的各类分支。和软件开发中的其他实践一样，Git 分支管理并没有普遍适用的最佳做法，而只有对每个团队和项目而言最适合的做法。简单来说，在项目开发中使用多个分支会带来额外的管理和维护开销，但是多个分支对于项目的团队合作、新功能开发和发布管理都是有一定好处的。不同的团队可以根据团队人员组成和意愿、项目的发布周期等因素选择最适合的策略，找到最适合团队的管理方式。本文将介绍三种常见的 Git 分支管理方式。 单主干单主干的分支实践（Trunk-based development，TBD）在 SVN 中比较流行。Google和Facebook都使用这种方式。trunk 是 SVN 中主干分支的名称，对应到 Git 中则是 master 分支。TBD 的特点是所有团队成员都在单个主干分支上进行开发。当需要发布时，先考虑使用标签（tag），即 tag 某个 commit 来作为发布的版本。如果仅靠 tag 不能满足要求，则从主干分支创建发布分支。bug 修复在主干分支中进行，再 cherry-pick 到发布分支。图 1 是 TBD 中分支流程的示意图。 图 1. TBD 中的分支流程的示意图 由于所有开发人员都在同一个分支上工作，团队需要合理的分工和充分的沟通来保证不同开发人员的代码尽可能少的发生冲突。持续集成和自动化测试是必要的，用来及时发现主干分支中的 bug。因为主干分支是所有开发人员公用的，一个开发人员引入的 bug 可能对其他很多人造成影响。不过好处是由于分支所带来的额外开销非常小。开发人员不需要频繁在不同的分支之间切换。 GitHub flowGitHub flow是 GitHub 所使用的一种简单的流程。该流程只使用两类分支，并依托于 GitHub 的 pull request 功能。在 GitHub flow 中，master 分支中包含稳定的代码。该分支已经或即将被部署到生产环境。master 分支的作用是提供一个稳定可靠的代码基础。任何开发人员都不允许把未测试或未审查的代码直接提交到 master 分支。 对代码的任何修改，包括 bug 修复、hotfix、新功能开发等都在单独的分支中进行。不管是一行代码的小改动，还是需要几个星期开发的新功能，都采用同样的方式来管理。当需要进行修改时，从 master 分支创建一个新的分支。新分支的名称应该简单清晰地描述该分支的作用。所有相关的代码修改都在新分支中进行。开发人员可以自由地提交代码和 push 到远程仓库。 当新分支中的代码全部完成之后，通过 GitHub 提交一个新的 pull request。团队中的其他人员会对代码进行审查，提出相关的修改意见。由持续集成服务器（如 Jenkins）对新分支进行自动化测试。当代码通过自动化测试和代码审查之后，该分支的代码被合并到 master 分支。再从 master 分支部署到生产环境。图 2 是 GitHub flow 分支流程的示意图。 图 2. Github flow 中的分支流程的示意图 GitHub flow 的好处在于非常简单实用。开发人员需要注意的事项非常少，很容易形成习惯。当需要进行任何修改时，总是从 master 分支创建新分支。完成之后通过 pull request 和相关的代码审查来合并回 master 分支。GitHub flow 要求项目有完善的自动化测试、持续集成和部署等相关的基础设施。每个新分支都需要测试和部署，如果这些不能自动化进行，会增加开发人员的工作量，导致无法有效地实施该流程。这种分支实践也要求团队有代码审查的相应流程。 git-flowgit-flow应该是目前流传最广的 Git 分支管理实践。git-flow 围绕的核心概念是版本发布（release）。因此 git-flow 适用于有较长版本发布周期的项目。虽然目前推崇的做法是持续集成和随时发布。有的项目甚至可以一天发布很多次。随时发布对于 SaaS 服务类的项目来说是很适合的。不过仍然有很大数量的项目的发布周期是几个星期甚至几个月。较长的发布周期可能是由于非技术相关的因素造成的，比如人员限制、管理层决策和市场营销策略等。 git-flow 流程中包含 5 类分支，分别是 master、develop、新功能分支（feature）、发布分支（release）和 hotfix。这些分支的作用和生命周期各不相同。master 分支中包含的是可以部署到生产环境中的代码，这一点和 GitHub flow 是相同的。develop 分支中包含的是下个版本需要发布的内容。从某种意义上来说，develop 是一个进行代码集成的分支。当 develop 分支集成了足够的新功能和 bug 修复代码之后，通过一个发布流程来完成新版本的发布。发布完成之后，develop 分支的代码会被合并到 master 分支中。 其余三类分支的描述如表 1所示。这三类分支只在需要时从 develop 或 master 分支创建。在完成之后合并到 develop 或 master 分支。合并完成之后该分支被删除。这几类分支的名称应该遵循一定的命名规范，以方便开发人员识别。 表 1. git-flow 分支类型 分支类型 命名规范 创建自 合并到 说明 feature feature/* develop develop 新功能 release release/* develop develop 和 master 一次新版本的发布 hotfix hotfix/* master develop 和 master 生产环境中发现的紧急 bug 的修复 对于开发过程中的不同任务，需要在对应的分支上进行工作并正确地进行合并。每个任务开始前需要按照指定的步骤完成分支的创建。例如当需要开发一个新的功能时，基本的流程如下： 从 develop 分支创建一个新的 feature 分支，如 feature/my-awesome-feature。 在该 feature 分支上进行开发，提交代码，push 到远端仓库。 当代码完成之后，合并到 develop 分支并删除当前 feature 分支。 在进行版本发布和 hotfix 时也有类似的流程。当需要发布新版本时，采用的是如下的流程： 从 develop 分支创建一个新的 release 分支，如 release/1.4。 把 release 分支部署到持续集成服务器上进行测试。测试包括自动化集成测试和手动的用户接受测试。 对于测试中发现的问题，直接在 release 分支上提交修改。完成修改之后再次部署和测试。 当 release 分支中的代码通过测试之后，把 release 分支合并到 develop 和 master 分支，并在 master 分支上添加相应的 tag。 因为 git-flow 相关的流程比较繁琐和难以记忆，在实践中一般使用辅助脚本来完成相关的工作。比如同样的开发新功能的任务，可以使用 git flow feature start my-awesome-feature 来完成新分支的创建，使用 git flow feature finish my-awesome-feature 来结束 feature 分支。辅助脚本会完成正确的分支创建、切换和合并等工作。 Maven JGit-Flow对于使用 Apache Maven 的项目来说，Atlassian 的JGit-Flow是一个更好的 git-flow 实现。JGit-Flow 是一个基于JGit的纯 Java 实现的 git-flow，并不需要安装额外的脚本，只需要作为 Maven 的插件添加到 Maven 项目中即可。JGit-Flow 同时可以替代Maven release 插件来进行发布管理。JGit-Flow 会负责正确的设置不同分支中的 Maven 项目的 POM 文件中的版本，这对于 Maven 项目的构建和发布是很重要的。 在 Maven 项目的 pom.xml 文件中添加代码清单1中的插件声明就可以使用 JGit-Flow。中包含的是 JGit-Flow 不同任务的配置。 清单 1. JGit-Flow 的 Maven 设置1234567891011121314external.atlassian.jgitflowjgitflow-maven-plugin1.0-m5.1 release- RC true true true JGit-Flow 提供了很多配置选项，可以在 POM 文件中声明。这些配置项可以对不同的任务生效。常用的配置项如表2所示。 表 2. JGit-Flow 的配置项 名称 描述 适用任务 flowInitContext 配置不同类型的分支的名称 全局 allowSnapshots 是否允许存在 SNAPSHOT 类型的依赖 allowUntracked 是否允许本地 Git 中存在未提交的内容 scmCommentPrefix 和 scmCommentSuffix JGit-Flow 会进行代码合并工作。通过这两个配置项来设置 JGit-Flow 进行代码提交时的消息的前缀和后缀 全局 username 和 password 进行 Git 认证时的用户名和密码，适用于 HTTPS 仓库 全局 releaseBranchVersionSuffix release 分支中项目的 POM 文件版本号的后缀 developmentVersion 下一个开发版本的版本号 releaseVersion 发布版本的版本号 pushReleases 是否把 release 分支 push 到远程仓库 goals 当部署发布版本到 Maven 仓库时执行的目标 release-finish，hotfix-finish keepBranch 当发布完成之后是否保留相应的分支 release-finish，hotfix-finish noDeploy 是否启用部署到 Maven 仓库的功能 release-finish，hotfix-finish noReleaseBuild 在完成发布时是否禁用 Maven 构建 release-finish noReleaseMerge 在完成发布时是否把 release 分支合并回 develop 和 master 分支 release-finish noTag 在完成发布时是否添加标签 release-finish，hotfix-finish squash 在进行分支合并时，是否把多个 commit 合并成一个 release-finish，hotfix-finish featureName 新特性分支的名称 feature-start，feature-finish，feature-deploy pushFeatures 是否把特性分支 push 到远程仓库 feature-start，feature-finish noFeatureBuild 在完成特性时是否禁用 Maven 构建 feature-finish noFeatureMerge 在完成特性时是否把特性分支合并回 develop feature-finish pushHotfixes 在完成 hotfix 时是否把分支合并回 master hotfix-finish noHotfixBuild 在完成 hotfix 时是否禁用 Maven 构建 hotfix-finish 其余的配置项可以参考插件不同任务的文档。 在启用了 JGit-Flow 之后，可以通过 mvn 运行 jgitflow:feature-start 和 jgitflow:feature-finish 来开始和结束新特性的开发。与版本发布和 hotfix 相关的命令分别是 jgitflow:release-start 和 jgitflow:release-finish 以及 jgitflow:hotfix-start 和 jgitflow:hotfix-finish。在运行命令之后，JGit-Flow 会完成相关的 Git 分支创建、合并、删除、添加 tag 等操作，不需要开发人员手动完成。 每个分支的 pom.xml 中的版本号的格式并不相同。如 master 分支的版本号是标准的发布版本号，如 1.2.3；develop 分支中则是 SNAPSHOT 版本，比 master 分支的版本号要高，如 1.2.4-SNAPSHOT；release 分支可以通过&lt;releaseBranchVersionSuffix&gt;配置来指定版本号的后缀，如 1.2.3-RC-SNAPSHOT；feature 分支可以通过&lt;enableFeatureVersions&gt;配置来把 feature 分支名称作为后缀添加到版本号中，如 1.2.3-my-awesome-feature-SNAPSHOT；hotfix 分支的版本号基于 master 分支的版本号，如 1.2.3.1 是对于 1.2.3 版本的第一个 hotfix。当使用 jgitflow:release-finish 完成一个 release 分支时，develop 分支的版本号会被自动更新成下一个小版本，如从 1.2.3 到 1.2.4。当需要手动修改版本号时，可以使用Versions 插件，如 mvn versions:set -DnewVersion=2.0.0-SNAPSHOT。 持续集成 由于 JGit-Flow 是纯 Java 的 Maven 插件实现，可以很容易的与常用的持续集成服务器进行集成。不过在与 Atlassian 的 Bamboo 集成时，有几个细节需要注意。首先是 Bamboo 在进行构建的时候，使用的是一个虚拟的 Git 仓库，其仓库地址是一个不存在的文件系统路径。因此需要在 JGit-Flow 的配置中手动设置 Git 仓库的地址，保证 Git 操作可以正确执行，如代码清单2所示。 清单 2. 手动设置 JGit-Flow 的 Git 仓库地址123[Git url]true 另外在开始新的 release 之前，需要确保前一个发布分支已经被删除。JGit-Flow 在默认情况下会自动在发布完成之后，删除对应的 Git 分支。但是可能本地仓库中还保留有之前的发布分支，这会导致新的 release-start 任务执行失败。一种解决方式是每次都重新 checkout 新的仓库，这样可以保证不会出现已经在远程被删除的分支。不过可能会增加构建的时间。另外一种解决方式是通过 Git 命令来删除本地分支，如代码清单3所示。 清单 3. 删除 Git 本地分支12345678$&#123;bamboo.capability.system.git.executable&#125; fetch --prune --verbose$&#123;bamboo.capability.system.git.executable&#125; branch -vv | awk '/: gone]/&#123;print $1&#125;' | xargs $&#123;bamboo.capability.system.git.executable&#125; branch -d 2&gt; /dev/nullecho 'stale branches deleted'Git 分支合并冲突处理 当把发布分支合并到 develop 时，可能会出现冲突。因为在发布分支中有与 bug fix 相关的改动，在 develop 分支中有可能修改相同的文件。当有冲突时，直接运行 JGit-Flow 的 release-finish 任务会出错。这个时候需要开发人员手动把发布分支合并到 develop 分支，并解决相应的冲突。然后再次运行 release-finish 任务即可。 选择合适的实践每个开发团队都应该根据团队自身和项目的特点来选择最适合的分支实践。首先是项目的版本发布周期。如果发布周期较长，则 git-flow 是最好的选择。git-flow 可以很好地解决新功能开发、版本发布、生产系统维护等问题；如果发布周期较短，则 TBD 和 GitHub flow 都是不错的选择。GitHub flow 的特色在于集成了 pull request 和代码审查。如果项目已经使用 GitHub，则 GitHub flow 是最佳的选择。GitHub flow 和 TBD 对持续集成和自动化测试等基础设施有比较高的要求。如果相关的基础设施不完善，则不建议使用。 小结Git 作为目前最流行的源代码管理工具，已经被很多开发人员所熟悉和使用。在基于 Git 的团队开发中，Git 分支的作用非常重要，可以让团队的不同成员同时在多个相对独立的特性上工作。本文对目前流行的 3 种 Git 分支管理实践做了介绍，并着重介绍了 git-flow 以及与之相关的 Maven JGit-Flow 插件。 相关主题 了解Google和Facebook的单主干实践。 了解GitHub flow的更多内容。 了解git-flow的更多内容。 了解 MavenJGit-Flow插件的更多内容。 developerWorks Java 技术专区：这里有数百篇关于 Java 编程各个方面的文章。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[选择一个开源软件协议]]></title>
    <url>%2Fposts%2F5e27%2F</url>
    <content type="text"><![CDATA[选择一个开源软件协议http://choosealicense.online/]]></content>
      <categories>
        <category>Travel</category>
      </categories>
      <tags>
        <tag>Travel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git把本地代码推送到远程仓库]]></title>
    <url>%2Fposts%2F3252%2F</url>
    <content type="text"><![CDATA[1234567891011git init // 初始化版本库git add . // 添加文件到版本库（只是添加到缓存区），.代表添加文件夹下所有文件 git commit -m "first commit" // 把添加的文件提交到版本库，并填写提交备注git remote add origin $你的远程库地址$ // 把本地库与远程库关联git push -u origin master // 第一次推送时git push origin master // 第一次推送后，直接使用该命令即可推送修改]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo OPS安装]]></title>
    <url>%2Fposts%2F80fa%2F</url>
    <content type="text"><![CDATA[1git clone https://github.com/apache/incubator-dubbo-ops.git 配置1234#配置文件为：dubbo-admin-server/src/main/resources/application.properties#主要的配置有：dubbo.registry.address=zookeeper://127.0.0.1:2181 打包1mvn clean package 启动1mvn --projects dubbo-admin-server spring-boot:run OR 12cd dubbo-admin-distribution/target; java -jar dubbo-admin-0.1.jar 访问1http://127.0.0.1:8080]]></content>
      <categories>
        <category>Dubbo</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库范式]]></title>
    <url>%2Fposts%2F77e3%2F</url>
    <content type="text"><![CDATA[“范式（NF）”是符合某一种级别的关系模式的集合，表示一个关系内部各属性之间的联系的合理化程度”。 总结：1NF： 字段是最小的的单元不可再分 2NF：满足1NF,表中的字段必须完全依赖于全部主键而非部分主键 (一般我们都会做到) 3NF：满足2NF,非主键外的所有字段必须互不依赖 4NF：满足3NF,消除表中的多值依赖 第一范式（1NF）符合1NF的关系中的每个属性都不可再分。1NF是所有关系型数据库的最基本要求。(一范式就是属性不可分割) 第二范式（2NF）2NF在1NF的基础之上，消除了非主属性对于码的部分函数依赖。(二范式就是要有主键) 函数依赖 ：若在一张表中，在属性（或属性组）X的值确定的情况下，必定能确定属性Y的值，那么就可以说Y函数依赖于X，写作 X → Y 完全函数依赖：在一张表中，若 X → Y，且对于 X 的任何一个真子集（假如属性组 X 包含超过一个属性的话），X ‘ → Y 不成立，那么我们称 Y 对于 X完全函数依赖，记作 $$X{F \over }&gt;Y$$ 部分函数依赖：假如 Y 函数依赖于 X，但同时 Y 并不完全函数依赖于 X，那么我们就称 Y 部分函数依赖于 X，记作 $$X{P\over}&gt;Y$$ 传递函数依赖：假如 Z 函数依赖于 Y，且 Y 函数依赖于 X（『Y 不包含于 X，且 X 不函数依赖于 Y』这个前提），那么我们就称 Z 传递函数依赖于 X ，记作 $$X{T\over}&gt;Z$$ 码：设 K 为某表中的一个属性或属性组，若除 K 之外的所有属性都完全函数依赖于 K（这个“完全”不要漏了），那么我们称 K 为候选码，简称为码 非主属性： 第三范式（3NF）3NF在2NF的基础之上，消除了非主属性对于码的传递函数依赖。(三范式就是要消除传递依赖) 数据库范式那些事]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iproute2 对决 net-tools]]></title>
    <url>%2Fposts%2Fb82b%2F</url>
    <content type="text"><![CDATA[如今很多系统管理员依然通过组合使用诸如ifconfig、route、arp和netstat等命令行工具（统称为net-tools）来配置网络功能，解决网络故障。net-tools起源于BSD的TCP/IP工具箱，后来成为老版本Linux内核中配置网络功能的工具。但自2001年起，Linux社区已经对其停止维护。同时，一些Linux发行版比如Arch Linux和CentOS/RHEL 7则已经完全抛弃了net-tools，只支持iproute2。 显示所有已连接的网络接口使用net-tools： 1$ ifconfig -a 使用iproute2： 1$ ip link show 激活或停用网络接口使用这些命令来激活或停用某个指定的网络接口。 使用net-tools： 12$ sudo ifconfig eth1 up$ sudo ifconfig eth1 down 使用iproute2： 12$ sudo ip link set down eth1$ sudo ip link set up eth1 为网络接口分配IPv4地址使用这些命令配置网络接口的IPv4地址。 使用net-tools： 1$ sudo ifconfig eth1 10.0.0.1/24 使用iproute2： 1$ sudo ip addr add 10.0.0.1/24 dev eth1 值得注意的是，可以使用iproute2给同一个接口分配多个IP地址，ifconfig则无法这么做。使用ifconfig的变通方案是使用IP别名。 123$ sudo ip addr add 10.0.0.1/24 broadcast 10.0.0.255 dev eth1$ sudo ip addr add 10.0.0.2/24 broadcast 10.0.0.255 dev eth1$ sudo ip addr add 10.0.0.3/24 broadcast 10.0.0.255 dev eth1 移除网络接口的IPv4地址就IP地址的移除而言，除了给接口分配全0地址外，net-tools没有提供任何合适的方法来移除网络接口的IPv4地址。相反，iproute2则能很好地完全。 使用net-tools： 1$ sudo ifconfig eth1 0 使用iproute2： 1$ sudo ip addr del 10.0.0.1/24 dev eth1 显示网络接口的IPv4地址按照如下操作可查看某个指定网络接口的IPv4地址。 使用net-tools： 1$ ifconfig eth1 使用iproute2： 1$ ip addr show dev eth1 同样，如果接口分配了多个IP地址，iproute2会显示出所有地址，而net-tools只能显示一个IP地址。 为网络接口分配IPv6地址使用这些命令为网络接口添加IPv6地址。net-tools和iproute2都允许用户为一个接口添加多个IPv6地址。 使用net-tools： 12$ sudo ifconfig eth1 inet6 add 2002:0db5:0:f102::1/64$ sudo ifconfig eth1 inet6 add 2003:0db5:0:f102::1/64 使用iproute2： 12$ sudo ip -6 addr add 2002:0db5:0:f102::1/64 dev eth1$ sudo ip -6 addr add 2003:0db5:0:f102::1/64 dev eth1 显示网络接口的IPv6地址按照如下操作可显示某个指定网络接口的IPv6地址。net-tools和iproute2都可以显示出所有已分配的IPv6地址。 使用net-tools： 1$ ifconfig eth1 使用iproute2： 1$ ip -6 addr show dev eth1 移除网络设备的IPv6地址使用这些命令可移除接口中不必要的IPv6地址。 使用net-tools： 1$ sudo ifconfig eth1 inet6 del 2002:0db5:0:f102::1/64 使用iproute2： 1$ sudo ip -6 addr del 2002:0db5:0:f102::1/64 dev eth1 改变网络接口的MAC地址使用下面的命令可篡改网络接口的MAC地址，请注意在更改MAC地址前，需要停用接口。 使用net-tools： 1$ sudo ifconfig eth1 hw ether 08:00:27:75:2a:66 使用iproute2： 1$ sudo ip link set dev eth1 address 08:00:27:75:2a:67 查看IP路由表net-tools中有两个选择来显示内核的IP路由表：route和netstat。在iproute2中，使用命令ip route。 使用net-tools： 1$ route -n 1$ netstat -rn 使用iproute2： 1$ ip route show 添加和修改默认路由这里的命令用来添加或修改内核IP路由表中的默认路由规则。请注意在net-tools中可通过添加新的默认路由、删除旧的默认路由来实现修改默认路由。在iproute2使用ip route命令来代替。 使用net-tools： 12$ sudo route add default gw 192.168.1.2 eth0$ sudo route del default gw 192.168.1.1 eth0 使用iproute2: 12$ sudo ip route add default via 192.168.1.2 dev eth0$ sudo ip route replace default via 192.168.1.2 dev eth0 添加和移除静态路由使用下面命令添加或移除一个静态路由。 使用net-tools： 12$ sudo route add -net 172.16.32.0/24 gw 192.168.1.1 dev eth0$ sudo route del -net 172.16.32.0/24 使用iproute2： 12$ sudo ip route add 172.16.32.0/24 via 192.168.1.1 dev eth0$ sudo ip route del 172.16.32.0/24 查看套接字统计信息这里的命令用来查看套接字统计信息（比如活跃或监听状态的TCP/UDP套接字）。 使用net-tools： 12$ netstat$ netstat -l 使用iproute2： 12$ ss$ ss -l 查看ARP表使用这些命令显示内核的ARP表。 使用net-tools: 1$ arp -an 使用iproute2: 1$ ip neigh 添加或删除静态ARP项按照如下操作在本地ARP表中添加或删除一个静态ARP项。 使用net-tools： 12$ sudo arp -s 192.168.1.100 00:0c:29:c0:5a:ef$ sudo arp -d 192.168.1.100 使用iproute2： 12$ sudo ip neigh add 192.168.1.100 lladdr 00:0c:29:c0:5a:ef dev eth0$ sudo ip neigh del 192.168.1.100 dev eth0 添加、删除或查看多播地址使用下面的命令配置或查看网络接口上的多播地址。 使用net-tools: 1234$ sudo ipmaddr add 33:44:00:00:00:01 dev eth0$ sudo ipmaddr del 33:44:00:00:00:01 dev eth0$ ipmaddr show dev eth0$ netstat -g 使用iproute2： 123$ sudo ip maddr add 33:44:00:00:00:01 dev eth0$ sudo ip maddr del 33:44:00:00:00:01 dev eth0$ ip maddr list dev eth0]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP状态码]]></title>
    <url>%2Fposts%2F6440%2F</url>
    <content type="text"><![CDATA[HTTP状态码由三个十进制数字组成，第一个十进制数字定义了状态码的类型，后两个数字没有分类的作用。 HTTP状态码分类 分类 分类描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 HTTP状态码列表 状态码 状态码英文名称 中文描述 100 Continue 继续。客户端应继续其请求 101 Switching Protocols 切换协议。服务器根据客户端的请求切换协议。只能切换到更高级的协议，例如，切换到HTTP的新版本协议 200 OK 请求成功。一般用于GET与POST请求 201 Created 已创建。成功请求并创建了新的资源 202 Accepted 已接受。已经接受请求，但未处理完成 203 Non-Authoritative Information 非授权信息。请求成功。但返回的meta信息不在原始的服务器，而是一个副本 204 No Content 无内容。服务器成功处理，但未返回内容。在未更新网页的情况下，可确保浏览器继续显示当前文档 205 Reset Content 重置内容。服务器处理成功，用户终端（例如：浏览器）应重置文档视图。可通过此返回码清除浏览器的表单域 206 Partial Content 部分内容。服务器成功处理了部分GET请求 300 Multiple Choices 多种选择。请求的资源可包括多个位置，相应可返回一个资源特征与地址的列表用于用户终端（例如：浏览器）选择 301 Moved Permanently 永久移动。请求的资源已被永久的移动到新URI，返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替 302 Found 临时移动。与301类似。但资源只是临时被移动。客户端应继续使用原有URI 303 See Other 查看其它地址。与301类似。使用GET和POST请求查看 304 Not Modified 未修改。所请求的资源未修改，服务器返回此状态码时，不会返回任何资源。客户端通常会缓存访问过的资源，通过提供一个头信息指出客户端希望只返回在指定日期之后修改的资源 305 Use Proxy 使用代理。所请求的资源必须通过代理访问 306 Unused 已经被废弃的HTTP状态码 307 Temporary Redirect 临时重定向。与302类似。使用GET请求重定向 400 Bad Request 客户端请求的语法错误，服务器无法理解 401 Unauthorized 请求要求用户的身份认证 402 Payment Required 保留，将来使用 403 Forbidden 服务器理解请求客户端的请求，但是拒绝执行此请求 404 Not Found 服务器无法根据客户端的请求找到资源（网页）。通过此代码，网站设计人员可设置”您所请求的资源无法找到”的个性页面 405 Method Not Allowed 客户端请求中的方法被禁止 406 Not Acceptable 服务器无法根据客户端请求的内容特性完成请求 407 Proxy Authentication Required 请求要求代理的身份认证，与401类似，但请求者应当使用代理进行授权 408 Request Time-out 服务器等待客户端发送的请求时间过长，超时 409 Conflict 服务器完成客户端的PUT请求是可能返回此代码，服务器处理请求时发生了冲突 410 Gone 客户端请求的资源已经不存在。410不同于404，如果资源以前有现在被永久删除了可使用410代码，网站设计人员可通过301代码指定资源的新位置 411 Length Required 服务器无法处理客户端发送的不带Content-Length的请求信息 412 Precondition Failed 客户端请求信息的先决条件错误 413 Request Entity Too Large 由于请求的实体过大，服务器无法处理，因此拒绝请求。为防止客户端的连续请求，服务器可能会关闭连接。如果只是服务器暂时无法处理，则会包含一个Retry-After的响应信息 414 Request-URI Too Large 请求的URI过长（URI通常为网址），服务器无法处理 415 Unsupported Media Type 服务器无法处理请求附带的媒体格式 416 Requested range not satisfiable 客户端请求的范围无效 417 Expectation Failed 服务器无法满足Expect的请求头信息 500 Internal Server Error 服务器内部错误，无法完成请求 501 Not Implemented 服务器不支持请求的功能，无法完成请求 502 Bad Gateway 作为网关或者代理工作的服务器尝试执行请求时，从远程服务器接收到了一个无效的响应 503 Service Unavailable 由于超载或系统维护，服务器暂时的无法处理客户端的请求。延时的长度可包含在服务器的Retry-After头信息中 504 Gateway Time-out 充当网关或代理的服务器，未及时从远端服务器获取请求 505 HTTP Version not supported 服务器不支持请求的HTTP协议的版本，无法完成处理]]></content>
      <categories>
        <category>Http</category>
      </categories>
      <tags>
        <tag>Http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven Scope]]></title>
    <url>%2Fposts%2F7b73%2F</url>
    <content type="text"><![CDATA[compile默认就是compile，什么都不配置也就是意味着compile。compile表示被依赖项目需要参与当前项目的编译，当然后续的测试，运行周期也参与其中，是一个比较强的依赖。打包的时候通常需要包含进去。 testscope为test表示依赖项目仅仅参与测试相关的工作，包括测试代码的编译，执行。比较典型的如junit。 runntimerunntime表示被依赖项目无需参与项目的编译，不过后期的测试和运行周期需要其参与。与compile相比，跳过编译而已，说实话在终端的项目（非开源，企业内部系统）中，和compile区别不是很大。比较常见的如JSR×××的实现，对应的API jar是compile的，具体实现是runtime的，compile只需要知道接口就足够了。oracle jdbc驱动架包就是一个很好的例子，一般scope为runntime。另外runntime的依赖通常和optional搭配使用，optional为true。我可以用A实现，也可以用B实现。 providedprovided意味着打包的时候可以不用包进去，别的设施(Web Container)会提供。事实上该依赖理论上可以参与编译，测试，运行等周期。相当于compile，但是在打包阶段做了exclude的动作。 system从参与度来说，也provided相同，不过被依赖项不会从maven仓库抓，而是从本地文件系统拿，一定需要配合systemPath属性使用。 Dependency Scope compile This is the default scope, used if none is specified. Compile dependencies are available in all classpaths of a project. Furthermore, those dependencies are propagated to dependent projects. provided This is much like compile, but indicates you expect the JDK or a container to provide the dependency at runtime. For example, when building a web application for the Java Enterprise Edition, you would set the dependency on the Servlet API and related Java EE APIs to scope provided because the web container provides those classes. This scope is only available on the compilation and test classpath, and is not transitive. runtime This scope indicates that the dependency is not required for compilation, but is for execution. It is in the runtime and test classpaths, but not the compile classpath. test This scope indicates that the dependency is not required for normal use of the application, and is only available for the test compilation and execution phases. This scope is not transitive. system This scope is similar to provided except that you have to provide the JAR which contains it explicitly. The artifact is always available and is not looked up in a repository. import This scope is only supported on a dependency of type pom in the section. It indicates the dependency to be replaced with the effective list of dependencies in the specified POM’s section. Since they are replaced, dependencies with a scope of import do not actually participate in limiting the transitivity of a dependency.]]></content>
      <categories>
        <category>Maven</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebSocket]]></title>
    <url>%2Fposts%2F67c3%2F</url>
    <content type="text"><![CDATA[WebSocket是一种通信协议，可在单个TCP连接上进行全双工通信。 优点 较少的控制开销。在连接创建后，服务器和客户端之间交换数据时，用于协议控制的数据包头部相对较小。在不包含扩展的情况下，对于服务器到客户端的内容，此头部大小只有2至10字节（和数据包长度有关）；对于客户端到服务器的内容，此头部还需要加上额外的4字节的掩码。相对于HTTP请求每次都要携带完整的头部，此项开销显著减少了。 更强的实时性。由于协议是全双工的，所以服务器可以随时主动给客户端下发数据。相对于HTTP请求需要等待客户端发起请求服务端才能响应，延迟明显更少；即使是和Comet等类似的长轮询比较，其也能在短时间内更多次地传递数据。 保持连接状态。与HTTP不同的是，Websocket需要先创建连接，这就使得其成为一种有状态的协议，之后通信时可以省略部分状态信息。而HTTP请求可能需要在每个请求都携带状态信息（如身份认证等）。 更好的二进制支持。Websocket定义了二进制帧，相对HTTP，可以更轻松地处理二进制内容。 可以支持扩展。Websocket定义了扩展，用户可以扩展协议、实现部分自定义的子协议。如部分浏览器支持压缩等。 更好的压缩效果。相对于HTTP压缩，Websocket在适当的扩展支持下，可以沿用之前内容的上下文，在传递类似的数据时，可以显著地提高压缩率。 握手协议WebSocket 是独立的、创建在 TCP 上的协议。 Websocket 通过 HTTP/1.1 协议的101状态码进行握手。 为了创建Websocket连接，需要通过浏览器发出请求，之后服务器进行回应，这个过程通常称为“握手 “握手 (技术)”)”（handshaking）。 例子一个典型的Websocket握手请求如下： 客户端请求 1234567GET / HTTP/1.1Upgrade: websocketConnection: UpgradeHost: example.comOrigin: http://example.comSec-WebSocket-Key: sN9cRrP/n9NdMgdcy2VJFQ==Sec-WebSocket-Version: 13 服务器回应 12345HTTP/1.1 101 Switching ProtocolsUpgrade: websocketConnection: UpgradeSec-WebSocket-Accept: fFBooB7FAkLlXgRSz0BT3v4hq5s=Sec-WebSocket-Location: ws://example.com/ 字段说明 Connection必须设置Upgrade，表示客户端希望连接升级。 Upgrade字段必须设置Websocket，表示希望升级到Websocket协议。 Sec-WebSocket-Key是随机的字符串，服务器端会用这些数据来构造出一个SHA-1的信息摘要。把“Sec-WebSocket-Key”加上一个特殊字符串“258EAFA5-E914-47DA-95CA-C5AB0DC85B11”，然后计算SHA-1摘要，之后进行BASE-64编码，将结果做为“Sec-WebSocket-Accept”头的值，返回给客户端。如此操作，可以尽量避免普通HTTP请求被误认为Websocket协议。 Sec-WebSocket-Version 表示支持的Websocket版本。RFC6455要求使用的版本是13，之前草案的版本均应当弃用。 Origin字段是可选的，通常用来表示在浏览器中发起此Websocket连接所在的页面，类似于Referer。但是，与Referer不同的是，Origin只包含了协议和主机名称。 其他一些定义在HTTP协议中的字段，如Cookie等，也可以在Websocket中使用。 其他伪长链接方式AJAX 轮询浏览器隔个几秒就发送一次请求。需要服务器有很快的处理速度和资源。 Long Pull原理跟 ajax轮询 差不多，都是采用轮询的方式，不过采取的是阻塞模型。需要有很高的并发。]]></content>
      <categories>
        <category>WebSocket</category>
      </categories>
      <tags>
        <tag>WebSocket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cloud Toolkit之Command编写指南]]></title>
    <url>%2Fposts%2F671a%2F</url>
    <content type="text"><![CDATA[Cloud Toolkit 帮助开发者将本地应用程序一键部署到阿里云 ECS、EDAS 和 Kubernetes 和任意服务器中去。 Spring Boot 应用 start.sh 12345#!/bin/bashknowledge_pwd=/home/jhuser/knowledgecd $knowledge_pwdecho startingnohup jdk1.8.0_151/bin/java -jar knowledge-aggregator-1.0-SNAPSHOT.jar &gt; nohup.out 2&gt;&amp;1 &amp; stop.sh 123456789#!/bin/bashPID=$(ps -ef | grep knowledge-aggregator-1.0-SNAPSHOT.jar | grep -v grep | awk '&#123; print $2 &#125;')if [ -z "$PID" ]then echo knowledge is already stoppedelse echo kill $PID kill $PIDfi restart.sh 12345#!/bin/bashecho stop knowledgesource /home/jhuser/knowledge/stop.shecho start knowledgesource /home/jhuser/knowledge/start.sh 标准的 Java Web Tomcat 应用 restart-tomcat.sh 12345678910111213141516#!/bin/bashecho "stop hug_interview....."PID=$(ps -ef | grep tomcat_2.0/bin | grep -v grep | awk '&#123; print $2 &#125;')if [ -z "$PID" ]then echo hug_interview is already stoppedelse echo kill $PID kill $PIDfiecho "remove old hug_interview"rm -rf /home/hug_interview/2.0/tomcat_2.0/webapps/hug_interviewecho "start hug_interview....."sh /home/hug_interview/2.0/tomcat_2.0/bin/startup.sh]]></content>
      <categories>
        <category>Idea</category>
      </categories>
      <tags>
        <tag>Idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile相关命令介绍]]></title>
    <url>%2Fposts%2F3e37%2F</url>
    <content type="text"><![CDATA[使用docker build命令或使用Docker Hub的自动构建功能构建Docker镜像时，都需要一个Dockerfile文件。Dockerfile文件是一个由一系列构建指令组成的文本文件，docker build命令会根据这些构建指令完成Docker镜像的构建。本文将会介绍Dockerfile文件，及其中使用的构建指令。 Dockerfile文件使用docker build命令会根据Dockerfile文件及上下文构建新Docker镜像。构建上下文是指Dockerfile所在的本地路径或一个URL（Git仓库地址）。构建上下文环境会被递归处理，所以，构建所指定的路径还包括了子目录，而URL还包括了其中指定的子模块。 构建镜像 将当前目录做为构建上下文时，可以像下面这样使用docker build命令构建镜像： 123$ docker build .Sending build context to Docker daemon 6.51 MB... 说明：构建会在Docker后台守护进程（daemon）中执行，而不是CLI中。构建前，构建进程会将全部内容（递归）发送到守护进程。大多情况下，应该将一个空目录作为构建上下文环境，并将Dockerfile文件放在该目录下。 在构建上下文中使用的Dockerfile文件，是一个构建指令文件。为了提高构建性能，可以通过.dockerignore文件排除上下文目录下，不需要的文件和目录。 Dockerfile一般位于构建上下文的根目录下，也可以通过-f指定该文件的位置： 1$ docker build -f /path/to/a/Dockerfile . 构建时，还可以通过-t参数指定构建成后，镜像的仓库、标签等： 镜像标签 1$ docker build -t shykes/myapp \. 如果存在多个仓库下，或使用多个镜像标签，就可以使用多个-t参数： $ docker build -t shykes/myapp:1.0.2 -t shykes/myapp:latest . 在Docker守护进程执行Dockerfile中的指令前，首先会对Dockerfile进行语法检查，有语法错误时会返回： 123$ docker build -t test/myapp \.Sending build context to Docker daemon 2.048 kBError response from daemon: Unknown instruction: RUNCMD 缓存 Docker 守护进程会一条一条的执行Dockerfile中的指令，而且会在每一步提交并生成一个新镜像，最后会输出最终镜像的ID。生成完成后，Docker 守护进程会自动清理你发送的上下文。 Dockerfile文件中的每条指令会被独立执行，并会创建一个新镜像，RUN cd /tmp等命令不会对下条指令产生影响。 Docker 会重用已生成的中间镜像，以加速docker build的构建速度。以下是一个使用了缓存镜像的执行过程： 123456789101112131415$ docker build -t svendowideit/ambassador \.Sending build context to Docker daemon 15.36 kBStep 1/4 : FROM alpine:3.2 ---&gt; 31f630c65071Step 2/4 : MAINTAINER SvenDowideit@home.org.au ---&gt; Using cache ---&gt; 2a1c91448f5fStep 3/4 : RUN apk update &amp;&amp; apk add socat &amp;&amp; rm -r /var/cache/ ---&gt; Using cache ---&gt; 21ed6e7fbb73Step 4/4 : CMD env | grep _TCP= | (sed 's/.*_PORT_\([0-9]*\)_TCP=tcp:\/\/\(.*\):\(.*\)/socat -t 100000000 TCP4-LISTEN:\1,fork,reuseaddr TCP4:\2:\3 \&amp;/' &amp;&amp; echo wait) | sh ---&gt; Using cache ---&gt; 7ea8aef582ccSuccessfully built 7ea8aef582cc 构建缓存仅会使用本地父生成链上的镜像。如果不想使用本地缓存的镜像，也可以通过--cache-from指定缓存。指定后将再不使用本地生成的镜像链，而是从镜像仓库中下载。 Dockerfile文件格式Dockerfile文件格式如下： 12345# CommentINSTRUCTION arguments# 注释指令 参数 Dockerfile文件中指令不区分大小写，但为了更易区分，约定使用大写形式。 Docker 会依次执行Dockerfile中的指令，文件中的第一条指令必须是FROM，FROM指令用于指定一个基础镜像。 以#开头的行，Docker会认为是注释。但#出现在指令参数中时，则不是注释。如： # 12# CommentRUN echo 'we are running some # of cool things' Dockerfile中使用指令FROMFROM指令用于指定其后构建新镜像所使用的基础镜像。FROM指令必是Dockerfile文件中的首条命令，启动构建流程后，Docker将会基于该镜像构建新镜像，FROM后的命令也会基于这个基础镜像。 FROM语法格式为： 1FROM &lt;image&gt; 或 1FROM &lt;image&gt;:&lt;tag&gt; 或 1FROM &lt;image&gt;:&lt;digest&gt; 通过FROM指定的镜像，可以是任何有效的基础镜像。FROM有以下限制： FROM必须是Dockerfile中第一条非注释命令 在一个Dockerfile文件中创建多个镜像时，FROM可以多次出现。只需在每个新命令FROM之前，记录提交上次的镜像ID。 tag或digest是可选的，如果不使用这两个值时，会使用latest版本的基础镜像 RUNRUN用于在镜像容器中执行命令，其有以下两种命令执行方式： shell执行 在这种方式会在shell中执行命令，Linux下默认使用/bin/sh -c，Windows下使用cmd /S /C。 注意：通过SHELL命令修改RUN所使用的默认shell 1RUN &lt;command&gt; exec执行 RUN [“executable”, “param1”, “param2”] RUN可以执行任何命令，然后在当前镜像上创建一个新层并提交。提交后的结果镜像将会用在Dockerfile文件的下一步。 通过RUN执行多条命令时，可以通过\换行执行： 12RUN /bin/bash -c 'source $HOME/.bashrc; \echo $HOME' 也可以在同一行中，通过分号分隔命令： 1RUN /bin/bash -c 'source $HOME/.bashrc; echo $HOME' RUN指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像，可以在构建时指定--no-cache参数，如：docker build --no-cache。 CMDCMD用于指定在容器启动时所要执行的命令。CMD有以下三种格式： 123CMD ["executable","param1","param2"]CMD ["param1","param2"]CMD command param1 param2 CMD不同于RUN，CMD用于指定在容器启动时所要执行的命令，而RUN用于指定镜像构建时所要执行的命令。 CMD与RUN在功能实现上也有相似之处。如： 1docker run -t -i itbilu/static_web_server /bin/true 等价于： 1cmd ["/bin/true"] CMD在Dockerfile文件中仅可指定一次，指定多次时，会覆盖前的指令。 另外，docker run命令也会覆盖Dockerfile中CMD命令。如果docker run运行容器时，使用了Dockerfile中CMD相同的命令，就会覆盖Dockerfile中的CMD命令。 如，我们在构建镜像的Dockerfile文件中使用了如下指令： 1CMD ["/bin/bash"] 使用docker build构建一个新镜像，镜像名为itbilu/test。构建完成后，使用这个镜像运行一个新容器，运行效果如下： 12$ sudo docker run -i -t itbilu/testroot@e3597c81aef4:/# 在使用docker run运行容器时，我们并没有在命令结尾指定会在容器中执行的命令，这时Docker就会执行在Dockerfile的CMD中指定的命令。 如果不想使用CMD中指定的命令，就可以在docker run命令的结尾指定所要运行的命令： 123$ sudo docker run -i -t itbilu/test /bin/ps PID TTY TIME CMD 1 ? 00:00:00 ps 这时，docker run结尾指定的/bin/ps命令覆盖了Dockerfile的CMD中指定的命令。 ENTRYPOINTENTRYPOINT用于给容器配置一个可执行程序。也就是说，每次使用镜像创建容器时，通过ENTRYPOINT指定的程序都会被设置为默认程序。ENTRYPOINT有以下两种形式： 12ENTRYPOINT ["executable", "param1", "param2"]ENTRYPOINT command param1 param2 ENTRYPOINT与CMD非常类似，不同的是通过docker run执行的命令不会覆盖ENTRYPOINT，而docker run命令中指定的任何参数，都会被当做参数再次传递给ENTRYPOINT。Dockerfile中只允许有一个ENTRYPOINT命令，多指定时会覆盖前面的设置，而只执行最后的ENTRYPOINT指令。 docker run运行容器时指定的参数都会被传递给ENTRYPOINT，且会覆盖CMD命令指定的参数。如，执行docker run &lt;image&gt; -d时，-d参数将被传递给入口点。 也可以通过docker run --entrypoint重写ENTRYPOINT入口点。 如：可以像下面这样指定一个容器执行程序： 1ENTRYPOINT ["/usr/bin/nginx"] 完整构建代码： 123456789# Version: 0.0.3FROM ubuntu:16.04MAINTAINER 何民三 "cn.liuht@gmail.com"RUN apt-get updateRUN apt-get install -y nginxRUN echo 'Hello World, 我是个容器' \ &gt; /var/www/html/index.htmlENTRYPOINT ["/usr/sbin/nginx"]EXPOSE 80 使用docker build构建镜像，并将镜像指定为itbilu/test： 1$ sudo docker build -t="itbilu/test" . 构建完成后，使用itbilu/test启动一个容器： 1$ sudo docker run -i -t itbilu/test -g "daemon off;" 在运行容器时，我们使用了-g &quot;daemon off;&quot;，这个参数将会被传递给ENTRYPOINT，最终在容器中执行的命令为/usr/sbin/nginx -g &quot;daemon off;&quot;。 LABELLABEL用于为镜像添加元数据，元数以键值对的形式指定： 1LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ... 使用LABEL指定元数据时，一条LABEL指定可以指定一或多条元数据，指定多条元数据时不同元数据之间通过空格分隔。推荐将所有的元数据通过一条LABEL指令指定，以免生成过多的中间镜像。 如，通过LABEL指定一些元数据： 1LABEL version="1.0" description="这是一个Web服务器" by="IT笔录" 指定后可以通过docker inspect查看： 123456$sudo docker inspect itbilu/test"Labels": &#123; "version": "1.0", "description": "这是一个Web服务器", "by": "IT笔录"&#125;, 注意；Dockerfile中还有个MAINTAINER命令，该命令用于指定镜像作者。但MAINTAINER并不推荐使用，更推荐使用LABEL来指定镜像作者。如： 1LABEL maintainer="itbilu.com" EXPOSEEXPOSE用于指定容器在运行时监听的端口： 1EXPOSE &lt;port&gt; [&lt;port&gt;...] EXPOSE并不会让容器的端口访问到主机。要使其可访问，需要在docker run运行容器时通过-p来发布这些端口，或通过-P参数来发布EXPOSE导出的所有端口。 ENVENV用于设置环境变量，其有以下两种设置形式： 12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key&gt;=&lt;value&gt; ... 如，通过ENV设置一个环境变量： 1ENV ITBILU_PATH /home/itbilu/ 设置后，这个环境变量在ENV命令后都可以使用。如： 1WORKERDIR $ITBILU_PATH 这些环境变量不仅可以构建镜像过程使用，使用该镜像创建的容器中也可以使用。如： 123$ docker run -i -t itbilu/test root@196ca123c0c3:/# cd $ITBILU_PATHroot@196ca123c0c3:/home/itbilu# ADDADD用于复制构建环境中的文件或目录到镜像中。其有以下两种使用方式： 12ADD &lt;src&gt;... &lt;dest&gt;ADD ["&lt;src&gt;",... "&lt;dest&gt;"] 通过ADD复制文件时，需要通过指定源文件位置，并通过&lt;dest&gt;来指定目标位置。可以是一个构建上下文中的文件或目录，也可以是一个URL，但不能访问构建上下文之外的文件或目录。 如，通过ADD复制一个网络文件： 1ADD http://wordpress.org/latest.zip $ITBILU_PATH 在上例中，$ITBILU_PATH是我们使用ENV指定的一个环境变量。 另外，如果使用的是本地归档文件（gzip、bzip2、xz）时，Docker会自动进行解包操作，类似使用tar -x。 COPYCOPY同样用于复制构建环境中的文件或目录到镜像中。其有以下两种使用方式： 12COPY &lt;src&gt;... &lt;dest&gt;COPY ["&lt;src&gt;",... "&lt;dest&gt;"] COPY指令非常类似于ADD，不同点在于COPY只会复制构建目录下的文件，不能使用URL也不会进行解压操作。 VOLUMEVOLUME用于创建挂载点，即向基于所构建镜像创始的容器添加卷： 1VOLUME ["/data"] 一个卷可以存在于一个或多个容器的指定目录，该目录可以绕过联合文件系统，并具有以下功能： 卷可以容器间共享和重用 容器并不一定要和其它容器共享卷 修改卷后会立即生效 对卷的修改不会对镜像产生影响 卷会一直存在，直到没有任何容器在使用它 VOLUME让我们可以将源代码、数据或其它内容添加到镜像中，而又不并提交到镜像中，并使我们可以多个容器间共享这些内容。 如，通过VOLUME创建一个挂载点： 12ENV ITBILU_PATH /home/itbilu/VOLUME [$ITBILU_PATH] 构建的镜像，并指定镜像名为itbilu/test。构建镜像后，使用新构建的运行一个容器。运行容器时，需-v参将能本地目录绑定到容器的卷（挂载点）上，以使容器可以访问宿主机的数据。 1234$ sudo docker run -i -t -v ~/code/itbilu:/home/itbilu/ itbilu/test root@31b0fac536c4:/# cd /home/itbilu/root@31b0fac536c4:/home/itbilu# lsREADME.md app.js bin config.js controller db demo document lib minify.js node_modules package.json public routes test views 如上所示，我们已经可以容器的/home/itbilu/目录下访问到宿主机~/code/itbilu目录下的数据了。 USERUSER用于指定运行镜像所使用的用户： 1USER daemon 使用USER指定用户时，可以使用用户名、UID或GID，或是两者的组合。以下都是合法的指定试： 123456USER userUSER user:groupUSER uidUSER uid:gidUSER user:gidUSER uid:group 使用USER指定用户后，Dockerfile中其后的命令RUN、CMD、ENTRYPOINT都将使用该用户。镜像构建完成后，通过docker run运行容器时，可以通过-u参数来覆盖所指定的用户。 WORKDIRWORKDIR用于在容器内设置一个工作目录： 1WORKDIR /path/to/workdir 通过WORKDIR设置工作目录后，Dockerfile中其后的命令RUN、CMD、ENTRYPOINT、ADD、COPY等命令都会在该目录下执行。 如，使用WORKDIR设置工作目录： 1234WORKDIR /aWORKDIR bWORKDIR cRUN pwd 在以上示例中，pwd最终将会在/a/b/c目录中执行。 在使用docker run运行容器时，可以通过-w参数覆盖构建时所设置的工作目录。 ARGARG用于指定传递给构建运行时的变量： 1ARG &lt;name&gt;[=&lt;default value&gt;] 如，通过ARG指定两个变量： 12ARG siteARG build_user=IT笔录 以上我们指定了site和build_user两个变量，其中build_user指定了默认值。在使用docker build构建镜像时，可以通过--build-arg &lt;varname&gt;=&lt;value&gt;参数来指定或重设置这些变量的值。 1$ sudo docker build --build-arg site=itiblu.com -t itbilu/test . 这样我们构建了itbilu/test镜像，其中site会被设置为itbilu.com，由于没有指定build_user，其值将是默认值IT笔录。 ONBUILDONBUILD用于设置镜像触发器： 1ONBUILD [INSTRUCTION] 当所构建的镜像被用做其它镜像的基础镜像，该镜像中的触发器将会被钥触发。 如，当镜像被使用时，可能需要做一些处理： 1234[...]ONBUILD ADD . /app/srcONBUILD RUN /usr/local/bin/python-build --dir /app/src[...] STOPSIGNALSTOPSIGNAL用于设置停止容器所要发送的系统调用信号： STOPSIGNAL signal 所使用的信号必须是内核系统调用表中的合法的值，如：9、SIGKILL。 SHELLSHELL用于设置执行命令（shell式）所使用的的默认shell类型： 1SHELL ["executable", "parameters"] SHELL在Windows环境下比较有用，Windows下通常会有cmd和powershell两种shell，可能还会有sh。这时就可以通过SHELL来指定所使用的shell类型： 1FROM microsoft/windowsservercore 123456789101112131415FROM microsoft/windowsservercore# Executed as cmd /S /C echo defaultRUN echo default# Executed as cmd /S /C powershell -command Write-Host defaultRUN powershell -command Write-Host default# Executed as powershell -command Write-Host helloSHELL ["powershell", "-command"]RUN Write-Host hello# Executed as cmd /S /C echo helloSHELL ["cmd", "/S"", "/C"]RUN echo helloD]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解JDBC超时]]></title>
    <url>%2Fposts%2Ff7b3%2F</url>
    <content type="text"><![CDATA[这是最近读到的讲关于 JDBC 的超时问题最透彻的文章，原文是http://www.cubrid.org/blog/understanding-jdbc-internals-and-timeout-configuration ，网上现有的翻译感觉磕磕绊绊的，很多上下文信息丢失了，这里用我的理解重新翻译一下。 应用程序中配置恰当的 JDBC 超时时间能减少服务失败的时间，这篇文章我们将讨论不同种类的超时和推荐的配置。 Web 应用服务器在 DDoS 攻击后变得无响应(这是一个真实案例的发生过程复述) 在 DDoS 攻击之后，整个服务都不能正常工作了，因为第四层交换机不能工作，网络连接断开了，这也导致 WAS （可以将 WAS 理解为作者公司的应用程序）不能正常工作。攻击发生后不久，安全团队拦截了所有 DDoS 攻击，然后网络恢复正常，但 WAS 还是不能工作。 通过分析系统的 dump 日志发现，业务系统停在了 JDBC API 的调用上。20分钟后系统仍处于等待状态无法响应，大概过了30分钟，系统突然发生异常，然后服务恢复正常。 为什么已经将查询超时时间设置成3秒， WAS 却等待了30分钟？为什么30分钟后 WAS 又开始工作了？ 如果理解了 JDBC 的超时机制就能找到答案。 为什么我们需要知道 JDBC 驱动当有性能问题或系统级错误时，WAS 和数据库是我们关注的两个重要层面。在我公司 WAS 和数据库通常由不同的部门负责，因此每个部门聚焦在各自负责的领域来设法弄清楚状况。此时 WAS 和数据库之间的部分会因为得不到足够的关注而产生盲区。对于 Java 应用，这个盲区在数据库连接池和 JDBC 之间，本文我们将重点讨论 JDBC。 什么是 JDBC 驱动JDBC 是 Java 应用程序中用于访问数据库的一套标准 API，Sun 公司定义了4种类型的 JDBC 驱动。我公司主要用的是第4种，该类型驱动由纯 Java 语言编写，在 Java 应用中通过 socket 与数据库通信。 图1: 类型4驱动 类型4驱动是通过 socket 来处理字节流的，它的基本操作和 HttpClient 这种网络操作类库相同。同其他网络类库一样，也会在发生超时的时候占用大量的 CPU 资源从而失去响应。如果你之前用过 HttpClient ，肯定遇到过因为没有设置超时导致的错误。如果 socket 超时设置不合适，类型4驱动也可能有同样的错误（连接被阻塞）。 下面让我们了解如何配置 JDBC 驱动的 socket 超时，以及设置时需考虑哪些问题。 WAS 与数据库间的设置超时的层次 图2: 超时的层次 图2展示了简化的 WAS 和数据库通信时的超时层次。 更上层的超时依赖于下层的超时，只有当较低层的超时机制正常工作，上层的超时才会正常。如果 JDBC 驱动程序的 socket 超时工作不正常，那么更上层的超时比如 Statement 超时和事务超时都不会正常工作。 我们收到很多评论说： 即使配置了 Statement 超时，应用程序还是不能从故障中恢复，因为 Statement 超时在网络故障时不起作用。 Statement 超时在网络故障时不起作用。它只能做到：限制一次Statement 执行的时间，处理超时以防网络故障必须由 JDBC 驱动来做。 JDBC 驱动的 socket 超时还会受操作系统的 socket 超时配置的影响。这解释了为什么案例中的 JDBC 连接在网络故障后阻塞了30分钟才恢复，即使没配置 JDBC 驱动的 socket 超时。 DBCP 连接池位于图2的左边。你会发现各种层面的超时与 DBCP 是分开的。DBCP 负责数据库连接（即本文中说到的Connection）的创建和管理，并不涉及超时的处理。当在 DBCP 中创建了一个数据库连接或发送了一条查询校验的 sql 语句用于检查连接有效性时，socket 超时会影响这些过程的处理，但并不直接影响应用程序。 然而在应用程序中调用 DBCP 的 getConnection() 方法时，你能指定应用程序获取数据库连接的超时时间，但这和 JDBC 的连接超时无关。 图3: 每一层级的超时 什么是事务超时事务超时是在框架（Spring、EJB容器）或应用程序层面上才有效的超时。 事务超时可能是个不常见的概念。简单讲，事务超时等于 Statement 超时 * N（需要执行的 Statement 的数量） + 其它（垃圾回收等其他时间）。事务超时被用来限制执行一个事务之内所有 Statement 执行的总时长。 比如，假设执行一次 Statement 执行需0.1秒，那执行几次 Statement并不是什么问题，但如果是执行十万次则需要一万秒（大约7个小时），这就可以用上事务超时了。 EJB 的声明式事务管理 (容器管理事务) 就是一种典型的使用场景，但声明式事务管理只是定义了相应的规范，容器内事务的处理过程和具体实现由容器的开发者负责。我们公司并没有用 EJB，用的是最常见的 Spring 框架，所以事务超时的配置也由 Spring 来管理。在 Spring 中，事务超时可以在 XML 文件显式配置或在 Java 代码中用 Transactional 注解来配置。 123&lt;tx:attributes&gt; &lt;tx:method name=&quot;…&quot; timeout=&quot;3&quot;/&gt;&lt;/tx:attributes&gt; Spring 提供的事务超时的配置非常简单，它会记录每个事务的开始时间和消耗时间，当特定的事件发生时会对已消耗掉的时间做校验，如果超出了配置将抛出异常。 Spring 中数据库连接被保存在线程本地变量（ThreadLocal）中，这被称作事务同步（Transaction Synchronization）。当数据库连接被保存到 ThreadLocal 时，同时会记录事务的开始时间和超时时间。所以通过数据库连接的代理创建的 Statement 在执行时就会校验这个时间。 EJB 的声明式事务管理的实现也是类似，实现的思路非常简单。如果事务超时非常重要，但你所使用的容器或框架不提供此功能，你也可以选择自己实现，关于事务超时并没有制定标准的 API。 Lucy 框架的1.5和1.6版不支持事务超时，但你可以通过 Spring 的事务管理达到相同的效果。 假设一个事务里有5条 Statement ，每条 Statement 执行时间是200毫秒，其它业务逻辑或框架操作的执行时间是100毫秒，那事务允许的超时时间至少应该1100毫秒（200 * 5 + 100）。 什么是 Statement 超时Statement 超时是用来限制 Statement 的执行时间的，它的具体值是通过 JDBC API 来设置的。JDBC 驱动程序基于这个值进行 Statement 执行时的超时处理。Statement 超时是通过 JDBC API 中java.sql.Statement 类的 setQueryTimeout(int timeout) 方法配置的。不过现在的开发者已经很少直接在代码中配置它了，更多是通过框架来进行设置。 以 iBatis 为例，可以通过 SqlMapConfig.xml 中的 setting 属性defaultStatementTimeout 来设置全局的 statement 超时缺省值。你也可以通过在具体的 sql 映射文件中的 select insert update 标签的 statement 属性来覆盖。 当你用 Lucy 1.5或1.6版时，可以通过设置 queryTimeout 属性在数据源层面设置 Statement 超时。 Statement 超时的具体数值需要根据每个应用自身的情况而定，并没有推荐的配置。 JDBC 驱动中的 Statement 超时处理过程每个数据库和驱动程序的 Statement 超时的处理也是不同的。Oracle 和 SQLServer 的工作方式比较像，MySQL 和 CUBRID 比较像。 Oracle 中的 Statement 超时处理 调用 Connection 的 createStatement() 方法创建一个 Statement 对象 调用 Statement 的 executeQuery() 方法 Statement 通过内部绑定的 Connection 对象将查询命令发送到 Oracle 数据库 Statement 向 Oracle 的超时处理线程 OracleTimeoutPollingThread（每个类加载器一个该线程）注册一个 Statement 用于处理超时 发生超时 Oracle 的 OracleTimeoutPollingThread 调用 OracleStatement 的 cancel() 方法 通过 Statement 的 Connection 发送一条消息取消还在执行的查询 图4 Oracle 的 Statement 超时执行过程 JTDS (MS SQLServer) 中的 Statement 超时处理1.调用 Connection 的 createStatement() 方法创建一个 Statement 对象 调用 Statement 的 executeQuery() 方法 Statement 通过内部的 Connection 将查询命令发送到 MS SqlServer 数据库 Statement 向 MS SQLServer 的 TimerThread 线程注册一个 Statement 用于处理超时 发生超时 TimerThread 调用 JtdsStatement 内部的 TsdCore.cancel()方法 通过 ConnectionJDBC 发送一条消息取消还在执行的查询 图5 MS SQLServer 的 Statement 超时执行过程 MySQL (5.0.8) 中的 Statement 超时处理 调用 Connection 的 createStatement() 方法创建一个 Statement 对象 调用 Statement 的 executeQuery() 方法 Statement 通过内部的 Connection 将查询命令传输到 MySqlServer 数据库 Statement 创建一个新的超时执行线程(timeout-execution)来处理超时 5.1以上版本改为每个连接分配一个线程 向 timeout-execution 线程注册当前的 Statement 发生超时 timeout-execution 线程创建一个相同配置的 Connection 用新创建的 Connection 发送取消查询的命令 图6 MySQL 的 Statement 超时执行过程 CUBRID中的 Statement 超时处理 调用 Connection 的 createStatement() 方法创建一个 Statement 对象 调用 Statement 的 executeQuery() 方法 Statement 通过内部的 Connection 将查询命令发送到 CUBRID 数据库 Statement 创建一个新的超时执行线程(timeout-execution)来处理超时 向 timeout-execution 线程注册当前的 Statement 发生超时 timeout-execution 线程创建一个相同配置的Connection 用新创建的 Connection 发送取消查询的命令 图7 CUBRID 的 Statement 超时执行过程 什么是 Socket 超时类型4的 JDBC 驱动是用 Socket 方式与数据库连接的，应用程序和数据库之间的连接超时并不是由数据库处理的。 当数据库突然宕掉或发生网络错误（设备故障等）时，JDBC 驱动的 Socket 超时的值是必须的。由于 TCP/IP 的结构，Socket 没有办法检测到网络错误，因此应用不能检测到与数据库到连接断开了。如果没有设置 Socket 超时，应用程序会一直等待数据库返回结果。（这个连接也被叫做“死连接”） 为了避免死连接，Socket 必须要设置超时时间。Socket 超时可以通过 JDBC 驱动程序配置。通过设置 Socket 超时，可以防止出现网络错误时一直等待的情况并缩短故障时间。 不推荐使用 Socket 超时来限制一个 Statement 的执行时间，因此Socket 超时的值必须要高于 Statement 的超时时间，否则 Socket 超时将会先生效，这样 Statement 超时就没有意义，也无法生效。 下面展示了 Socket 超时设置的连个选项，其配置因不同的驱动而异。 Socket 连接时的超时：通过 Socket 对象的 connect(SocketAddress endpoint, int timeout) 方法来配置 Socket 读写时的超时：通过 Socket 对象的 setSoTimeout(int timeout) 方法来配置 通过查看CUBRID，MySQL，MS SQL Server (JTDS) 和 Oracle 的JDBC 驱动源码，我们确认以上所有驱动都是使用上面的2个 API 来设置socket 超时的。 下面列出了如何配置 Socket 超时 JDBC 驱动 连接超时配置 Socket 超时配置 JDBC Url 格式 示例 MySQL connectTimeout（默认值：0，单位：毫秒） socketTimeout（默认值：0，单位：ms） jdbc:mysql://[[host:port],[host:port]…/[database]][?propertyName1][=propertyValue1][&amp;propertyName2][=propertyValue2]… jdbc:[mysql://xxx.xx.xxx.xxx:3306/database?connectTimeout=60000&amp;socketTimeout=60000] MS-SQL , jTDS loginTimeout（默认值：0，单位：秒） socketTimeout（默认值：0，单位：s） jdbc:jtds:\&lt;server_type&gt;://\[:\][/\][;\=\[;…]] jdbc:jtds:sqlserver://server:port/database;loginTimeout=60;socketTimeout=60 Oracle oracle.net.CONNECT_TIMEOUT （默认值：0，单位：毫秒） oracle.jdbc.ReadTimeout（默认值：0，单位：毫秒） 不支持通过url配置，只能通过OracleDatasource.setConnectionProperties() API设置，使用DBCP时可以调用BasicDatasource.setConnectionProperties()或BasicDatasource.addConnectionProperties()进行设置 CUBRID 无单独配置项（默认值：5,000，单位：毫秒） 无单独配置项（默认值：5,000，单位：毫秒） connectTimeout 和 socketTimeout 的默认值是 0 ，这意味着不会发生超时。 你也可以通过属性进行配置，而无需直接使用 DBCP 的 API 。 通过属性进行配置时，需要传入的 key 为 “connectionProperties”，其 value 的格式为” [propertyName=property;]*”。下面是 iBatis 中通过 xml 文件配置属性的例子。 123456&lt;transactionManager type=&quot;JDBC&quot;&gt; &lt;dataSource type=&quot;com.nhncorp.lucy.db.DbcpDSFactory&quot;&gt; .... &lt;property name=&quot;connectionProperties&quot; value=&quot;oracle.net.CONNECT_TIMEOUT=6000;oracle.jdbc.ReadTimeout=6000&quot;/&gt; &lt;/dataSource&gt;&lt;/transactionManager&gt; 操作系统层面的 Socket 超时配置如果没设置 Socket 超时或连接超时，应用程序多数情况下无法检测到网络错误。此时，应用程序将一直等待下去，直到连接上数据库或能读取到数据。然而，如果查看实际服务遇到的实际情况会发现问题常常在在应用程序（WAS）在30分钟后尝试重新连接到网络后被解决了。这是因为操作系统也配置了 Socket 超时时间。我公司使用的 Linux 服务器将 Socket 超时时间设置为30分钟。它将在操作系统层面对网络连接做校验。因为公司的 Linux 服务器的 KeepAlive 检查周期为30分钟，因此即使应用程序里将 Socket 超时设置为0，由网络原因引起的数据库网络连接问题也不会超过30分钟。 通常，应用程序会在调用 Socket 的 read() 方法时由于网络问题而阻塞住。然而很少在调用 Socket 的 write() 方法时处于等待状态，这取决于网络构成和错误类型。当应用程序调用 Socket 的 write() 方法时，数据被记录到操作系统的内核缓冲区，然后将控制权立即交还给应用程序。因此，一旦数据已经写入内核缓冲区，write() 的调用始终是成功。但是，如果操作系统内核缓冲区由于特殊的网络错误而满了的话，write() 方法也会进入等待状态。这种情况下，操作系统会尝试重新发送数据包一段时间，并在达到超时限制时产生错误。 在公司的 Linux服务器上这种情况的超时时间设置为15分钟。 至此，我已经解释了 JDBC 的内部操作，希望这将帮助你正确的超时配置超时时间从而减少错误。 至此，我已经对JDBC的内部操作做了讲解，希望能够让大家学会如何正确的配置超时时间，从而减少错误的发生。]]></content>
      <categories>
        <category>JDBC</category>
      </categories>
      <tags>
        <tag>JDBC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker创建镜像]]></title>
    <url>%2Fposts%2F6e0f%2F</url>
    <content type="text"><![CDATA[修改已有镜像 使用下载的镜像启动容器 12$ sudo docker run -t -i training/sinatra /bin/bashroot@0b2616b0e5a8:$ 在容器中添加 json 和 gem 两个应用 1root@0b2616b0e5a8:$ gem install json 提交更新后的副本 12$ sudo docker commit -m "Added json gem" -a "Docker Newbee" 0b2616b0e5a8 ouruser/sinatra:v24f177bd27a9ff0f6dc2a830403925b5360bfe0b93d476f7fc3231110e7f71b1c 其中，-m 来指定提交的说明信息，跟我们使用的版本控制工具一样；-a 可以指定更新的用户信息；之后是用来创建镜像的容器的 ID；最后指定目标镜像的仓库名和 tag 信息。创建成功后会返回这个镜像的 ID 信息。 使用 docker images 来查看新创建的镜像。 12345$ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEtraining/sinatra latest 5bc342fa0b91 10 hours ago 446.7 MBouruser/sinatra v2 3c59e02ddd1a 10 hours ago 446.7 MBouruser/sinatra latest 5db5f8471261 10 hours ago 446.7 MB 可以使用新的镜像来启动容器 12$ sudo docker run -t -i ouruser/sinatra:v2 /bin/bashroot@78e82f680994:$ 利用Dockerfile来创建镜像 新建一个目录和一个 Dockerfile 123$ mkdir sinatra$ cd sinatra$ touch Dockerfile Dockerfile 中每一条指令都创建镜像的一层，例如： 123456# This is a commentFROM ubuntu:14.04MAINTAINER Docker Newbee &lt;newbee@docker.com&gt;RUN apt-get -qq updateRUN apt-get -qqy install ruby ruby-devRUN gem install sinatra Dockerfile 基本的语法是 使用#来注释 FROM 指令告诉 Docker 使用哪个镜像作为基础 接着是维护者的信息 RUN开头的指令会在创建中运行，比如安装一个软件包，在这里使用 apt-get 来安装了一些软件 编写完成 Dockerfile 后可以使用 docker build 来生成镜像。 1234567891011121314151617181920212223242526272829303132$ sudo docker build -t="ouruser/sinatra:v2" .Uploading context 2.56 kBUploading contextStep 0 : FROM ubuntu:14.04 ---&gt; 99ec81b80c55Step 1 : MAINTAINER Newbee &lt;newbee@docker.com&gt; ---&gt; Running in 7c5664a8a0c1 ---&gt; 2fa8ca4e2a13Removing intermediate container 7c5664a8a0c1Step 2 : RUN apt-get -qq update ---&gt; Running in b07cc3fb4256 ---&gt; 50d21070ec0cRemoving intermediate container b07cc3fb4256Step 3 : RUN apt-get -qqy install ruby ruby-dev ---&gt; Running in a5b038dd127eSelecting previously unselected package libasan0:amd64.(Reading database ... 11518 files and directories currently installed.)Preparing to unpack .../libasan0_4.8.2-19ubuntu1_amd64.deb ...Setting up ruby (1:1.9.3.4) ...Setting up ruby1.9.1 (1.9.3.484-2ubuntu1) ...Processing triggers for libc-bin (2.19-0ubuntu6) ... ---&gt; 2acb20f17878Removing intermediate container a5b038dd127eStep 4 : RUN gem install sinatra ---&gt; Running in 5e9d0065c1f7. . .Successfully installed rack-protection-1.5.3Successfully installed sinatra-1.4.54 gems installed ---&gt; 324104cde6adRemoving intermediate container 5e9d0065c1f7Successfully built 324104cde6ad 其中 `-t` 标记来添加 tag，指定新的镜像的用户信息。 “.” 是 Dockerfile 所在的路径（当前目录），也可以替换为一个具体的 Dockerfile 的路径。 注意一个镜像不能超过 127 层]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker常用命令]]></title>
    <url>%2Fposts%2F1d91%2F</url>
    <content type="text"><![CDATA[查看版本123$ docker --version$ docker-compose --version$ docker-machine --version 从docker注册中心拉取镜像1$ docker pull &#123;container_name&#125; 运行容器，1$ docker run &#123;container_name&#125; -p {HOST_PORT}:{CLIENT_PORT} 端口映射（不指定端口则随机映射） -t 让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上 -i 让容器的标准输入保持打开 -d 让 Docker 在后台运行 查看容器端口1$ docker port &#123;container_name&#125; 停止容器1$ docker stop &#123;container_name&#125; 获取本地镜像列表1$ docker images 显示当前正在运行的所有容器1$ docker ps or 1docker container ls 显示所有运行过的容器1$ docker ps -a 删除容器1$ docker rm 305297d7a235 ff0a5c3750b9 删除所有停止的容器1$ docker container prune or 1$ docker rm $(docker ps -a -q -f status=exited) 删除所有挂起的镜像1$ docker image prune 搜素镜像1$ docker search xxx 拷贝文件到容器1$ docker cp [source_file] [container_name]:[target_path] 拷贝文件到宿主机1$ docker cp [container_name]:[source_file] [target_path] 保存对容器的修改1$ docker commit &lt;container_id&gt; &lt;image_name&gt;]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql processlist中哪些状态要引起关注]]></title>
    <url>%2Fposts%2F3b7f%2F</url>
    <content type="text"><![CDATA[一般而言，我们在processlist结果中如果经常能看到某些SQL的话，至少可以说明这些SQL的频率很高，通常需要对这些SQL进行进一步优化。 状态 建议 copy to tmp table 执行ALTER TABLE修改表结构时 建议：放在凌晨执行或者采用类似pt-osc工具 Copying to tmp table 拷贝数据到内存中的临时表，常见于GROUP BY操作时 建议：创建适当的索引 Copying to tmp table on disk 临时结果集太大，内存中放不下，需要将内存中的临时表拷贝到磁盘上，形成 #sql.MYD、#sql.MYI(在5.6及更高的版本，临时表可以改成InnoDB引擎了，可以参考选项 default_tmp_storage_engine) 建议：创建适当的索引，并且适当加大 sort_buffer_size/tmp_table_size/max_heap_table_size Creating sort index 当前的SELECT中需要用到临时表在进行ORDER BY排序 建议：创建适当的索引 Creating tmp table 创建基于内存或磁盘的临时表，当从内存转成磁盘的临时表时，状态会变成：Copying to tmp table on disk 建议：创建适当的索引，或者少用UNION、视图(VIEW)、子查询(SUBQUERY)之类的，确实需要用到临时表的时候，可以在session级临时适当调大 tmp_table_size/max_heap_table_size 的值 Reading from net 表示server端正通过网络读取客户端发送过来的请求 建议：减小客户端发送数据包大小，提高网络带宽/质量 Sending data 从server端发送数据到客户端，也有可能是接收存储引擎层返回的数据，再发送给客户端，数据量很大时尤其经常能看见备注：Sending Data不是网络发送，是从硬盘读取，发送到网络是Writing to net 建议：通过索引或加上LIMIT，减少需要扫描并且发送给客户端的数据量 Sorting result 正在对结果进行排序，类似Creating sort index，不过是正常表，而不是在内存表中进行排序 建议：创建适当的索引 statistics 进行数据统计以便解析执行计划，如果状态比较经常出现，有可能是磁盘IO性能很差 建议：查看当前io性能状态，例如iowait Waiting for global read lock FLUSH TABLES WITH READ LOCK整等待全局读锁 建议：不要对线上业务数据库加上全局读锁，通常是备份引起，可以放在业务低谷期间执行或者放在slave服务器上执行备份 Waiting for tables, Waiting for table flush FLUSH TABLES, ALTER TABLE, RENAME TABLE, REPAIR TABLE, ANALYZE TABLE, OPTIMIZE TABLE等需要刷新表结构并重新打开 建议：不要对线上业务数据库执行这些操作，可以放在业务低谷期间执行 Waiting for lock_type lock 等待各种类型的锁：• Waiting for event metadata lock• Waiting for global read lock• Waiting for schema metadata lock• Waiting for stored function metadata lock• Waiting for stored procedure metadata lock • Waiting for table level lock • Waiting for table metadata lock • Waiting for trigger metadata lock 建议：比较常见的是上面提到的global read lock以及table metadata lock，建议不要对线上业务数据库执行这些操作，可以放在业务低谷期间执行。如果是table level lock，通常是因为还在使用MyISAM引擎表，赶紧转投InnoDB引擎吧，别再老顽固了 更多详情可参考官方手册： 8.14.2 General Thread States]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[婚礼当天流程]]></title>
    <url>%2Fposts%2Fd823%2F</url>
    <content type="text"><![CDATA[上午安排： 6:00享喜（新郎妈妈） 7:00铺床、打扫新房（林晶、钟鸣亮） 8:00买鲜花、气球装饰（林晶） 9:00铺红地毯（钟鸣亮、张永峰） 9:00皇冠酒店装喜糖（林潇潇、陈瑞） 8:00–10:00新娘化妆（伴娘到达新娘家），联系摄像师到达时间 10:00–11:00 新娘、伴娘外景拍摄（小区内草坪） 12:00–12:30新郎负责婚车装饰，伴郎到位，安排好车辆 迎新娘： 12:58新郎带领迎亲车队出发去新娘家 到达前新郎准备好鲜花、红包 、饼干、巧克力、纸包（拿回一袋）、香烟 出门前告知新娘 抵达前10分钟告知迎亲爆竹准备 进门前新娘亲友提问阻挠（可有可无）、塞红包吧 新娘提问、新郎承诺 新郎给女方父母敬茶、合影 桂圆、莲心、红枣（水浦蛋）汤（视习俗） 出门前新娘向父母告别、新郎向女方父母承诺 、女方父母祝福语 14:18迎亲车队离开新娘家，爆竹准备 到新房： 车队从新娘家出发后开到新郎家 抵达前10分钟告知迎亲爆竹准备 进门前新郎家亲友拦门（钟鸣亮、张永峰） 男方父母准备见面红包 新娘给男方父母敬茶、交接钥匙、合影（林晶）、楼梯、新房拍照 桂圆、莲心、红枣（水浦蛋）汤（视习俗） 出门前向父母告别、男方父母祝福语 15:00左右从新郎家出发去酒店，出发前爆竹准备。 拍摄外景 外景拍摄地（酒店公共区域、无边际泳池、沙滩），直奔无边际泳池 外景拍摄约为一个小时 酒店准备： 糖、烟、酒、茶、饮料等带至酒店 检查酒席安排、音响、桌牌、签到处等细节 伴手礼放置签到处 酒店迎宾： 新郎新娘到达酒店休息，补妆，新郎补拍 签到处人员就位（林晶、钟鸣亮） 迎宾人员门口就位 新郎新娘伴郎伴娘门口迎宾 晚上 倒kelou（花花、满满） 尿尿（等等） 相关人员联系方式 人员 任务 电话 林剑（新郎） 15067172995 任霄（新娘） 15868417801 林宗跃（新郎爸爸） 13906841252 周玲玲（新郎妈妈） 18757457452 任祥基（新娘爸爸） 13586817026 苏静燕（新娘妈妈） 1373212429518989349490 林晶（新郎大姐） 13695776221 林潇潇（新郎二姐） 13989342347 泮伟光 婚车租赁（12点前到） 13777191855 花先生 婚车装饰 15888068655]]></content>
      <categories>
        <category>Other</category>
      </categories>
      <tags>
        <tag>Other</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Next美化]]></title>
    <url>%2Fposts%2F3495%2F</url>
    <content type="text"><![CDATA[设置语言为中文_config.yml 1language: zh-Hans Local Search 安装 hexo-generator-searchdb，在站点的根目录下执行以下命令： 编辑站点配置文件，新增以下内容到任意位置： 12345search: path: search.xml field: post format: html limit: 10000 编辑主题配置文件，启用本地搜索功能： 123# Local searchlocal_search: enable: true 修改文章底部标签图标NexT主题中所有图标来源于Font Awesome图标，因此和更改侧边栏社交链接图标同样，不再赘述。 修改模板/themes/next/layout/_macro/post.swig，搜索 rel=&quot;tag&quot;&gt;#，将 # 换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt; 即可。 隐藏网页底部 powered By Hexo打开themes/next/layout/_partials/footer.swig,注释相关代码 右下角显示当前浏览百分比打开 themes/next/_config.yml ，搜索关键字 scrollpercent 。把 false 改为 true。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat指定JDK版本]]></title>
    <url>%2Fposts%2F929d%2F</url>
    <content type="text"><![CDATA[Windows/bin/setclasspath.bat 12set JAVA_HOME=D:\Program Files\Java\jdk7\jdk1.7.0_51set JRE_HOME=D:\Program Files\Java\jdk7\jre7 Linux/bin/setclasspath.sh 12export JAVA_HOME=/home/jdk/Java\jdk7\jdk1.7.0_51export JRE_HOME=/home/jdk/Java\jdk7\jre7 原理启动tomcat可以通过运行bin下的startup.bat，startup.bat会调用catalina.bat文件，而catalina.bat会调用setclasspath.bat文件来获取JAVA_HOME和JRE_HOME这两个环境变量的值，因此若要在tomcat启动时指向特定的JDK，则需在setclasspath.bat文件的开头处加上JAVA_HOME和JRE_HOME。 查看Tomcat JDK版本 /bin/version.bat – Windows下的批处理脚本 /bin/version.sh – Linux下的Shell脚本]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用词汇]]></title>
    <url>%2Fposts%2F9587%2F</url>
    <content type="text"><![CDATA[中文 英文 孕婴 PregnantBaby 孕妇 PregnantWoman 孕期 Pregnancy 分娩期 childbirth 婴儿期 Infancy 末次月经 last menstrual period 编号 number 孕周 gestation 预产期 due date 孕次 gravida 产次 parity 胎盘 placenta 脐血 cord blood]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hexo常用命令]]></title>
    <url>%2Fposts%2F3e38%2F</url>
    <content type="text"><![CDATA[清理缓存文件1hexo clean 生成静态文件1hexo g 部署1hexo d 启动本地服务器1hexo s 部署网站1hexo d 调试模式1hexo --debug]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My First Blog]]></title>
    <url>%2Fposts%2F81cc%2F</url>
    <content type="text"><![CDATA[This is My First Blog!]]></content>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fposts%2F3eeb%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[Liquibase创建索引]]></title>
    <url>%2Fposts%2F5f4b%2F</url>
    <content type="text"><![CDATA[创建索引liquibase preconditions 创建索引前先判断索引是否存在 12345678910&lt;changeSet id=&quot;228-039&quot; author=&quot;jlin&quot;&gt; &lt;preConditions onFail=&quot;MARK_RAN&quot;&gt; &lt;not&gt; &lt;indexExists tableName=&quot;t_health_abnormal&quot; indexName=&quot;idx_create_time&quot; columnNames=&quot;create_time&quot;/&gt; &lt;/not&gt; &lt;/preConditions&gt; &lt;createIndex tableName=&quot;t_health_abnormal&quot; indexName=&quot;idx_create_time&quot;&gt; &lt;column name=&quot;create_time&quot; /&gt; &lt;/createIndex&gt; &lt;/changeSet&gt; Available attributes Attribute Description onFail What to do when preconditions fail (see below). onError What to do when preconditions error (see below). onUpdateSQL What to do in updateSQL mode (see below).Since 1.9.5 onFailMessage Custom message to output when preconditions fail.Since 2.0 onErrorMessage Custom message to output when preconditions fail.Since 2.0 Possible onFail/onError values Value Description HALT Immediately halt the execution of the entire change log.[DEFAULT] CONTINUE Skip over the change set. Execution of the change set will be attempted again on the next update. Continue with the change log. MARK_RAN Skip over the change set, but mark it as executed. Continue with the change log. WARN Output a warning and continue executing the change set/change log as normal. Possible onUpdateSQL values Value Description RUN Run the changeSet in updateSQL mode. FAIL Fail the preCondition in updateSQL mode. IGNORE Ignore the preCondition in updateSQL mode.]]></content>
      <categories>
        <category>Liquibase</category>
      </categories>
      <tags>
        <tag>Liquibase</tag>
      </tags>
  </entry>
</search>
